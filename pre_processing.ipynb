{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section1: Data Importation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 Get the basic paper review information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_info(conference_name, paper_type):\n",
    "    reviews_dir = f\"PeerRead/data/{conference_name}/{paper_type}/reviews\"\n",
    "    papers = []\n",
    "    for filename in os.listdir(reviews_dir):\n",
    "        file_path = os.path.join(reviews_dir, filename)\n",
    "        if filename.endswith('.json'):\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                title = data['title']\n",
    "                for review in data['reviews']:\n",
    "                    paper = {\n",
    "                        'title': title,\n",
    "                        \"IMPACT\": review.get(\"IMPACT\", np.nan),\n",
    "                        \"SUBSTANCE\": review.get(\"SUBSTANCE\", np.nan),\n",
    "                        \"APPROPRIATENESS\": review.get(\"APPROPRIATENESS\", np.nan),\n",
    "                        \"MEANINGFUL_COMPARISON\": review.get(\"MEANINGFUL_COMPARISON\", np.nan),\n",
    "                        \"PRESENTATION_FORMAT\": review.get(\"PRESENTATION_FORMAT\", np.nan),\n",
    "                        \"comments\": review.get(\"comments\", np.nan),\n",
    "                        \"SOUNDNESS_CORRECTNESS\": review.get(\"SOUNDNESS_CORRECTNESS\", np.nan),\n",
    "                        \"ORIGINALITY\": review.get(\"ORIGINALITY\", np.nan),\n",
    "                        \"RECOMMENDATION\": review.get(\"RECOMMENDATION\", np.nan),\n",
    "                        \"CLARITY\": review.get(\"CLARITY\", np.nan),\n",
    "                        \"REVIEWER_CONFIDENCE\": review.get(\"REVIEWER_CONFIDENCE\", np.nan)\n",
    "                    }\n",
    "                    papers.append(paper)\n",
    "    return papers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_names = ['conll_2016', 'acl_2017']\n",
    "paper_types = ['train', 'dev', 'test']\n",
    "papers_conll_2016 = []\n",
    "papers_iclr_2017 = []\n",
    "for conference_name in conference_names:\n",
    "    for paper_type in paper_types:\n",
    "        try:\n",
    "            papers = get_paper_info(conference_name, paper_type)\n",
    "        except:\n",
    "            print(f\"Error processing {conference_name}/{paper_type}\")\n",
    "        papers = get_paper_info(conference_name, paper_type)\n",
    " \n",
    "        if conference_name == 'conll_2016':\n",
    "            papers_conll_2016 += papers\n",
    "        else:\n",
    "            papers_iclr_2017 += papers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_2016_df = pd.DataFrame(papers_conll_2016)\n",
    "acl_2017_df = pd.DataFrame(papers_iclr_2017)\n",
    "conll_2016_df.to_pickle('data/conll_2016.pkl')\n",
    "acl_2017_df.to_pickle('data/acl_2017.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_2016_title = list(conll_2016_df['title'])\n",
    "acl_2017_title = list(acl_2017_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_title = set(conll_2016_title).union(set(acl_2017_title))\n",
    "with open('data/all_title.txt', 'w') as f:\n",
    "    for title in all_title:\n",
    "        f.write(\"%s\\n\" % title)\n",
    "# After this step, we use the Chatgpt for us to collect the citations for each of the papers in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 Get the acceptance and citation information using GPT and google scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/citation.json', 'r') as file:\n",
    "    data_citation = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_accepted': True, 'citation_num': 230}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_citation[\"A Comparison of Robust Parsing Methods for HPSG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>is_accepted</th>\n",
       "      <th>citation_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CANE: Context-Aware Network Embedding for Rela...</td>\n",
       "      <td>True</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weakly Supervised Cross-Lingual Named Entity R...</td>\n",
       "      <td>True</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discovering Correspondences between Multiple L...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Learning a Neural Semantic Parser from User Fe...</td>\n",
       "      <td>True</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Morphological Inflection Generation with Hard ...</td>\n",
       "      <td>True</td>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         paper_title  is_accepted  \\\n",
       "0  CANE: Context-Aware Network Embedding for Rela...         True   \n",
       "1  Weakly Supervised Cross-Lingual Named Entity R...         True   \n",
       "2  Discovering Correspondences between Multiple L...        False   \n",
       "3  Learning a Neural Semantic Parser from User Fe...         True   \n",
       "4  Morphological Inflection Generation with Hard ...         True   \n",
       "\n",
       "   citation_num  \n",
       "0         158.0  \n",
       "1         135.0  \n",
       "2           NaN  \n",
       "3          89.0  \n",
       "4         183.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_citation = pd.DataFrame.from_dict(data_citation, orient='index')\n",
    "df_citation.reset_index(inplace=True)\n",
    "df_citation.rename(columns={'index': 'paper_title'}, inplace=True)\n",
    "df_citation.head(5)\n",
    "\n",
    "# Nan values in the dataframe indicate that the paper was not found in Google Scholar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_title = list(df_citation[df_citation['is_accepted'] == True]['paper_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_accepted_df = conll_2016_df[conll_2016_df['title'].isin(valid_title)].reset_index(drop=True)\n",
    "conll_accepted_df['citation_count'] = conll_accepted_df['title'].apply(\n",
    "    lambda x: data_citation[x]['citation_num'] \n",
    ")\n",
    "conll_accepted_df['source'] = 'conll_2016' \n",
    "conll_accepted_df.to_pickle('data/conll_accepted.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_accepted = acl_2017_df[acl_2017_df['title'].isin(valid_title)].reset_index(drop=True)\n",
    "acl_accepted['citation_count'] = acl_accepted['title'].apply(\n",
    "    lambda x: data_citation[x]['citation_num'] \n",
    ")\n",
    "acl_accepted['source'] = 'acl_2017'\n",
    "acl_accepted.to_pickle('data/acl_accepted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union the two sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([conll_accepted_df, acl_accepted], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_pickle('data/combined_accepted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section2: Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_paper = pd.read_pickle('data/combined_accepted.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>IMPACT</th>\n",
       "      <th>SUBSTANCE</th>\n",
       "      <th>APPROPRIATENESS</th>\n",
       "      <th>MEANINGFUL_COMPARISON</th>\n",
       "      <th>PRESENTATION_FORMAT</th>\n",
       "      <th>comments</th>\n",
       "      <th>SOUNDNESS_CORRECTNESS</th>\n",
       "      <th>ORIGINALITY</th>\n",
       "      <th>RECOMMENDATION</th>\n",
       "      <th>CLARITY</th>\n",
       "      <th>REVIEWER_CONFIDENCE</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Redefining part-of-speech classes with distrib...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Poster</td>\n",
       "      <td>The aim of this paper is to show that distribu...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>conll_2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Redefining part-of-speech classes with distrib...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Poster</td>\n",
       "      <td>## General comments:\\nThis paper presents an e...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>conll_2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Massively Multilingual Word Embeddings</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Poster</td>\n",
       "      <td>This paper describes four methods of obtaining...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "      <td>conll_2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Massively Multilingual Word Embeddings</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Poster</td>\n",
       "      <td>This paper proposes two dictionary-based metho...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "      <td>conll_2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Semi-supervised Convolutional Networks for Tra...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Poster</td>\n",
       "      <td>The paper describes an MT training data select...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>conll_2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title IMPACT SUBSTANCE  \\\n",
       "0  Redefining part-of-speech classes with distrib...      2         1   \n",
       "1  Redefining part-of-speech classes with distrib...      2         4   \n",
       "2             Massively Multilingual Word Embeddings      3         3   \n",
       "3             Massively Multilingual Word Embeddings      4         5   \n",
       "4  Semi-supervised Convolutional Networks for Tra...      4         4   \n",
       "\n",
       "  APPROPRIATENESS MEANINGFUL_COMPARISON PRESENTATION_FORMAT  \\\n",
       "0               3                     3              Poster   \n",
       "1               5                     1              Poster   \n",
       "2               4                     3              Poster   \n",
       "3               4                     5              Poster   \n",
       "4               5                     4              Poster   \n",
       "\n",
       "                                            comments SOUNDNESS_CORRECTNESS  \\\n",
       "0  The aim of this paper is to show that distribu...                     4   \n",
       "1  ## General comments:\\nThis paper presents an e...                     4   \n",
       "2  This paper describes four methods of obtaining...                     4   \n",
       "3  This paper proposes two dictionary-based metho...                     5   \n",
       "4  The paper describes an MT training data select...                     4   \n",
       "\n",
       "  ORIGINALITY RECOMMENDATION CLARITY REVIEWER_CONFIDENCE  citation_count  \\\n",
       "0           2              2       4                   4              80   \n",
       "1           2              2       4                   5              80   \n",
       "2           3              3       4                   4             130   \n",
       "3           3              3       5                   4             130   \n",
       "4           3              4       3                   4              70   \n",
       "\n",
       "       source  \n",
       "0  conll_2016  \n",
       "1  conll_2016  \n",
       "2  conll_2016  \n",
       "3  conll_2016  \n",
       "4  conll_2016  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accepted_paper.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'IMPACT', 'SUBSTANCE', 'APPROPRIATENESS',\n",
       "       'MEANINGFUL_COMPARISON', 'PRESENTATION_FORMAT', 'comments',\n",
       "       'SOUNDNESS_CORRECTNESS', 'ORIGINALITY', 'RECOMMENDATION', 'CLARITY',\n",
       "       'REVIEWER_CONFIDENCE', 'citation_count', 'source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accepted_paper.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 Paper Review Index Breakdown\n",
    "\n",
    "| Index | What It Means | Why It Matters |\n",
    "|-------|----------------|----------------|\n",
    "| `title` | Title of the paper | Useful for identifying the paper, but doesn't influence acceptance or citations. |\n",
    "| `IMPACT` | Potential impact of the work on the field | **Important for both acceptance & citations**. High-impact papers are more likely to be accepted and cited. |\n",
    "| `SUBSTANCE` | Depth and completeness of the contribution | **Highly important** for acceptance. Reviewers value comprehensive and substantial work. |\n",
    "| `APPROPRIATENESS` | Suitability for the conference or venue | **Critical for acceptance**. Even great work can be rejected if off-topic. |\n",
    "| `MEANINGFUL_COMPARISON` | Quality of experiments and baselines | **Relevant to acceptance**. Good comparisons increase credibility and reviewer confidence. |\n",
    "| `PRESENTATION_FORMAT` | Adherence to formatting guidelines | **Moderate impact on acceptance**. Poor formatting can result in desk rejection. |\n",
    "| `comments` | Free-form reviewer feedback | Provides insights into reviewer thought process. Useful for qualitative analysis. |\n",
    "| `SOUNDNESS_CORRECTNESS` | Theoretical and empirical validity | **Crucial for acceptance**. Flawed logic or methodology is a major rejection reason. |\n",
    "| `ORIGINALITY` | Novelty and innovativeness | **Important for both acceptance & citations**. More original work tends to attract attention. |\n",
    "| `RECOMMENDATION` | Reviewer's overall recommendation (e.g. Accept/Reject) | **Directly tied to acceptance**. Summarizes reviewer evaluation. |\n",
    "| `CLARITY` | How clearly the work is written | **Impacts both acceptance & citations**. Clear papers are easier to understand and build upon. |\n",
    "| `REVIEWER_CONFIDENCE` | How confident the reviewer is in their review | Useful for interpreting other scores. Low confidence can indicate reviewer uncertainty. |\n",
    "| `citation_count` | Number of citations the paper has received | **Measures post-acceptance impact**. Not relevant to acceptance, but very important for assessing long-term value. |\n",
    "| `source` | Conference or dataset source | Contextual info; may be useful for filtering or analysis but not directly impactful. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section3 Data Pre-processing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the non-numerical to numerical one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRESENTATION_FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Poster', 'Oral Presentation'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accepted_paper['PRESENTATION_FORMAT'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_num(format_str):\n",
    "    if format_str == 'Oral Presentation':\n",
    "        return 1\n",
    "    elif format_str == 'Poster':\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_paper['PRESENTATION_FORMAT_NUM']  = accepted_paper['PRESENTATION_FORMAT'].apply(format_to_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aim of this paper is to show that distributional information stored in word\n",
      "vector models contain information about POS labels. They use a version of the\n",
      "BNC annotated with UD POS and in which words have been replaced by lemmas. They\n",
      "train word embeddings on this corpus, then use the resulting vectors to train a\n",
      "logistic classifier to predict the word POS. Evaluations are performed on the\n",
      "same corpus (using cross-validation) as well as on other corpora. Results are\n",
      "clearly presented and discussed and analyzed at length.\n",
      "\n",
      "The paper is clear and well-written. The main issue with this paper is that it\n",
      "does not contain anything new in terms of NLP or ML. It describe a set of\n",
      "straightforward experiments without any new NLP or ML ideas or methods. Results\n",
      "are interesting indeed, in so far that they provide an empirical grounding to\n",
      "the notion of POS. In that regard, it is certainly worth being published in a\n",
      "(quantitative/emprirical) linguistic venue.\n",
      "\n",
      "On another note, the literature on POS tagging and POS induction using word\n",
      "embeddings should be cited more extensively (cf. for instance Lin, Ammar, Duer\n",
      "and Levin 2015; Ling et al. 2015 [EMNLP]; Plank, SÃ¸gaard and Goldberg\n",
      "2016...).\n",
      "## General comments:\n",
      "This paper presents an exploration of the connection between part-of-speech\n",
      "tags and word embeddings. Specifically the authors use word embeddings to draw\n",
      "some interesting (if not somewhat straightforward) conclusions about the\n",
      "consistency of PoS tags and the clear connection of word vector representations\n",
      "to PoS. The detailed error analysis (outliers of classification) is definitely\n",
      "a strong point of this paper.\n",
      "\n",
      "However, the paper seems to have missing one critical main point: the reason\n",
      "that corpora such as the BNC were PoS tagged in the first place. Unlike a\n",
      "purely linguistic exploration of morphosyntactic categories (which are\n",
      "underlined by a semantic prototype theory - e.g. see Croft, 1991), these\n",
      "corpora were created and tagged to facilitate further NLP tasks, mostly\n",
      "parsing. The whole discussion could then be reframed as whether the\n",
      "distinctions made by the distributional vectors are more beneficial to parsing\n",
      "as compared to the original tags (or UPOS for that matter). \n",
      "\n",
      "Also, this paper is missing a lot of related work in the context of\n",
      "distributional PoS induction. I recommend starting with the review\n",
      "Christodoulopoulos et al. 2010 and adding some more recent non-DNN work\n",
      "including Blunsom and Cohn (2011), Yatbaz et al. (2012), etc. In light of this\n",
      "body of work, the results of section 5 are barely novel (there are systems with\n",
      "more restrictions in terms of their external knowledge that achieve comparable\n",
      "results).\n",
      "\n",
      "## Specific issues\n",
      "In the abstract one of the contributed results is that \"distributional vectors\n",
      "do contain information about PoS affiliation\". Unless I'm misunderstanding the\n",
      "sentence, this is hardly a new result, especially for English: every\n",
      "distributionally-based PoS induction system in the past 15 years that presents\n",
      "\"many-to-one\" or \"cluster purity\" numbers shows the same result.\n",
      "\n",
      "The assertion in lines 79-80 (\"relations between... vectors... are mostly\n",
      "semantic\") is not correct: the <MIKOLOV or COLOBERT> paper (and subsequent\n",
      "work) shows that there is a lot of syntactic information in these vectors. Also\n",
      "see previous comment about cluster purity scores. In fact you revert that\n",
      "statement in the beginning of section 2 (lines 107-108).\n",
      "\n",
      "Why move to UPOS? Surely the fine-grained distinctions of the original tagset\n",
      "are more interesting.\n",
      "\n",
      "I do not understand footnote 3. Were these failed attempts performed by you or\n",
      "other works? Under what criteria did they fail? What about Brown cluster\n",
      "vectors? They almost perfectly align with UPOS tags.\n",
      "\n",
      "Is the observation that \"proper nouns are not much similar to common nouns\"\n",
      "(lines 331-332) that interesting? Doesn't the existence of \"the\" (the most\n",
      "frequent function word) almost singlehandedly explain this difference?\n",
      "\n",
      "While I understand the practical reasons for analysing the most frequent\n",
      "word/tag pairs, it would be interesting to see what happens in the tail, both\n",
      "in terms of the vectors and also for the types of errors the classifier makes.\n",
      "You could then try to imagine alternatives to pure distributional (and\n",
      "morphological - since you're lemmatizing) features that would allow better\n",
      "generalizations of the PoS tags to these low-frequency words.\n",
      "\n",
      "## Minor issues\n",
      "Change the sentential references to \\newcite{}: e.g. \"Mikolov et al. (2013b)\n",
      "showed\"\n",
      "This paper describes four methods of obtaining multilingual word embeddings and\n",
      "a modified QVEC metric for evaluating the efficacy of these embeddings. The\n",
      "embedding methods are: \n",
      "\n",
      "(1) multiCluster : Uses a dictionary to map words to multilingual clusters.\n",
      "Cluster embeddings are then obtained which serve as embeddings for the words\n",
      "that reside in each cluster. \n",
      "\n",
      "(2) multiCCA : Extends the approach presented by Faruqui and Dyer (2014) for\n",
      "embedding bilingual words, to multilingual words by using English embeddings as\n",
      "the anchor space. Bilingual dictionaries (other_language -> English) are then\n",
      "used to obtain projections from other monolingual embeddings for words in other\n",
      "languages to the anchor space. \n",
      "\n",
      "(3) multiSkip : Extends the approach presented by Luong et al. (2015b) for\n",
      "embedding using source and target context (via alignment), to the multilingual\n",
      "case by extending the objective function to include components for all\n",
      "available parallel corpora. \n",
      "\n",
      "(4) Translation invariance : Uses a low rank decomposition of the word PMI\n",
      "matrix with an objective with includes bilingual alignment frequency\n",
      "components. May only work for  bilingual embeddings. \n",
      "\n",
      "The evaluation method uses CCA to maximize the correlation between the word\n",
      "embeddings and possibly hand crafted linguistic data. Basis vectors are\n",
      "obtained for the aligned dimensions which produce a score which is invariant to\n",
      "rotation and linear transformations. The proposed method also extends this to\n",
      "multilingual evaluations. \n",
      "\n",
      "In general, the paper is well written and describes the work clearly. A few\n",
      "major issues:\n",
      "\n",
      "(1) What is the new contribution with respect to the translation invariance\n",
      "embedding approach of Gardner et al.? If it is the extension to multilingual\n",
      "embeddings, a few lines explaining the novelty would help. \n",
      "\n",
      "(2) The use of super-sense annotations across multiple languages is a problem.\n",
      "The number of features in the intersection of multiple languages may become\n",
      "really small. How do the authors propose to address this problem (beyond\n",
      "footnote 9)?\n",
      "\n",
      "(3) How much does coverage affect the score in table 2? For example, for\n",
      "dependency parsing, multi cluster and multiCCA have significantly different\n",
      "coverage numbers with scores that are close. \n",
      "\n",
      "(4) In general, the results in table 3 do not tell a consistent story. Mainly,\n",
      "for most of the intrinsic metrics, the multilingual embedding techniques do not\n",
      "seem to perform the best.  Given that one of the primary goals of this paper\n",
      "was to create embeddings that perform well under the word translation metric\n",
      "(intra-language), it is disappointing that the method that performs best (by\n",
      "far) is the invariance approach. It is also strange that the multi-cluster\n",
      "approach, which discards inter-cluster (word and language) semantic information\n",
      "performs the best with respect to the extrinsic metrics.\n",
      "\n",
      "Other questions for the authors:\n",
      "\n",
      "(1) What is the loss in performance by fixing the word embeddings in the\n",
      "dependency parsing task? What was the gain by simply using these embeddings as\n",
      "alternatives to the random embeddings in the LSTM stack parser? \n",
      "\n",
      "(2) Is table 1 an average over the 17 embeddings described in section 5.1? \n",
      "\n",
      "(3) Are there any advantages of using the multi-Skip approach instead of\n",
      "learning bilingual embeddings and performing multi-CCA to learning projections\n",
      "across the distinct spaces?\n",
      "\n",
      "(4) The dictionary extraction approach (from parallel corpora via alignments or\n",
      "from google translate) may not reflect the challenges of using real lexicons.\n",
      "Did you explore the use of any real multi-lingual dictionaries?\n",
      "This paper proposes two dictionary-based methods for estimating multilingual\n",
      "word embeddings, one motivated in clustering (MultiCluster) and another in\n",
      "canonical correlation analysis (MultiCCA).\n",
      "In addition, a supersense similarity measure is proposed that improves on QVEC\n",
      "by substituting its correlation component with CCA, and by taking into account\n",
      "multilingual evaluation.\n",
      " The evaluation is performed on a wide range of tasks using the web portal\n",
      "developed by the authors; it is shown that in some cases the proposed\n",
      "representation methods outperform two other baselines.\n",
      "\n",
      "I think the paper is very well written, and represents a substantial amount of\n",
      "work done. The presented representation-learning and evaluation methods are\n",
      "certainly timely. I also applaud the authors for the meticulous documentation.\n",
      "\n",
      "My general feel about this paper, however, is that it goes (perhaps) in too\n",
      "much breadth at the expense of some depth. I'd prefer to see a thorougher\n",
      "discussion of results (e.g. regarding the conflicting outcome for MultiCluster\n",
      "between 59- and 12-language set-up; regarding the effect of estimation\n",
      "parameters and decisions in MultiCluster/CCA). So, while I think the paper is\n",
      "of high practical value to me and the research community (improved QVEC\n",
      "measure, web portal), I frankly haven't learned that much from reading it, i.e.\n",
      "in terms of research questions addressed and answered.\n",
      "\n",
      "Below are some more concrete remarks.\n",
      "\n",
      "It would make sense to include the correlation results (Table 1) for\n",
      "monolingual QVEC and QVEC-CCA as well. After all, it is stated in l.326--328\n",
      "that the proposed QVEC-CCA is an improvement over QVEC.\n",
      "\n",
      "Minor:\n",
      "l. 304: \"a combination of several cross-lingual word similarity datasets\" ->\n",
      "this sounds as though they are of different nature, whereas they are really of\n",
      "the same kind, just different languages, right?\n",
      "\n",
      "p. 3: two equations exceed the column margin\n",
      "\n",
      "Lines 121 and 147 only mention Coulmance et al and Guo et al when referring to\n",
      "the MultiSkip baseline, but section 2.3 then only mentions Luong et al. So,\n",
      "what's the correspondence between these works?\n",
      "\n",
      "While I think the paper does reasonable justice in citing the related works,\n",
      "there are more that are relevant and could be included:\n",
      "\n",
      "Multilingual embeddings and clustering:\n",
      "Chandar A P, S., Lauly, S., Larochelle, H., Khapra, M. M., Ravindran, B.,\n",
      "Raykar, V. C., and Saha, A. (2014). An autoencoder approach to learning\n",
      "bilingual word representations. In NIPS.\n",
      "Hill, F., Cho, K., Jean, S., Devin, C., and Bengio, Y. (2014). Embedding word\n",
      "similarity with neural machine translation. arXiv preprint arXiv:1412.6448.\n",
      "Lu, A., Wang, W., Bansal, M., Gimpel, K., & Livescu, K. (2015). Deep\n",
      "multilingual correlation for improved word embeddings. In NAACL.\n",
      "Faruqui, M., & Dyer, C. (2013). An Information Theoretic Approach to Bilingual\n",
      "Word Clustering. In ACL.\n",
      "\n",
      "Multilingual training of embeddings for the sake of better source-language\n",
      "embeddings:\n",
      "Suster, S., Titov, I., and van Noord, G. (2016). Bilingual learning of\n",
      "multi-sense embeddings with discrete autoencoders. In NAACL-HLT.\n",
      "Guo, J., Che, W., Wang, H., and Liu, T. (2014). Learning sense-specific word\n",
      "embeddings by exploiting bilingual resources. In COLING.\n",
      "\n",
      "More broadly, translational context has been explored e.g. in\n",
      "Diab, M., & Resnik, P. (2002). An unsupervised method for word sense tagging\n",
      "using parallel corpora. In ACL.\n",
      "The paper describes an MT training data selection approach that scores and\n",
      "ranks general-domain sentences using a CNN classifier. Comparison to prior work\n",
      "using continuous or n-gram based language models is well done, even though  it\n",
      "is not clear of the paper also compared against bilingual data selection (e.g.\n",
      "sum of difference of cross-entropies).\n",
      "The motivation to use a CNN instead of an RNN/LSTM was first unclear to me, but\n",
      "it is a strength of the paper to argue that certain sections of a text/sentence\n",
      "are more important than others and this is achieved by a CNN. However, the\n",
      "paper does not experimentally show whether a BOW or SEQ (or the combination of\n",
      "both( representation is more important and why.\n",
      "The textual description of the CNN (one-hot or semi-supervised using\n",
      "pre-trained embeddings) \n",
      "is clear, detailed, and points out the important aspects. However, a picture of\n",
      "the layers showing how inputs are combined would be worth a thousand words.\n",
      "\n",
      "The paper is overall well written, but some parentheses for citations are not\n",
      "necessary (\\citet vs. \\citep) (e.g line 385).\n",
      "\n",
      "Experiments and evaluation support the claims of the paper, but I am a little\n",
      "bit concerned about the method of determining the number of selected in-domain\n",
      "sentences (line 443) based on a separate validation set:\n",
      "- What validation data is used here? It is also not clear on what data\n",
      "hyperparameters of the CNN models are chosen. How sensitive are the models to\n",
      "this?\n",
      "- Table 2 should really compare scores of different approaches with the same\n",
      "number of sentences selected. As Figure 1 shows, the approach of the paper\n",
      "still seems to outperform the baselines in this case. \n",
      "\n",
      "Other comments:\n",
      "- I would be interested in an experiment that compares the technique of the\n",
      "paper against baselines when more in-domain data is available, not just the\n",
      "development set.\n",
      "- The results or discussion section could feature some example sentences\n",
      "selected by the different methods to support the claims made in section 5.4.\n",
      "- In regards to the argument of abstracting away from surface forms in 5.4:\n",
      "Another baseline to compare against could have been the work of Axelrod, 2015,\n",
      "who replace some words with POS tags to reduce LM data sparsity to see whether\n",
      "the word2vec embeddings provide an additional advantage over this.\n",
      "- Using the sum of source and target classification scores is very similar to\n",
      "source & target Lewis-Moore LM data selection: sum of difference of\n",
      "cross-entropies. A reference to this work around line 435 would be reasonable.\n",
      "\n",
      "Finally, I wonder if you could learn weights for the sum of both source &\n",
      "target classification scores by extending the CNN model to the\n",
      "bilingual/parallel setting.\n",
      "The paper describes a method for in-domain data selection for SMT with a\n",
      "convolutional neural network classifier, applying the same framework as Johnson\n",
      "and Zhang, 2015. The method performs about 0.5 BLEU points better than language\n",
      "model based data selection, and, unlike the other methods, is robust even if\n",
      "only a very small in-domain data set is provided. \n",
      "\n",
      "The paper claims improvements of 3.1 BLEU points. However, from the results we\n",
      "see that improvements of this magnitude are only achieved if there are\n",
      "in-domain data in the training set - training only on the in-domain data\n",
      "already produces +2.8 BLEU. It might be interesting to also compare this to a\n",
      "system which interpolates separate in- and out-domain models. \n",
      "\n",
      "The more impressive result, in my opinion, comes from the second experiment,\n",
      "which demonstrates that the CNN classifier is still effective if there is very\n",
      "little in-domain data. However, the second experiment is only run on the zh2en\n",
      "task which includes actual in-domain data in the training set, possibly making\n",
      "selection easier. Would the result also hold for the other tasks, where there\n",
      "is no in-domain data in the training set? The results for the en2es and en2zh\n",
      "task already point in this direction, since the development sets only contain a\n",
      "few hundred sentence pairs. I think the claim would be better supported if\n",
      "results were reported for all tasks when only 100 sentence pairs are used for\n",
      "training.  \n",
      "\n",
      "When translating social media text one often has to face very different\n",
      "problems from other domains, the most striking being a high OOV rate due to\n",
      "non-conventional spelling (for Latin scripts, at least). The texts can also\n",
      "contain special character sequences such as usernames, hashtags or emoticons.\n",
      "Was there any special preprocessing or filtering step applied to the data?  \n",
      "Since data selection cannot address the OOV problem, it would be interesting to\n",
      "know in more detail what kinds of improvements are made through adaptation via\n",
      "data selection, maybe by providing examples.   \n",
      "\n",
      "The following remarks concern specific sections:\n",
      "\n",
      "Section 3.2:\n",
      "- It could be made clearer how the different vectors (word embeddings, segment\n",
      "vectors and one-hot vectors) are combined in the model. An illustration of the\n",
      "architecture would be very helpful. \n",
      "- What was the \"designated loss function\"?\n",
      "\n",
      "Section 5.2:\n",
      "For completeness' sake, it could be mentioned how the system weights were\n",
      "tuned.\n",
      "This paper presents a Stack LSTM parser based on the work of Henderson et al.\n",
      "(2008, 2013) on joint syntactic/semantic transition-based parsing and Dyer et\n",
      "al. (2015) on stack LSTM syntactic parsing. The use of the transition system\n",
      "from the former and the stack LSTM from the latter shows interesting results\n",
      "compared to the joint systems on the CoNLL 2008 and 2009 shared tasks.\n",
      "\n",
      "I like this paper a lot because it is well-written, well-explained, the related\n",
      "work is good and the results are very interesting. The methodology is sound\n",
      "(with a minor concern regarding the Chinese embeddings, leading me to believe\n",
      "than very good embeddings can be more informative than a very clever model...).\n",
      "\n",
      "Moreover, the description of the system is clear, the hyperparameters are\n",
      "justified and the discussion is interesting.\n",
      "\n",
      "The only thing I would say is that the proposed system lacks originality in the\n",
      "sense that the work of Henderson et al. puts the basis of semi-synchronised\n",
      "joint syntax-semantic transition-based parsing several years ago and Dyer et\n",
      "al. came up with the stack LSTM last year, so it is not a new method, per say.\n",
      "But in my opinion, we were waiting for such a parser to be designed and so I'm\n",
      "glad it was done here.\n",
      "General comments\n",
      "================\n",
      "\n",
      "The paper presents a joint syntactic and semantic transition-based dependency\n",
      "parser,\n",
      "inspired from the joint parser of Henderson et al. (2008).\n",
      "The authors claim two main differences:\n",
      "- vectorial representations are used for the whole parser's state, instead of\n",
      "the top elements of the stack / the last parser's configurations\n",
      "- the algorithm is a plain greedy search\n",
      "\n",
      "The key idea is to take advantage of stack LSTMs so that the vector\n",
      "representing the state of the parser\n",
      "keeps memory of potentially large scoped syntactic features, which\n",
      "are known to be decisive features for semantic role labeling\n",
      "(such as the path between the predicate and the candidate role filler head).\n",
      "\n",
      "The system is tested on the CoNLL 2008 data set (English) and on the\n",
      "multilingual CoNLL 2009 data set.\n",
      "The authors compare their system's performance to previously reported\n",
      "performances,\n",
      "showing their system does well compared to the 2008 / 2009 systems, \n",
      "but less compared to more recent proposals (cf. bottom of table 3).\n",
      "They emphasized though that the proposed system does not require any hand-craft\n",
      "features,\n",
      "and is fast due to the simple greedy algorithm.\n",
      "\n",
      "The paper is well written and describes a substantial amount of work,\n",
      "building on the recently popular LSTMs, applied to the Henderson et al.\n",
      "algorithm\n",
      "which appears now to have been somewhat visionary.\n",
      "\n",
      "I have reservations concerning the choice of the simple greedy algorithm:\n",
      "it renders results not comparable to some of the cited works.\n",
      "It would not have been too much additional work nor space to provide for\n",
      "instance beam-searched performance.\n",
      "\n",
      "More detailed comments / questions\n",
      "==================================\n",
      "\n",
      "Section 2:\n",
      "\n",
      "A comment on the presence of both A1 and C-A1 links would help understanding\n",
      "better the target task of the paper.\n",
      "\n",
      "A summary of the differences between the set of transitions used in this work\n",
      "and that of Henderson et al. should be provided. In its current form, it is\n",
      "difficult to \n",
      "tell what is directly reused from Henderson et al. and what is new / slightly\n",
      "modified.\n",
      "\n",
      "Section 3.3\n",
      "\n",
      "Why do you need representations concatenating the word predicate and its\n",
      "disambiguated sense,\n",
      "this seems redundant since the disambiguated sense are specific to a predicate\n",
      "?\n",
      "\n",
      "Section 4\n",
      "\n",
      "The organization if the 4.1 / 4.2 sections is confusing concerning\n",
      "multilinguality.\n",
      "Conll 2008 focused on English, and CoNLL 2009 shared task extended it to a few\n",
      "other languages.\n",
      "This paper performs an overdue circling-back to the problem of joint semantic\n",
      "and syntactic dependency parsing, applying the recent insights from neural\n",
      "network models. Joint models are one of the most promising things about the\n",
      "success of transition-based neural network parsers.\n",
      "\n",
      "There are two contributions here. First, the authors present a new transition\n",
      "system, that seems better than the Hendersen (2008) system it is based on. The\n",
      "other contribution is to show that the neural network succeeds on this problem,\n",
      "where linear models had previously struggled. The authors attribute this\n",
      "success to the ability of the neural network to automatically learn which\n",
      "features to extract. However, I think there's another advantage to the neural\n",
      "network here, that might be worth mentioning. In a linear model, you need to\n",
      "learn a weight for each feature/class pair. This means that if you jointly\n",
      "learn two problems, you have to learn many more parameters. The neural network\n",
      "is much more economical in this respect.\n",
      "\n",
      "I suspect the transition-system would work just as well with a variety of other\n",
      "neural network models, e.g. the global beam-search model of Andor (2016). There\n",
      "are many other orthogonal improvements that could be made. I expect extensions\n",
      "to the authors' method to produce state-of-the-art results.\n",
      "\n",
      "It would be nice to see an attempt to derive a dynamic\n",
      "oracle for this transition system, even if it's only in an appendix or in\n",
      "follow-up work. At first glance, it seems similar to the\n",
      "arc-eager oracle. The M-S action excludes all semantic arcs between the word at\n",
      "the start of the buffer and the words on the semantic stack, and the M-D action\n",
      "excludes all semantic arcs between the word at the top of the stack and the\n",
      "words in the buffer. The L and R actions seem to each exclude the reverse arc,\n",
      "and no other.\n",
      "This paper proposed a very interesting idea of using cognitive features for\n",
      "sentiment analysis and sarcasm detection. More specifically, the eye-movement\n",
      "patterns of human annotators are recorded to derive a new set of features. The\n",
      "authors claim that this is the first work to include cognitive features into\n",
      "the NLP community. \n",
      "\n",
      "Strength: \n",
      "1. The paper is generally well written and easy to follow\n",
      "2. Very interesting idea which may inspire research in other NLP tasks.\n",
      "\n",
      "Weakness:\n",
      "1. The motivation of using cognitive features for sentiment analysis is not\n",
      "very well justified. I can imagine these features may help reflect the reading\n",
      "ease, but I don't see why they are helpful in detecting sentiment polarities.\n",
      "2. The improvement is marginal after considering cognitive features by\n",
      "comparing Sn+Sr+Gz with Sn+Sr.\n",
      "3. Although the authors discussed about the feasibility of the approach in\n",
      "Section 7, but I'm not convinced, especially about the example given in section\n",
      "7.2, I don't see why this technique is helpful in such a scenario.\n",
      "This paper is about introducing eye-tracking features for sentiment analysis as\n",
      "a type of cognitive feature.  I think that the idea of introducing eye-tracking\n",
      "features as a proxy for cognitive load for sentiment analysis is an interesting\n",
      "one.  \n",
      "\n",
      "I think the discussion on the features and comparison of feature sets is clear\n",
      "and very helpful.  I also like that the feasibility of the approach is\n",
      "addressed in section 7.\n",
      "\n",
      "I wonder if it would help the evaluation if the datasets didn't conflate\n",
      "different domains, e.g., the movie review corpus and the tweet corpus.             \n",
      "For one\n",
      "it might improve the prediction of movie review (resp. tweets) if the tweets\n",
      "(resp. movie reviews) weren't in the training.              It would also make the\n",
      "results\n",
      "easier to interpret.  The results in Table 2 would seem rather low compared to\n",
      "state-of-the art results for the Pang and Lee data, but look much better if\n",
      "compared to results for Twitter data.\n",
      "\n",
      "In Section 3.3, there are no overlapping snippets in the training data and\n",
      "testing data of datasets 1 and 2, right?  Even if they come from the same\n",
      "sources (e.g., Pang & Lee and Sentiment 140).\n",
      "\n",
      "Minor: some of the extra use of bold is distracting (or maybe it's just me);\n",
      "The authors present a new version of the coreference task tailored to\n",
      "Wikipedia. The task is to identify the coreference chain specifically\n",
      "corresponding to the entity that the Wikipedia article is about.  The authors\n",
      "annotate 30 documents with all coreference chains, of which roughly 25% of the\n",
      "mentions refer to the \"main concept\" of the article. They then describe some\n",
      "simple baselines and a basic classifier which outperforms these. Moreover, they\n",
      "integrate their classifier into the Stanford (rule-based) coreference system\n",
      "and see substantial benefit over all state-of-the-art systems on Wikipedia.\n",
      "\n",
      "I think this paper proposes an interesting twist on coreference that makes good\n",
      "sense from an information extraction perspective, has the potential to somewhat\n",
      "revitalize and shake up coreference research, and might bridge the gap in an\n",
      "interesting way between coreference literature and entity linking literature. \n",
      "I am sometimes unimpressed by papers that dredge up a new task that standard\n",
      "systems perform poorly on and then propose a tweak so that their system does\n",
      "better. However, in this case, the actual task itself is quite motivating to me\n",
      "and rather than the authors fishing for a new domain to run things in, it\n",
      "really does feel like \"hey, wait, these standard systems perform poorly in a\n",
      "setting that's actually pretty important.\"\n",
      "\n",
      "THE TASK: Main concept resolution is an intriguing task from an IE perspective.\n",
      " I can imagine many times where documents revolve primarily around a particular\n",
      "entity (biographical documents, dossiers or briefings about a person or event,\n",
      "clinical records, etc.) and where the information we care about extracting is\n",
      "specific to that entity. The standard coreference task has always had the issue\n",
      "of large numbers of mentions that would seemingly be pretty irrelevant for most\n",
      "IE problems (like generic mentions), and this task is unquestionably composed\n",
      "of mentions that actually do matter.\n",
      "\n",
      "From a methodology standpoint, the notion of a \"main concept\" provides a bit of\n",
      "a discourse anchor that is useful for coreference, but there appears to still\n",
      "be substantial overhead to improve beyond the baselines, particularly on\n",
      "non-pronominal mentions. Doing coreference directly on Wikipedia also opens the\n",
      "doors for more interesting use of knowledge, which the authors illustrate here.\n",
      "So I think this domain is likely to be an interesting testbed for ideas which\n",
      "would improve coreference overall, but which in the general setting would be\n",
      "more difficult to get robust improvements with and which would be dwarfed by\n",
      "the amount of work dealing with other aspects of the problem.\n",
      "\n",
      "Moreover, unlike past work which has carved off a slice of coreference (e.g.\n",
      "the Winograd schema work), this paper makes a big impact on the metrics of the\n",
      "*overall* coreference problem on a domain (Wikipedia) that many in the ACL\n",
      "community are pretty interested in.\n",
      "\n",
      "THE TECHNIQUES: Overall, the techniques are not the strong point of this paper,\n",
      "though they do seem to be effective. The features seem pretty sensible, but it\n",
      "seems like additional conjunctions of these may help (and it's unclear whether\n",
      "the authors did any experimentation in this vein).  The authors should also\n",
      "state earlier in the work that their primary MC resolution system is a binary\n",
      "classifier; this is not explicitly stated early enough and the model is left\n",
      "undefined throughout the description of featurization.\n",
      "\n",
      "MINOR DETAILS:\n",
      "\n",
      "Organization: I would perhaps introduce the dataset immediately after \"Related\n",
      "Works\" (i.e. have it be the new Section 3) so that concrete results can be\n",
      "given in \"Baselines\", further motivating \"Approach\".\n",
      "\n",
      "When Section 4 refers to Dcoref and Scoref, you should cite the Stanford papers\n",
      "or make it clear that it's the Stanford coreference system (many will be\n",
      "unfamiliar with the Dcoref/Scoref names).\n",
      "\n",
      "The use of the term \"candidate list\" was unclear, especially in the following:\n",
      "\n",
      "\"We leverage the hyperlink structure of the article in order to enrich the list\n",
      "of mentions with shallow semantic attributes. For each link found within the\n",
      "article under consideration, we look through the candidate list for all\n",
      "mentions that match the surface string of the link.\"\n",
      "\n",
      "Please make it clear that the \"candidate list\" is the set of mentions in the\n",
      "article that are possible candidates for being coreferent with the MC.        I think\n",
      "most readers will understand that this module is supposed to import semantic\n",
      "information from the link structure of Wikipedia (e.g. if a mention is\n",
      "hyperlinked to an article that is female in Freebase, that mention is female),\n",
      "so try to keep the terminology clear.\n",
      "\n",
      "Section 6.1 says \"we consider the union of WCR mentions and all mentions\n",
      "predicted by the method described in (Raghunathan et al., 2010).\" However,\n",
      "Section 4.1 implies that these are the same? I'm missing where additional WCR\n",
      "mentions would be extracted.\n",
      "This paper proposes an approach for multi-lingual named entity recognition\n",
      "using features from Wikipedia. By relying on a cross-lingual Wikifier, it\n",
      "identifies English Wikipedia articles for phrases in a target language and uses\n",
      "features based on the wikipedia entry. Experiments show that this new feature\n",
      "helps not only in the monolingual case, but also in the more interesting direct\n",
      "transfer setting, where the English model is tested on a target language.\n",
      "\n",
      "I liked this paper. It proposes a new feature for named entity recognition and\n",
      "conducts a fairly thorough set of experiments to show the utility of the\n",
      "feature. The analysis on low resource and the non-latin languages are\n",
      "particularly interesting.\n",
      "\n",
      "But what about named entities that are not on Wikipedia? In addition to the\n",
      "results in the paper, it would be interesting to see results on how these\n",
      "entities are affected by the proposed method. \n",
      "\n",
      "The proposed method is strongly dependent on the success of the cross-lingual\n",
      "wikifier. With this additional step in the pipeline, how often do we get errors\n",
      "in the prediction because of errors in the wikifier?\n",
      "\n",
      "Given the poor performance of direct transfer on Tamil and Bengali when lexical\n",
      "features are added, I wonder if it is possible to regularize the various\n",
      "feature classes differently, so that the model does not become over-reliant on\n",
      "the lexical features.\n",
      "This paper is concerned with cross-lingual direct transfer of NER models using\n",
      "a very recent cross-lingual wikification model. In general, the key idea is not\n",
      "highly innovative and creative, as it does not really propose any core new\n",
      "technology. The contribution is mostly incremental, and marries the two\n",
      "research paths: (1) direct transfer for downstream NLP tasks (such as NER,\n",
      "parsing, or POS tagging), and (2) very recent developments in the cross-lingual\n",
      "wikification technology. However, I pretty much liked the paper, as it is built\n",
      "on a coherent and clear story with enough experiments and empirical evidence to\n",
      "support its claims, with convincing results. I still have several comments\n",
      "concerning the presentation of the work.\n",
      "\n",
      "Related work: a more detailed description in related work on how this paper\n",
      "relates to work of Kazama and Torisawa (2007) is needed. It is also required to\n",
      "state a clear difference with other related NER system that in one way or\n",
      "another relied on the encyclopaedic Wikipedia knowledge. The differences are\n",
      "indeed given in the text, but they have to be further stressed to facilitate\n",
      "reading and placing the work in context. \n",
      "\n",
      "Although the authors argue why they decided to leave out POS tags as features,\n",
      "it would still be interesting to report experiments with POS tags features\n",
      "similar to Tackstrom et al.: the reader might get an overview supported by\n",
      "empirical evidence regarding the usefulness (or its lack) of such features for\n",
      "different languages (i.e., for the languages for which universal POS are\n",
      "available at least). \n",
      "\n",
      "Section 3.3 could contribute from a running example, as I am still not exactly\n",
      "sure how the edited model from Tsai and Roth works now (i.e., the given\n",
      "description is not entirely clear).\n",
      "\n",
      "Since the authors mention several times that the approaches from Tackstrom et\n",
      "al. (2012) and Nothman et al. (2012) are orthogonal to theirs and that they can\n",
      "be combined with the proposed approach, it would be beneficial if they simply\n",
      "reported some preliminary results on a selection of languages using the\n",
      "combination of the models. It will add more flavour to the discussion. Along\n",
      "the same line, although I do acknowledge that this is also orthogonal approach,\n",
      "why not comparing with a strong projection baseline, again to put the results\n",
      "into more even more context, and show the usefulness (or limitations) of\n",
      "wikification-based approaches.\n",
      "\n",
      "Why is Dutch the best training language for Spanish, and Spanish the best\n",
      "language for Yoruba? Only a statistical coincidence or something more\n",
      "interesting is going on there? A paragraph or two discussing these results in\n",
      "more depth would be quite interesting.\n",
      "\n",
      "Although the idea is sound, the results from Table 4 are not that convincing\n",
      "with only small improvements detected (and not in all scenarios). A statistical\n",
      "significance test reported for the results from Table 4 could help support the\n",
      "claims.\n",
      "\n",
      "Minor comments:\n",
      "\n",
      "- Sect. 2.1: Projection can also be performed via methods that do not require\n",
      "parallel data, which makes such models more widely applicable (even for\n",
      "languages that do not have any parallel resources): e.g., see the work of\n",
      "Peirsman and Pado (NAACL 2009) or Vulic and Moens (EMNLP 2013) which exploit\n",
      "bilingual semantic spaces instead of direct alignment links to perform the\n",
      "transfer.\n",
      "\n",
      "- Several typos detected in the text, so the paper should gain quite a bit from\n",
      "a more careful proofreading (e.g., first sentence of Section 3: \"as a the base\n",
      "model\"; This sentence is not 'parsable', Page 3: \"They avoid the traditional\n",
      "pipeline of NER then EL by...\", \"to disambiguate every n-grams\" on Page 8)\n",
      "I reviewed this paper earlier, when it was an ACL 2016 short paper draft. At\n",
      "that point, it had a flaw in the experiment setup, which is now corrected.\n",
      "\n",
      "Since back then I suggested I'd be willing to accept the draft for another *ACL\n",
      "event provided that the flaw is corrected, I now see no obstacles in doing so.\n",
      "\n",
      "Another reviewer did point out that the setup of the paper is somewhat\n",
      "artificial if we focus on real low-resource languages, relating to the costs of\n",
      "*finding* vs. *paying* the annotators. I believe this should be exposed in the\n",
      "writeup not to oversell the method.\n",
      "\n",
      "There are relevant lines of work in annotation projection for extremely\n",
      "low-resource languages, e.g., Johannsen et al. (2016, ACL) and Agic et al.\n",
      "(2015, ACL). It would be nice to reflect on those in the related work\n",
      "discussion for completeness.\n",
      "\n",
      "In summary, I think this is a nice contribution, and I vote accept.\n",
      "\n",
      "It should be indicated whether the data is made available. I evaluate those\n",
      "parts in good faith now, presuming public availability of research.\n",
      "The paper describes a modification to the output layer of recurrent neural\n",
      "network models which enables learning the model parameters from both gold and\n",
      "projected annotations in a low-resource language. The traditional softmax\n",
      "output layer which defines a distribution over possible labels is further\n",
      "multiplied by a fully connected layer which models the noise generation\n",
      "process, resulting in another output layer representing the distribution over\n",
      "noisy labels. \n",
      "\n",
      "Overall, this is a strong submission. The proposed method is apt, simple and\n",
      "elegant. The paper reports good results on POS tagging for eight simulated\n",
      "low-resource languages and two truly low-resource languages, making use of a\n",
      "small set of gold annotations and a large set of cross-lingually projected\n",
      "annotations for training. The method is modular enough that researchers working\n",
      "on different NLP problems in low-resource scenarios are likely to use it.\n",
      "\n",
      "From a practical standpoint, the experimental setup is unusual. While I can\n",
      "think of some circumstances where one needs to build a POS tagger with as\n",
      "little as 1000 token annotations (e.g., evaluations in some DARPA-sponsored\n",
      "research projects), it is fairly rare. A better empirical validation of the\n",
      "proposed method would have been to plot the tagging accuracy of the proposed\n",
      "method (and baselines) while varying the size of gold annotations. This plot\n",
      "would help answer questions such as: Does it hurt the performance on a target\n",
      "language if we use this method while having plenty of gold annotations? What is\n",
      "the amount of gold annotations, approximately, below which this method is\n",
      "beneficial? Does the answer depend on the target language?\n",
      "\n",
      "Beyond cross-lingual projections, noisy labels could potentially be obtained\n",
      "from other sources (e.g., crowd sourcing) and in different tag sets than gold\n",
      "annotations. Although the additional potential impact is exciting, the paper\n",
      "only shows results with cross-lingual projections with the same tag set. \n",
      "\n",
      "It is surprising that the proposed training objective gives equal weights to\n",
      "gold vs. noisy labels. Since the setup assumes the availability of a small gold\n",
      "annotated corpus, it would have been informative to report whether it is\n",
      "beneficial to tune the contribution of the two terms in the objective function.\n",
      "\n",
      "\n",
      "In line 357, the paper describes the projected data as pairs of word tokens\n",
      "(x_t) and their vector representations \\tilde{y}, but does not explicitly\n",
      "mention what the vector representation looks like (e.g., a distribution over\n",
      "cross-lingually projected POS tags for this word type). A natural question to\n",
      "ask here is whether the approach still works if we construct \\tilde{y} using\n",
      "the projected POS tags at the token level (rather than aggregating all\n",
      "predictions for the same word type). Also, since only one-to-one word\n",
      "alignments are preserved, it is not clear how to construct \\tilde{y} for words\n",
      "which are never aligned.\n",
      "\n",
      "Line 267, replace one of the two closing brackets with an opening bracket.\n",
      "General comments\n",
      "=============================\n",
      "The paper reports experiments on predicting the level of compositionality of\n",
      "compounds in English. \n",
      "The dataset used is a previously existing set of 90 compounds, whose\n",
      "compositionality was ranked from 1 to 5\n",
      "(by a non specified number of judges).\n",
      "The general form of each experiment is to compute a cosine similarity between\n",
      "the vector of the compound (treated as one token) and a composition of the\n",
      "vectors of the components.\n",
      "Evaluation is performed using a Spearman correlation between the cosine\n",
      "similarity and the human judgments.\n",
      "\n",
      "The experiments vary\n",
      "- for the vectors used: neural embeddings versus syntactic-context count\n",
      "vectors\n",
      "- and for the latter case, whether plain or \"aligned\" vectors should be used,\n",
      "for the dependent component of the compound. The alignment tries to capture a\n",
      "shift from the dependent to the head. Alignment were proposed in a previous\n",
      "suppressed reference.\n",
      "\n",
      "The results indicate that syntactic-context count vectors outperform\n",
      "embeddings, and the use of aligned alone performs less well than non-modified\n",
      "vectors, and a highly-tuned combination of aligned and unaligned vectors\n",
      "provides a slight improvement.\n",
      "\n",
      "Regarding the form of the paper, I found the introduction quite well written,\n",
      "but other parts (like section 5.1) are difficult to read, although the\n",
      "underlying notions are not very complicated. Rephrasing with running examples\n",
      "could help.\n",
      "\n",
      "Regarding the substance, I have several concerns:\n",
      "\n",
      "- the innovation with respect to Reddy et al. seems to be the use of the\n",
      "aligned vectors\n",
      "but they have been published in a previous \"suppressed reference\" by the\n",
      "authors.\n",
      "\n",
      "- the dataset is small, and not enough described. In particular, ranges of\n",
      "frequences are quite likely to impact the results. \n",
      "Since the improvements using aligned vectors are marginal, over a small\n",
      "dataset, in which it is unclear how the choice of the compounds was performed,\n",
      "I find that the findings in the paper are quite fragile.\n",
      "\n",
      "More detailed comments/questions\n",
      "================================\n",
      "\n",
      "Section 3\n",
      "\n",
      "I don't understand the need for the new name \"packed anchored tree\".\n",
      "It seems to me a plain extraction of the paths between two lexical items in a\n",
      "dependency tree,\n",
      "namely a plain extension of what is traditionally done in syntactic\n",
      "distributional representations of words\n",
      "(which typically (as far as Lin 98) use paths of length one, or length 2, with\n",
      "collapsed prepositions).\n",
      "\n",
      "Further, why is it called a tree? what are \"elementary APTs\" (section 5.1) ?\n",
      "\n",
      "Table 2 : didn't you forget to mention that you discard features of order more\n",
      "than 3 \n",
      "(and that's why for instance NMOD.overline(NSUBJ).DOBJ does not appear in\n",
      "leftmost bottom cell of table 2\n",
      "Or does it have to do with the elimination of some incompatible types you\n",
      "mention\n",
      "(for which an example should be provided, I did not find it very clear).\n",
      "\n",
      "Section 4:\n",
      "\n",
      "Since the Reddy et al. dataset is central to your work, it seems necessary to\n",
      "explain how the 90 compounds were selected. What are the frequency ranges of\n",
      "the compounds / the components etc... ? There is a lot of chance that results\n",
      "vary depending on the frequency ranges.\n",
      "\n",
      "How many judgments were provided for a given compound? Are there many compounds\n",
      "with same final compositionality score? Isn't it a problem when ranking them to\n",
      "compute the Spearman correlation ?\n",
      "\n",
      "Apparently you use \"constituent\" for a component of the N N sequence. I would\n",
      "suggest \"component\", as \"constituent\" also has the sense of \"phrase\" (syntagm).\n",
      "\n",
      "\"... the intuition that if a constituent is used literally within a phrase then\n",
      "it is highly likely that the compound and the constituent share co-occurrences\"\n",
      ": note the intuition is certainly true if the constituent is the head of the\n",
      "phrase, otherwise much less true (e.g. \"spelling bee\" does not have the\n",
      "distribution of \"spelling\").\n",
      "\n",
      "Section 5\n",
      "\n",
      "\"Note that the elementary representation for the constituent of a compound\n",
      "phrase will not contain any of the contextual features associated with the\n",
      "compound phrase token unless they occurred with the constituent in some other\n",
      "context. \"\n",
      "Please provide a running example in order to help the reader follow which\n",
      "object you're talking about.\n",
      "Does \"compound phrase token\" refer to the merged components of the compound?\n",
      "\n",
      "Section 5.1\n",
      "\n",
      "I guess that \"elementary APTs\" are a triplet target word w + dependency path r\n",
      "+ other word w'?\n",
      "I find the name confusing.\n",
      "\n",
      "Clarify whether \"shifted PMI\" refer to PMI as defined in equation (3).\n",
      "\n",
      "\"Removing features which tend to go with lots of\n",
      " things (low positive PMI) means that these phrases\n",
      " appear to have been observed in a very small num-\n",
      " ber of (highly informative) contexts.\"\n",
      "Do \"these phrases\" co-refer with \"things\" here?\n",
      "The whole sentence seems contradictory, please clarify.\n",
      "\n",
      "\"In general, we would expect there to be little 558\n",
      "overlap between APTs which have not been prop-\n",
      "erly aligned.\"\n",
      "What does \"not properly aligned\" means? You mean not aligned at all?\n",
      "\n",
      "I don't understand paragraph 558 to 563.\n",
      "Why should the potential overlap be considerable\n",
      "in the particular case of the NMOD relation between the two components?\n",
      "\n",
      "Paragraph 575 to 580 is quite puzzling.\n",
      "Why does the whole paper make use of higher order dependency features\n",
      "and then suddenly, at the critical point of actually measuring the crucial\n",
      "metric\n",
      "of similarity between composed and observed phrasal vectors, you use\n",
      "first order features only?\n",
      "\n",
      "Note 3 is supposed to provide an answer, but I don't understand the explanation\n",
      "of why the 2nd order paths in the composed representations are not reliable,\n",
      "please clarify.\n",
      "\n",
      "Section 6\n",
      "\n",
      "\"Smoothing the PPMI calculation with a value of Î± = 0.75 generally has a 663\n",
      "small positive effect.\"\n",
      "does not seem so obvious from table 3.\n",
      "\n",
      "What are the optimal values for h and q in equation 8 and 9? They are important\n",
      "in order to estimate\n",
      "how much of \"hybridity\" provides the slight gains with respect to the unaligned\n",
      "results.\n",
      "\n",
      "It seems that in table 4 results correspond to using the add combination, it\n",
      "could help to have this in the legend.\n",
      "Also, couldn't you provide the results from the word2vec vectors for the\n",
      "compound phrases?\n",
      "\n",
      "I don't understand the intuition behind the FREQ baseline. Why would a frequent\n",
      "compound tend to be compositional? This suggests maybe a bias in the dataset.\n",
      "This paper proposes the new (to my knowledge) step of proposing to treat a\n",
      "number of sentence pair scoring tasks (e.g. Answer Set Scoring, RTE,\n",
      "Paraphrasing,\n",
      "among others) as instances of a more general task of understanding semantic\n",
      "relations\n",
      "between two sentences. Furthermore, they investigate the potential of learning\n",
      "generally-\n",
      "applicable neural network models for the family of tasks. I find this to be an\n",
      "exciting\n",
      "proposal that's worthy of both presentation at CoNLL and further discussion and\n",
      "investigation.\n",
      "\n",
      "The main problem I have with the paper is that it in fact feels unfinished. It\n",
      "should be\n",
      "accepted for publication only with the proviso that a number of updates will be\n",
      "made\n",
      "for the final version:\n",
      "1 - the first results table needs to be completed\n",
      "2 - given the large number of individual results, the written discussion of\n",
      "results\n",
      "is terribly short. Much more interpretation and discussion of the results is\n",
      "sorely needed.\n",
      "3 - the abstract promises presentation of a new, more challenging dataset which\n",
      "the paper\n",
      "does not seem to deliver. This incongruity needs to be resolved.\n",
      "4 - the results vary quite a bit across different tasks - could some\n",
      "investigation be made into\n",
      "how and why the models fail for some of the tasks, and how and why they succeed\n",
      "for others?\n",
      "Even if no solid answer is found, it would be interesting to hear the authors'\n",
      "position regarding\n",
      "whether this is a question of modeling or rather dissimilarity between the\n",
      "tasks. Does it really\n",
      "work to group them into a unified whole?\n",
      "5 - please include example instances of the various datasets used, including\n",
      "both prototypical\n",
      "sentence pairs and pairs which pose problems for classification\n",
      "6 - the Ubu. RNN transfer learning model is recommended for new tasks, but is\n",
      "this because\n",
      "of the nature of the data (is it a more general task) or rather the size of the\n",
      "dataset? How can\n",
      "we determine an answer to that question?\n",
      "\n",
      "Despite the unpolished nature of the paper, though, it's an exciting approach\n",
      "that\n",
      "could generate much interesting discussion, and I'd be happy to see it\n",
      "published\n",
      "IN A MORE FINISHED FORM.\n",
      "I do recognize that this view may not be shared by other reviewers!\n",
      "\n",
      "Some minor points about language:\n",
      "* \"weigh\" and \"weighed\" are consistently used in contexts that rather require\n",
      "\"weight\" and\n",
      "\"weighted\"\n",
      "* there are several misspellings of \"sentence\" (as \"sentene\")\n",
      "* what is \"interpunction\"?\n",
      "* one instance of \"world overlap\" instead of \"word overlap\"\n",
      "This paper describes a new deterministic dependency parsing algorithm and\n",
      "analyses its behaviour across a range of languages.\n",
      "The core of the algorithm is a set of rules defining permitted dependencies\n",
      "based on POS tags.\n",
      "The algorithm starts by ranking words using a slightly biased PageRank over a\n",
      "graph with edges defined by the permitted dependencies.\n",
      "Stepping through the ranking, each word is linked to the closest word that will\n",
      "maintain a tree and is permitted by the head rules and a directionality\n",
      "constraint.\n",
      "\n",
      "Overall, the paper is interesting and clearly presented, though seems to differ\n",
      "only slightly from Sogaard (2012), \"Unsupervised Dependency Parsing without\n",
      "Training\".\n",
      "I have a few questions and suggestions:\n",
      "\n",
      "Head Rules (Table 1) - It would be good to have some analysis of these rules in\n",
      "relation to the corpus.\n",
      "For example, in section 3.1 the fact that they do not always lead to a\n",
      "connected graph is mentioned, but not how frequently it occurs, or how large\n",
      "the components typically are.\n",
      "\n",
      "I was surprised that head direction was chosen using the test data rather than\n",
      "training or development data.\n",
      "Given how fast the decision converges (10-15 sentences), this is not a major\n",
      "issue, but a surprising choice.\n",
      "\n",
      "How does tie-breaking for words with the same PageRank score work?\n",
      "Does it impact performance significantly, or are ties rare enough that it\n",
      "doesn't have an impact?\n",
      "\n",
      "The various types of constraints (head rules, directionality, distance) will\n",
      "lead to upper bounds on possible performance of the system.\n",
      "It would be informative to include oracle results for each constraint, to show\n",
      "how much they hurt the maximum possible score.\n",
      "That would be particularly helpful for guiding future work in terms of where to\n",
      "try to modify this system.\n",
      "\n",
      "Minor:\n",
      "\n",
      "- 4.1, \"we obtain [the] rank\"\n",
      "\n",
      "- Table 5 and Table 7 have columns in different orders. I found the Table 7\n",
      "arrangement clearer.\n",
      "\n",
      "- 6.1, \"isolate the [contribution] of both\"\n",
      "The authors proposed an unsupervised algorithm for Universal Dependencies that\n",
      "does not require training. The tagging is based on PageRank for the words and a\n",
      "small amount of hard-coded rules.\n",
      "The article is well written, very detailed and the intuition behind all prior\n",
      "information being added to the model is explained clearly.\n",
      "I think that the contribution is substantial to the field of unsupervised\n",
      "parsing, and the possibilities for future work presented by the authors give\n",
      "rise to additional research.\n",
      "This paper presents a way to parse trees (namely the universal dependency\n",
      "treebanks) by relying only on POS and by using a modified version of the\n",
      "PageRank to give more way to some meaningful words (as opposed to stop words).\n",
      "\n",
      "This idea is interesting though very closed to what was done in SÃ¸gaard\n",
      "(2012)'s paper. The personalization factor giving more weight to the main\n",
      "predicate is nice but it would have been better to take it to the next level.\n",
      "As far as I can tell, the personalization is solely used for the main predicate\n",
      "and its weight of 5 seems arbitrary.\n",
      "\n",
      "Regarding the evaluation and the detailed analyses, some charts would have been\n",
      "beneficial, because it is sometimes hard to get the gist out of the tables.\n",
      "Finally, it would have been interesting to get the scores of the POS tagging in\n",
      "the prediction mode to be able to see if the degradation in parsing performance\n",
      "is heavily correlated to the degradation in tagging performance (which is what\n",
      "we expect).\n",
      "\n",
      "All in all, the paper is interesting but the increment over the work of\n",
      "SÃ¸gaard (2012) is small.\n",
      "\n",
      "Smaller issues:\n",
      "-------------------\n",
      "\n",
      "l. 207 : The the main idea -> The main idea\n",
      "This paper models event linking using CNNs. Given event mentions, the authors\n",
      "generate vector representations based on word embeddings passed through a CNN\n",
      "and followed by max-pooling. They also concatenate the resulting\n",
      "representations with several word embeddings around the mention. Together with\n",
      "certain pairwise features, they produce a vector of similarities using a\n",
      "single-layer neural network, and compute a coreference score. \n",
      "The model is tested on an ACE dataset and an expanded version with performance\n",
      "comparable to previous feature-rich systems.\n",
      "The main contribution of the paper, in my opinion, is in developing a neural\n",
      "approach for entity linking that combines word embeddings with several\n",
      "linguistic features. It is interesting to find out that just using the word\n",
      "embeddings is not sufficient for good performance. Fortunately, the linguistic\n",
      "features used are limited and do not require manually-crafted external\n",
      "resources.  \n",
      "\n",
      "Experimental setting\n",
      "- It appears that gold trigger words are used rather than predicted ones. The\n",
      "authors make an argument why this is reasonable, although I still would have\n",
      "liked to see performance with predicted triggers. This is especially\n",
      "problematic as one of the competitor systems used predicted triggers, so the\n",
      "comparison isn't fair. \n",
      "- The fact that different papers use different train/test splits is worrisome.\n",
      "I would encourage the authors to stick to previous splits as much as possible. \n",
      "\n",
      "Unclear points\n",
      "- The numbers indicating that cross-sentential information is needed are\n",
      "convincing. However, the last statement in the second paragraph (lines 65-70)\n",
      "was not clear to me.\n",
      "- Embeddings for positions are said to be generaties \"in a way similar to word\n",
      "embeddings\". How exactly? Are they randomly initialized? Are they lexicalized?\n",
      "It is not clear to me why a relative position next to one word should have the\n",
      "same embedding as a relative position next to a different word.\n",
      "- How exactly are left vs right neighbors used to create the representation\n",
      "(lines 307-311)? Does this only affect the max-pooling operation?\n",
      "- The word embeddings of one word before and one word after the trigger words\n",
      "are appended to it. This seems a bit arbitrary. Why one word before and after\n",
      "and not some other choice?  \n",
      "- It is not clear how the event-mention representation v_e (line 330) is used?\n",
      "In the following sections only v_{sent+lex} appear to be used, not v_e.\n",
      "- How are pairwise features used in section 3.2? Most features are binary, so I\n",
      "assume they are encoded as a binary vector, but what about the distance feature\n",
      "for example? And, are these kept fixed during training?\n",
      "\n",
      "Other issues and suggestions\n",
      "- Can the approach be applied to entity coreference resolution as well? This\n",
      "would allow comparing with more previous work and popular datasets like\n",
      "OntoNotes. \n",
      "- The use of a square function as nonlinearity is interesting. Is it novel? Do\n",
      "you think it has applicability in other tasks?\n",
      "- Datasets: one dataset is publicly available, but results are also presented\n",
      "with ACE++, which is not. Do you have plans to release it? It would help other\n",
      "researchers compare new methods. At least, it would have been good to see a\n",
      "comparison to the feature-rich systems also on this dataset.\n",
      "- Results: some of the numbers reported in the results are quite close.\n",
      "Significance testing would help substantiating the comparisons.\n",
      "- Related work: among the work on (entity) coreference resolution, one might\n",
      "mention the neural network approach by Wiseman et al. (2015)  \n",
      "\n",
      "Minor issues\n",
      "- line 143, \"that\" is redundant. \n",
      "- One of the baselines is referred to as \"same type\" in table 6, but \"same\n",
      "event\" in the text (line 670).        \n",
      "\n",
      "Refs\n",
      "- Learning Anaphoricity and Antecedent Ranking Features for Coreference\n",
      "Resolution. Sam Wiseman, Alexander M. Rush, Jason Weston, and Stuart M.\n",
      "Shieber. ACL 2015.\n",
      "This paper presents a model for the task of event entity linking, where they\n",
      "propose to use sentential features from CNNs in place of external knowledge\n",
      "sources which earlier methods have used. They train a two-part model: the first\n",
      "part learns an event mention representation, and the second part learns to\n",
      "calculate a coreference score given two event entity mentions.\n",
      "\n",
      "The paper is well-written, well-presented and is easy to follow. I rather like\n",
      "the analysis done on the ACE corpus regarding the argument sharing between\n",
      "event coreferences. Furthermore, the analysis on the size impact of the\n",
      "dataset is a great motivation for creating their ACE++ dataset. However, there\n",
      "are a few\n",
      "major issues that need to be addressed:\n",
      "\n",
      "- The authors fail to motivate and analyze the pros and cons of using CNN for\n",
      "generating mention representations. It is not discussed why they chose CNN and\n",
      "there are no comparisons to the other models (e.g., straightforwardly an RNN).\n",
      "Given that the improvement their model makes according various metrics against\n",
      "the\n",
      "state-of-the-art is only 2 or 3 points on F1 score, there needs to be more\n",
      "evidence that this architecture is indeed superior.\n",
      "\n",
      "- It is not clear what is novel about the idea of tackling event linking with\n",
      "sentential features, given that using CNN in this fashion for a classification\n",
      "task is not new. The authors could explicitly point out and mainly compare to\n",
      "any existing continuous space methods for event linking. The choice of methods\n",
      "in Table 3 is not thorough enough.\n",
      "\n",
      "- There is no information regarding how the ACE++ dataset is collected. A major\n",
      "issue with the ACE dataset is its limited number of event types, making it too\n",
      "constrained and biased. It is important to know what event types ACE++ covers.\n",
      "This can also help support the claim in Section 5.1 that 'other approaches are\n",
      "strongly tied to the domain where these semantic features are availableâ¦our\n",
      "approach does not depend on resources with restrictedâ¦', you need to show\n",
      "that those earlier methods fail on some dataset that you succeed on. Also,\n",
      "for enabling any meaningful comparison in future, the authors should think\n",
      "about making this dataset publicly available.\n",
      "\n",
      "Some minor issues:\n",
      "- I would have liked to see the performance of your model without gold\n",
      "references in Table 3 as well.\n",
      "\n",
      "- It would be nice to explore how this model can or cannot be augmented with a\n",
      "vanilla coreference resolution system. For the specific example in line 687,\n",
      "the off-the-shelf CoreNLP system readily links 'It' to 'bombing', which can be\n",
      "somehow leveraged in an event entity linking baseline.\n",
      "\n",
      "- Given the relatively small size of the ACE dataset, I think having a\n",
      "compelling model requires testing on the other available resources as well.\n",
      "This further motivates working on entity and event coreference simultaneously.\n",
      "I also believe that testing on EventCorefBank in parallel with ACE is\n",
      "essential. \n",
      "\n",
      "- Table 5 shows that the pairwise features have been quite effective, which\n",
      "signals that feature engineering may still be crucial for having a competitive\n",
      "model (at least on the scale of the ACE dataset). One would wonder which\n",
      "features were the most effective, and why not report how the current set was\n",
      "chosen and what else was tried.\n",
      "- Strengths:\n",
      "\n",
      "The authors propose a kernel-based method that captures high-order patterns\n",
      "differentiting different types of rumors by evaluating the similarities between\n",
      "their propagation tree structures.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "maybe the maths is not always clear in Sect. 4. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The authors propose a propagation tree kernel, a kernel-based method that\n",
      "captures high-order patterns differentiating types of rumors by evaluating the\n",
      "similarities between their propagation tree structures. The proposed approach\n",
      "detects rumors more quickly and with a higher accuracy compared to the one\n",
      "obtained by the state of the art methods.\n",
      "\n",
      "The data set should be made public for research purposes.\n",
      "\n",
      "Typos need to be fixed (e.g. 326/3277: any subgraph which have->has; 472:\n",
      "TPK->PTK; 644: Table 2 show+s), missing information needs to be added (875:\n",
      "where was it published?), information needs to be in the same format (e.g. 822\n",
      "vs 897). Figure 5 is a bit small.\n",
      "- Strengths:\n",
      "i. Well organized and easy to understand\n",
      "ii. Provides detailed comparisons under various experimental settings and shows\n",
      "the state-of-the-art performances\n",
      "\n",
      "- Weaknesses:\n",
      "i. In experiments, this paper compares previous supervised approaches, but the\n",
      "proposed method is the semi-supervised approach even if the training data is\n",
      "enough to train.\n",
      "\n",
      "- General Discussion:\n",
      "This paper adopts a pre-training approach to improve Chinese word segmentation.\n",
      "Based on the transition-based neural word segmentation, this paper aims to\n",
      "pre-train incoming characters with external resources (punctuation, soft\n",
      "segmentation, POS, and heterogeneous training data) through multi-task\n",
      "learning. That is, this paper casts each external source as an auxiliary\n",
      "classification task. The experimental results show that the proposed method\n",
      "achieves the state-of-the-art performances in six out of seven datasets. \n",
      "\n",
      "This paper is well-written and easy to understand. A number of experiments\n",
      "prove the effectiveness of the proposed method. However, there exist an issue\n",
      "in this paper. The proposed method is a semi-supervised learning that uses\n",
      "external resources to pre-train the characters. Furthermore, this paper uses\n",
      "another heterogeneous training datasets even if it uses the datasets only for\n",
      "pre-training. Nevertheless, the baselines in the experiments are based on\n",
      "supervised learning. In general, the performance of semi-supervised learning is\n",
      "better than that of supervised learning because semi-supervised learning makes\n",
      "use of plentiful auxiliary information. In the experiments, this paper should\n",
      "have compared the proposed method with semi-supervised approaches.\n",
      "\n",
      "POST AUTHOR RESPONSE\n",
      "\n",
      "What the reviewer concerned is that this paper used additional\n",
      "“gold-labeled” dataset to pretrain the character embeddings. Some baselines\n",
      "in the experiments used label information, where the labels are predicted\n",
      "automatically by their base models as the authors pointed out. When insisting\n",
      "superiority of a method, all circumstances should be same. Thus, even if the\n",
      "gold dataset isn’t used to train the segmentation model directly, it seems to\n",
      "me that it is an unfair comparison because the proposed method used another\n",
      "“gold” dataset to train the character embeddings.\n",
      "- Strengths:\n",
      "\n",
      "The paper demonstrates that seq2seq models can be comparatively effectively\n",
      "applied to the tasks of AMR parsing and AMR realization by linearization of an\n",
      "engineered pre-processed version of the AMR graph and associated sentence,\n",
      "combined with 'Paired Training' (iterative back-translation of monolingual data\n",
      "combined with fine-tuning). While parsing performance is worse than other\n",
      "reported papers (e.g., Pust et al., 2015), those papers used additional\n",
      "semantic information. \n",
      "\n",
      "On the task of AMR realization, the paper demonstrates that utilizing\n",
      "additional monolingual data (via back-translation) is effective relative to a\n",
      "seq2seq model that does not use such information. (See note below about\n",
      "comparing realization results to previous non-seq2seq work for the realization\n",
      "task.)\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      " At a high-level, the main weakness is that the paper aims for empirical\n",
      "comparisons, but in comparing to other work, multiple aspects/dimensions are\n",
      "changing at the same time (in some cases, not comparable due to access to\n",
      "different information), complicating comparisons. \n",
      "\n",
      "For example, with the realization results (Table 2), PBMT (Pourdamghani et al.,\n",
      "2016) is apparently trained on LDC2014T12, which consists of 13,051 sentences,\n",
      "compared to the model of the paper, which is trained on LDC2015E86, which\n",
      "consists of 19,572 sentences, according to http://amr.isi.edu/download.html.\n",
      "This is used in making the claim of over 5 points improvement over the\n",
      "state-of-the-art (PBMT) in line 28/29, 120/121, and line 595, and is only\n",
      "qualified in the caption of Table 2. To make a valid comparison, the approach\n",
      "of the paper or PBMT needs to be re-evaluated after using the same training\n",
      "data.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "Is there any overlap between the sentences in your Gigaword sample and the test\n",
      "sentences of LDC2015E86? Apparently LDC2015E86 contains data from the ''proxy\n",
      "report data in LDC's DEFT Narrative Text Source Data R1 corpus (LDC2013E19)''\n",
      "(Accessible with LDC account: https://catalog.ldc.upenn.edu/LDC2015E86). It\n",
      "seems LDC2013E19 contains data from Gigaword\n",
      "(https://catalog.ldc.upenn.edu/LDC2013E19). Apparently AMR corpus LDC2014T12\n",
      "also contained ''data from newswire articles selected from the English Gigaword\n",
      "Corpus, Fifth Edition'' (publicly accessible link:\n",
      "https://catalog.ldc.upenn.edu/docs/LDC2014T12/README.txt). Please check that\n",
      "there is no test set contamination.\n",
      "\n",
      "Line 244-249: Did these two modifications to the encoder make a significant\n",
      "difference in effectiveness? What was the motivation behind these changes?\n",
      "\n",
      "Please make it clear (in an appendix is fine) for replication purposes whether\n",
      "the implementation is based on an existing seq2seq framework.\n",
      "\n",
      "Line 321: What was the final sequence length used? (Consider adding such\n",
      "details in an appendix.)\n",
      "\n",
      "Please label the columns of Table 1 (presumably dev and test). Also, there is a\n",
      "mismatch between Table 1 and the text: ''Table 1 summarizes our development\n",
      "results for different rounds of self-training.'' It appears that only the\n",
      "results of the second round of self-training are shown.\n",
      "\n",
      "Again, the columns for Table 1 are not labeled, but should the results for\n",
      "column 1 for CAMR instead be 71.2, 63.9, 67.3--the last line of Table 2 in\n",
      "http://www.aclweb.org/anthology/S16-1181 which is the configuration for\n",
      "+VERB+RNE+SRL+WIKI? It looks like the second from last row of Table 2 in CAMR\n",
      "(Wang et al., 2016) is currently being used. On this note, how does your\n",
      "approach handle the wikification information introduced in LDC2015E86? \n",
      "\n",
      "7.1.Stochastic is missing a reference to the example.\n",
      "\n",
      "Line 713-715: This seems like a hypothesis to be tested empirically rather than\n",
      "a forgone conclusion, as implied here.\n",
      "\n",
      "Given an extra page, please add a concluding section.\n",
      "\n",
      "How are you performing decoding? Are you using beam search?\n",
      "\n",
      "As a follow-up to line 161-163, it doesn't appear that the actual vocabulary\n",
      "size used in the experiments is mentioned. After preprocessing, are there any\n",
      "remaining unseen tokens in dev/test? In other words, is the unknown word\n",
      "replacement mechanism (using the attention weights), as described in Section\n",
      "3.2, ever used? \n",
      "\n",
      "For the realization case study, it would be of interest to see performance on\n",
      "phenomena that are known limitations of AMR, such as quantification and tense\n",
      "(https://github.com/amrisi/amr-guidelines/blob/master/amr.md).\n",
      "\n",
      "The paper would benefit from a brief discussion (perhaps a couple sentences)\n",
      "motivating the use of AMR as opposed to other semantic formalisms, as well as\n",
      "why the human-annotated AMR information/signal might be useful as opposed to\n",
      "learning a model (e.g., seq2seq itself) directly for a task (e.g., machine\n",
      "translation).\n",
      "\n",
      "For future work (not taken directly into account in the scores given here for\n",
      "the review, since the applicable paper is not yet formally published in the\n",
      "EACL proceedings): For parsing, what accounts for the difference from previous\n",
      "seq2seq approaches? Namely, between Peng and Xue, 2017 and AMR-only (as in\n",
      "Table 1) is the difference in effectiveness being driven by the architecture,\n",
      "the preprocessing, linearization, data, or some combination thereof? Consider\n",
      "isolating this difference. (Incidentally, the citation for Peng and Xue, 2017\n",
      "[''Addressing the Data Sparsity Issue in Neural AMR Parsing''] should\n",
      "apparently be Peng et al. 2017\n",
      "(http://eacl2017.org/index.php/program/accepted-papers;\n",
      "https://arxiv.org/pdf/1702.05053.pdf). The authors are flipped in the\n",
      "References section.\n",
      "\n",
      "Proofreading (not necessarily in the order of occurrence; note that these are\n",
      "provided for reference and did not influence my scoring of the paper):\n",
      "\n",
      "outperform state of the art->outperform the state of the art\n",
      "\n",
      "Zhou et al. (2016), extend->Zhou et al. (2016) extend\n",
      "\n",
      "(2016),Puzikov et al.->(2016), Puzikov et al.\n",
      "\n",
      "POS-based features, that->POS-based features that\n",
      "\n",
      "language pairs, by creating->language pairs by creating\n",
      "\n",
      "using a back-translation MT system and mix it with the human\n",
      "translations.->using a back-translation MT system, and mix it with the human\n",
      "translations.\n",
      "\n",
      "ProbBank-style (Palmer et al., 2005)->PropBank-style (Palmer et al., 2005)\n",
      "\n",
      "independent parameters ,->independent parameters,\n",
      "\n",
      "for the 9.6% of tokens->for 9.6% of tokens\n",
      "\n",
      "maintaining same embedding sizes->maintaining the same embedding sizes\n",
      "\n",
      "Table 4.Similar->Table 4. Similar\n",
      "\n",
      "realizer.The->realizer. The\n",
      "\n",
      "Notation: Line 215, 216: The sets C and W are defined, but never subsequently\n",
      "referenced. (However, W could/should be used in place of ''NL'' in line 346 if\n",
      "they are referring to the same vocabulary.)\n",
      "The authors use self-training to train a seq2seq-based AMR parser using a small\n",
      "annotated corpus and large amounts of unlabeled data. They then train a\n",
      "similar,\n",
      "seq2seq-based AMR-to-text generator using the annotated corpus and automatic\n",
      "AMRs produced by their parser from the unlabeled data. They use careful\n",
      "delexicalization for named entities in both tasks to avoid data sparsity. This\n",
      "is the first sucessful application of seq2seq models to AMR parsing and\n",
      "generation, and for generation, it most probably improves upon state-of-the\n",
      "art.\n",
      "\n",
      "In general, I really liked the approach as well as the experiments and the\n",
      "final performance analysis.\n",
      "The methods used are not revolutionary, but they are cleverly combined to\n",
      "achieve practial results.\n",
      "The description of the approach is quite detailed, and I believe that it is\n",
      "possible to reproduce the experiments without significant problems.\n",
      "The approach still requires some handcrafting, but I believe that this can be\n",
      "overcome in the future and that the authors are taking a good direction.\n",
      "\n",
      "(RESOLVED BY AUTHORS' RESPONSE) However, I have been made aware by another\n",
      "reviewer of a data overlap in the\n",
      "Gigaword and the Semeval 2016 dataset. This is potentially a very serious\n",
      "problem -- if there is a significant overlap in the test set, this would\n",
      "invalidate the results for generation (which are the main achievemnt of the\n",
      "paper). Unless the authors made sure that no test set sentences made their way\n",
      "to training through Gigaword, I cannot accept their results.\n",
      "\n",
      "(RESOLVED BY AUTHORS' RESPONSE)  Another question raised by another reviewer,\n",
      "which I fully agree with, is the \n",
      "5.4 point claim when comparing to a system tested on an earlier version of the\n",
      "AMR dataset. The paper could probably still claim improvement over state-of-the\n",
      "art, but I am not sure I can accept the 5.4 points claim in a direct comparison\n",
      "to Pourdamghani et al. -- why haven't the authors also tested their system on\n",
      "the older dataset version (or obtained Pourdamghani et al.'s scores for the\n",
      "newer version)?\n",
      "\n",
      "Otherwise I just have two minor comments to experiments: \n",
      "\n",
      "- Statistical significance tests would be advisable (even if the performance\n",
      "difference is very big for generation).\n",
      "\n",
      "- The linearization order experiment should be repeated with several times with\n",
      "different random seeds to overcome the bias of the particular random order\n",
      "chosen.\n",
      "\n",
      "The form of the paper definitely could be improved.\n",
      "The paper is very dense at some points and proofreading by an independent\n",
      "person (preferably an English native speaker) would be advisable. \n",
      "The model (especially the improvements over Luong et al., 2015) could be\n",
      "explained in more detail; consider adding a figure. The experiment description\n",
      "is missing the vocabulary size used.\n",
      "Most importantly, I missed a formal conclusion very much -- the paper ends\n",
      "abruptly after qualitative results are described, and it doesn't give a final\n",
      "overview of the work or future work notes.\n",
      "\n",
      "Minor factual notes:\n",
      "\n",
      "- Make it clear that you use the JAMR aligner, not the whole parser (at\n",
      "361-364). Also, do you not use the recorded mappings also when testing the\n",
      "parser (366-367)?\n",
      "\n",
      "- Your non-Gigaword model only improves on other seq2seq models by 3.5 F1\n",
      "points, not 5.4 (at 578).\n",
      "\n",
      "- \"voters\" in Figure 1 should be \"person :ARG0-of vote-01\" in AMR.\n",
      "\n",
      "Minor writing notes:\n",
      "\n",
      "- Try rewording and simplifying text near 131-133, 188-190, 280-289, 382-385,\n",
      "650-659, 683, 694-695.\n",
      "\n",
      "- Inter-sentitial punctuation is sometimes confusing and does not correspond to\n",
      "my experience with English syntax. There are lots of excessive as well as\n",
      "missing commas.\n",
      "\n",
      "- There are a few typos (e.g., 375, 615), some footnotes are missing full\n",
      "stops.\n",
      "\n",
      "- The linearization description is redundant at 429-433 and could just refer to\n",
      "Sect. 3.3.\n",
      "\n",
      "- When refering to the algorithm or figures (e.g., near 529, 538, 621-623),\n",
      "enclose the references in brackets rather than commas.\n",
      "\n",
      "- I think it would be nice to provide a reference for AMR itself and for the\n",
      "multi-BLEU script.\n",
      "\n",
      "- Also mention that you remove AMR variables in Footnote 3.\n",
      "\n",
      "- Consider renaming Sect. 7 to \"Linearization Evaluation\".\n",
      "\n",
      "- The order in Tables 1 and 2 seems a bit confusing to me, especially when your\n",
      "systems are not explicitly marked (I would expect your systems at the bottom).\n",
      "Also, Table 1 apparently lists development set scores even though its\n",
      "description says otherwise.\n",
      "\n",
      "- The labels in Table 3 are a bit confusing (when you read the table before\n",
      "reading the text).\n",
      "\n",
      "- In Figure 2, it's not entirely visible that you distinguish month names from\n",
      "month numbers, as you state at 376.\n",
      "\n",
      "- Bibliography lacks proper capitalization in paper titles, abbreviations and\n",
      "proper names should be capitalized (use curly braces to prevent BibTeX from\n",
      "lowercasing everything).\n",
      "\n",
      "- The \"Peng and Xue, 2017\" citation is listed improperly, there are actually\n",
      "four authors.\n",
      "\n",
      "***\n",
      "Summary:\n",
      "\n",
      "The paper presents first competitive results for neural AMR parsing and\n",
      "probably new state-of-the-art for AMR generation, using seq2seq models with\n",
      "clever\n",
      "preprocessing and exploiting large a unlabelled corpus. Even though revisions\n",
      "to the text are advisable, I liked the paper and would like to see it at the\n",
      "conference. \n",
      "\n",
      "(RESOLVED BY AUTHORS' RESPONSE) However, I am not sure if the comparison with\n",
      "previous\n",
      "state-of-the-art on generation is entirely sound, and most importantly, whether\n",
      "the good results are not actually caused by data overlap of Gigaword\n",
      "(additional training set) with the test set.\n",
      "\n",
      "***\n",
      "Comments after the authors' response:\n",
      "\n",
      "I thank the authors for addressing both of the major problems I had with the\n",
      "paper. I am happy with their explanation, and I raised my scores assuming that\n",
      "the authors will reflect our discussion in the final paper.\n",
      "- Strengths:\n",
      "\n",
      "- nice, clear application of linguistics ideas to distributional semantics\n",
      "- demonstrate very clear improvements on both intrinsic and extrinsic eval\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "- fairly straightforward extension of existing retrofitting work\n",
      "- would be nice to see some additional baselines (e.g. character embeddings)\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The paper describes \"morph-fitting\", a type of retrofitting for vector spaces\n",
      "that focuses specifically on incorporating morphological constraints into the\n",
      "vector space. The framework is based on the idea of \"attract\" and \"repel\"\n",
      "constraints, where attract constraints are used to pull morphological\n",
      "variations close together (e.g. look/looking) and repel constraints are used to\n",
      "push derivational antonyms apart (e.g. responsible/irresponsible). They test\n",
      "their algorithm on multiple different vector spaces and several language, and\n",
      "show consistent improvements on intrinsic evaluation (SimLex-999, and\n",
      "SimVerb-3500). They also test on the extrinsic task of dialogue state tracking,\n",
      "and again demonstrate measurable improvements over using\n",
      "morphologically-unaware word embeddings.\n",
      "\n",
      "I think this is a very nice paper. It is a simple and clean way to incorporate\n",
      "linguistic knowledge into distributional models of semantics, and the empirical\n",
      "results are very convincing. I have some questions/comments below, but nothing\n",
      "that I feel should prevent it from being published.\n",
      "\n",
      "- Comments for Authors\n",
      "\n",
      "1) I don't really understand the need for the morph-simlex evaluation set. It\n",
      "seems a bit suspect to create a dataset using the same algorithm that you\n",
      "ultimately aim to evaluate. It seems to me a no-brainer that your model will do\n",
      "well on a dataset that was constructed by making the same assumptions the model\n",
      "makes. I don't think you need to include this dataset at all, since it is a\n",
      "potentially erroneous evaluation that can cause confusion, and your results are\n",
      "convincing enough on the standard datasets.\n",
      "\n",
      "2) I really liked the morph-fix baseline, thank you for including that. I would\n",
      "have liked to see a baseline based on character embeddings, since this seems to\n",
      "be the most fashionable way, currently, to side-step dealing with morphological\n",
      "variation. You mentioned it in the related work, but it would be better to\n",
      "actually compare against it empirically.\n",
      "\n",
      "3) Ideally, we would have a vector space where morphological variants are just\n",
      "close together, but where we can assign specific semantics to the different\n",
      "inflections. Do you have any evidence that the geometry of the space you end\n",
      "with is meaningful. E.g. does \"looking\" - \"look\" + \"walk\" = \"walking\"? It would\n",
      "be nice to have some analysis that suggests the morphfitting results in a more\n",
      "meaningful space, not just better embeddings.\n",
      "The authors propose ‘morph-fitting’, a method that retrofits any given set\n",
      "of trained word embeddings based on a morphologically-driven objective that (1)\n",
      "pulls inflectional forms of the same word together (as in ‘slow’ and\n",
      "‘slowing’) and (2) pushes derivational antonyms apart (as in\n",
      "‘expensive’ and ‘inexpensive’). With this, the authors aim to improve\n",
      "the representation of low-frequency inflections of words as well as mitigate\n",
      "the tendency of corpus-based word embeddings to assign similar representations\n",
      "to antonyms. The method is based on relatively simple manually-constructed\n",
      "morphological rules and is demonstrated on both English, German, Italian and\n",
      "Russian. The experiments include intrinsic word similarity benchmarks, showing\n",
      "notable performance improvements achieved by applying morph-fitting to several\n",
      "different corpus-based embeddings. Performance improvement yielding new\n",
      "state-of-the-art results is also demonstrated for German and Italian on an\n",
      "extrinsic task - dialog state tracking. \n",
      "\n",
      "Strengths:\n",
      "\n",
      "- The proposed method is simple and shows nice performance improvements across\n",
      "a number of evaluations and in several languages. Compared to previous\n",
      "knowledge-based retrofitting approaches (Faruqui et al., 2015), it relies on a\n",
      "few manually-constructed rules, instead of a large-scale knowledge base, such\n",
      "as an ontology.\n",
      "\n",
      "- Like previous retrofitting approaches, this method is easy to apply to\n",
      "existing sets of embeddings and therefore it seems like the software that the\n",
      "authors intend to release could be useful to the community.\n",
      "\n",
      "- The method and experiments are clearly described. \n",
      "\n",
      "Weaknesses:\n",
      "\n",
      "- I was hoping to see some analysis of why the morph-fitted embeddings worked\n",
      "better in the evaluation, and how well that corresponds with the intuitive\n",
      "motivation of the authors. \n",
      "\n",
      "- The authors introduce a synthetic word similarity evaluation dataset,\n",
      "Morph-SimLex. They create it by applying their presumably\n",
      "semantic-meaning-preserving morphological rules to SimLex999 to generate many\n",
      "more pairs with morphological variability. They do not manually annotate these\n",
      "new pairs, but rather use the original similarity judgements from SimLex999.\n",
      "The obvious caveat with this dataset is that the similarity scores are presumed\n",
      "and therefore less reliable. Furthermore, the fact that this dataset was\n",
      "generated by the very same rules that are used in this work to morph-fit word\n",
      "embeddings, means that the results reported on this dataset in this work should\n",
      "be taken with a grain of salt. The authors should clearly state this in their\n",
      "paper.\n",
      "\n",
      "- (Soricut and Och, 2015) is mentioned as a future source for morphological\n",
      "knowledge, but in fact it is also an alternative approach to the one proposed\n",
      "in this paper for generating morphologically-aware word representations. The\n",
      "authors should present it as such and differentiate their work.\n",
      "\n",
      "- The evaluation does not include strong morphologically-informed embedding\n",
      "baselines. \n",
      "\n",
      "General Discussion:\n",
      "\n",
      "With the few exceptions noted, I like this work and I think it represents a\n",
      "nice contribution to the community. The authors presented a simple approach and\n",
      "showed that it can yield nice improvements using various common embeddings on\n",
      "several evaluations and four different languages. I’d be happy to see it in\n",
      "the conference.\n",
      "\n",
      "Minor comments:\n",
      "\n",
      "- Line 200: I found this phrasing unclear: “We then query … of linguistic\n",
      "constraints”.\n",
      "\n",
      "- Section 2.1: I suggest to elaborate a little more on what the delta is\n",
      "between the model used in this paper and the one it is based on in Wieting\n",
      "2015. It seemed to me that this was mostly the addition of the REPEL part.\n",
      "\n",
      "- Line 217: “The method’s cost function consists of three terms” - I\n",
      "suggest to spell this out in an equation.\n",
      "\n",
      "- Line 223:  x and t in this equation (and following ones) are the vector\n",
      "representations of the words. I suggest to denote that somehow. Also, are the\n",
      "vectors L2-normalized before this process? Also, when computing ‘nearest\n",
      "neighbor’ examples do you use cosine or dot-product? Please share these\n",
      "details.\n",
      "\n",
      "- Line 297-299: I suggest to move this text to Section 3, and make the note\n",
      "that you did not fine-tune the params in the main text and not in a footnote.\n",
      "\n",
      "- Line 327: (create, creates) seems like a wrong example for that rule. \n",
      "\n",
      "* I have read the author response\n",
      "This paper addresses the network embedding problem by introducing a neural\n",
      "network model which uses both the network structure and associated text on the\n",
      "nodes, with an attention model to vary the textual representation based on the\n",
      "text of the neighboring nodes.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "The model leverages both the network and the text to construct the latent\n",
      "representations, and the mutual attention approach seems sensible.\n",
      "\n",
      "A relatively thorough evaluation is provided, with multiple datasets,\n",
      "baselines, and evaluation tasks.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "Like many other papers in the \"network embedding\" literature, which use neural\n",
      "network techniques inspired by word embeddings to construct latent\n",
      "representations of nodes in a network, the previous line of work on\n",
      "statistical/probabilistic modeling of networks is ignored.  In particular, all\n",
      "\"network embedding\" papers need to start citing, and comparing to, the work on\n",
      "the latent space model of Peter Hoff et al., and subsequent papers in both\n",
      "statistical and probabilistic machine learning publication venues:\n",
      "\n",
      "P.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social\n",
      "network analysis. J. Amer. Statist. Assoc., 97(460):1090–1098, 2002.\n",
      "\n",
      "This latent space network model, which embeds each node into a low-dimensional\n",
      "latent space, was written as far back as 2002, and so it far pre-dates neural\n",
      "network-based network embeddings.\n",
      "\n",
      "Given that the aim of this paper is to model differing representations of\n",
      "social network actors' different roles, it should really cite and compare to\n",
      "the mixed membership stochastic blockmodel (MMSB):\n",
      "\n",
      "Airoldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed\n",
      "membership stochastic blockmodels. Journal of Machine Learning Research.\n",
      "\n",
      "The MMSB allows each node to randomly select a different \"role\" when deciding\n",
      "whether to form each edge.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The aforementioned statistical models do not leverage text, and they do not use\n",
      "scalable neural network implementations based on negative sampling, but they\n",
      "are based on well-principled generative models instead of heuristic neural\n",
      "network objective functions and algorithms.  There are more recent extensions\n",
      "of these models and inference algorithms which are more scalable, and which do\n",
      "leverage text.\n",
      "\n",
      "Is the difference in performance between CENE and CANE in Figure 3\n",
      "statistically insignificant? (A related question: were the experiments repeated\n",
      "more than once with random train/test splits?)\n",
      "\n",
      "Were the grid searches for hyperparameter values, mentioned in Section 5.3,\n",
      "performed with evaluation on the test set (which would be problematic), or on a\n",
      "validation set, or on the training set?\n",
      "- Strengths:\n",
      "This paper tries to use the information from arguments, which is usually\n",
      "ignored yet actually quite important, to improve the performance of event\n",
      "detection. The framework is clear and simple. With the help of the supervised\n",
      "attention mechanism, an important method that has been used in many tasks such\n",
      "as machine translation, the performance of their system outperforms the\n",
      "baseline significantly.\n",
      "\n",
      "- Weaknesses:\n",
      " The attention vector is simply the summation of two attention vectors of each\n",
      "part. Maybe the attention vector could be calculated in a more appropriate\n",
      "approach. For the supervised attention mechanism, two strategies are proposed.\n",
      "Both of them are quite straightforward. Some more complicated strategies can\n",
      "work better and can be tried.\n",
      "\n",
      "- General Discussion:\n",
      " Although there are some places that can be improved, this paper proposed a\n",
      "quite effective framework, and the performance is good. The experiment is\n",
      "solid. It can be considered to be accepted.\n",
      "This paper proposes to use an encoder-decoder framework for keyphrase\n",
      "generation. Experimental results show that the proposed model outperforms other\n",
      "baselines if supervised data is available.\n",
      "\n",
      "- Strengths:\n",
      "The paper is well-organized and easy to follow (the intuition of the proposed\n",
      "method is clear). It includes enough details to replicate experiments. Although\n",
      "the application of an encoder-decoder (+ copy mechanism) is straightforward,\n",
      "experimental results are reasonable and support the claim (generation of absent\n",
      "keyphrases) presented in this paper.\n",
      "\n",
      "- Weaknesses:\n",
      "As said above, there is little surprise in the proposed approach. Also, as\n",
      "described in Section 5.3, the trained model does not transfer well to new\n",
      "domain (it goes below unsupervised models). One of the contribution of this\n",
      "paper is to maintain training corpora in good quantity and quality, but it is\n",
      "not (explicitly) stated.\n",
      "\n",
      "- General Discussion:\n",
      "I like to read the paper and would be pleased to see it accepted. I would like\n",
      "to know how the training corpus (size and variation) affects the performance of\n",
      "the proposed method. Also, it would be beneficial to see the actual values of\n",
      "p_g and p_c (along with examples in Figure 1) in the CopyRNN model. From my\n",
      "experience in running the CopyNet, the copying mechanism sometimes works\n",
      "unexpectedly (not sure why this happens).\n",
      "This paper divides the keyphrases into two types: (1) Absent key phrases (such\n",
      "phrases do not match any contiguous subsequences of the source document) and\n",
      "(2) Present key phrases (such key phrases fully match a part of the text). The\n",
      "authors used RNN based generative models (discussed as RNN and Copy RNN) for\n",
      "keyphrase prediction and copy mechanism in RNN to predict the already occurred\n",
      "phrases. \n",
      "\n",
      "Strengths:\n",
      "\n",
      "1. The formation and extraction of key phrases, which are absent in the current\n",
      "document is an interesting idea of significant research interests. \n",
      "\n",
      "2. The paper is easily understandable.\n",
      "\n",
      "3. The use of RNN and Copy RNN in the current context is a new idea. As, deep\n",
      "recurrent neural networks are already used in keyphrase extraction (shows very\n",
      "good performance also), so, it will be interesting to have a proper motivation\n",
      "to justify the use of  RNN and Copy RNN over deep recurrent neural networks. \n",
      "\n",
      "Weaknesses:\n",
      "\n",
      "1. Some discussions are required on the convergence of the proposed joint\n",
      "learning process (for RNN and CopyRNN), so that readers can understand, how the\n",
      "stable points in probabilistic metric space are obtained? Otherwise, it may be\n",
      "tough to repeat the results.\n",
      "\n",
      "2. The evaluation process shows that the current system (which extracts 1.\n",
      "Present and 2. Absent both kinds of keyphrases) is evaluated against baselines\n",
      "(which contains only \"present\" type of keyphrases). Here there is no direct\n",
      "comparison of the performance of the current system w.r.t. other\n",
      "state-of-the-arts/benchmark systems on only \"present\" type of key phrases. It\n",
      "is important to note that local phrases (keyphrases) are also important for the\n",
      "document. The experiment does not discuss it explicitly. It will be interesting\n",
      "to see the impact of the RNN and Copy RNN based model on automatic extraction\n",
      "of local or \"present\" type of key phrases.\n",
      "\n",
      "3. The impact of document size in keyphrase extraction is also an important\n",
      "point. It is found that the published results of [1], (see reference below)\n",
      "performs better than (with a sufficiently high difference) the current system\n",
      "on Inspec (Hulth, 2003) abstracts dataset. \n",
      "\n",
      "4. It is reported that current system uses 527,830 documents for training,\n",
      "while 40,000 publications are held out for training baselines. Why are all\n",
      "publications not used in training the baselines? Additionally,        The topical\n",
      "details of the dataset (527,830 scientific documents) used in training RNN and\n",
      "Copy RNN are also missing. This may affect the chances of repeating results.\n",
      "\n",
      "5. As the current system captures the semantics through RNN based models. So,\n",
      "it would be better to compare this system, which also captures semantics. Even,\n",
      "Ref-[2] can be a strong baseline to compare the performance of the current\n",
      "system.\n",
      "\n",
      "Suggestions to improve:\n",
      "\n",
      "1. As, per the example, given in the Figure-1, it seems that all the \"absent\"\n",
      "type of key phrases are actually \"Topical phrases\". For example: \"video\n",
      "search\", \"video retrieval\", \"video indexing\" and \"relevance ranking\", etc.\n",
      "These all define the domain/sub-domain/topics of the document. So, In this\n",
      "case, it will be interesting to see the results (or will be helpful in\n",
      "evaluating \"absent type\" keyphrases): if we identify all the topical phrases of\n",
      "the entire corpus by using tf-idf and relate the document to the high-ranked\n",
      "extracted topical phrases (by using Normalized Google Distance, PMI, etc.). As\n",
      "similar efforts are already applied in several query expansion techniques (with\n",
      "the aim to relate the document with the query, if matching terms are absent in\n",
      "document).\n",
      "\n",
      "Reference:\n",
      "1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to\n",
      "find exemplar terms for keyphrase extraction. In Proceedings of the 2009\n",
      "Conference on Empirical Methods in Natural Language Processing, pages\n",
      "257–266.\n",
      "\n",
      "2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction\n",
      "using deep recurrent neural networks on Twitter. In Proceedings of the 2016\n",
      "Conference on Empirical Methods in Natural Language Processing (pp. 836-845).\n",
      "- Strengths:\n",
      "\n",
      "Novel model.  I particularly like the ability to generate keyphrases not\n",
      "present in the source text.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      " Needs to be explicit whether all evaluated models are trained and tested on\n",
      "the same data sets.  Exposition of the copy mechanism not quite\n",
      "clear/convincing.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "This paper presents a supervised neural network approach for keyphrase\n",
      "generation.  The model uses an encoder-decoder architecture that\n",
      "first encodes input text with a RNN, then uses an attention mechanism to\n",
      "generate keyphrases from\n",
      "the hidden states.  There is also a more advanced variant of the\n",
      "decoder which has an attention mechanism that conditions on the\n",
      "keyphrase generated in the previous time step.\n",
      "\n",
      "The model is interesting and novel. And I think the ability to\n",
      "generate keyphrases not in the source text is particularly\n",
      "appealing.  My main concern is with the evaluation:  Are all\n",
      "evaluated models trained with the same amount of data and evaluated\n",
      "on the same test sets?              It's not very clear.  For example, on the\n",
      "NUS data set, Section 4.2 line 464 says that the supervised baselines\n",
      "are evaluated with cross validation.\n",
      "\n",
      "Other comments:\n",
      "\n",
      "The paper is mostly clearly written and easy to follow.  However,\n",
      "some parts are unclear:\n",
      "\n",
      "- Absent keyphrases vs OOV.  I think there is a need to distinguish\n",
      "  between the two, and the usage meaning of OOV should be consistent.  The RNN\n",
      "models\n",
      "  use the most frequent 50000 words as the vocabulary (Section 3.4\n",
      "  line 372, Section 5.1 line 568), so I suppose OOV are words not in\n",
      "  this 50K vocabulary.              In line 568, do you mean OOV or absent\n",
      "  words/keyphrases?  Speaking of this, I'm wondering how many\n",
      "  keyphrases fall outside of this 50K?              The use of \"unknown words\"\n",
      "  in line 380 is also ambiguous.  I think it's probably clearer to say that\n",
      " the RNN models can generate words not present in the source text as long as\n",
      "they appear\n",
      "somewhere else in the corpus (and the 50K vocabulary)\n",
      "\n",
      "- Exposition of the copy mechanism (section 3.4).  This mechanism has a\n",
      "  more specific locality than the attention model in basic RNN model.\n",
      "  However, I find the explanation of the intuition misleading.              If I\n",
      "  understand correctly, the \"copy mechanism\" is conditioned on the\n",
      "  source text locations that matches the keyphrase in the previous\n",
      "  time step y_{t-1}.  So maybe it has a higher tendency to generate n-grams\n",
      "seen source text (Figure 1).  I buy the argument that the more sophisticated\n",
      "  attention model probably makes CopyRNN better than the RNN\n",
      "  overall, but why is the former model particularly better for absent\n",
      "  keyphrases?  It is as if both models perform equally well on present\n",
      "keyphrases.\n",
      "\n",
      "- How are the word embeddings initialized?\n",
      "- Strengths:\n",
      "\n",
      "1) This paper proposed a semi-automated framework (human generation -> auto\n",
      "expansion -> human post-editing) to construct a compositional\n",
      "semantic similarity evaluation data set.\n",
      "\n",
      "2) The proposed framework is used to create a Polish compositional semantic\n",
      "similarity evaluation data set which is useful for future work in developing\n",
      "Polish compositional semantic models.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "1) The proposed framework has only been tested on one language. It is not clear\n",
      "whether the framework is portable to other languages. For example, the proposed\n",
      "framework relies on a dependency parser which may not be available in some\n",
      "languages or in poor performance in some other languages.\n",
      "\n",
      "2) The number of sentence pairs edited by leader judges is not reported so the\n",
      "correctness and efficiency of the automatic expansion framework can not be\n",
      "evaluated. The fact that more than 3% (369 out of 10k) of the post-edited pairs\n",
      "need further post-editing is worrying. \n",
      "\n",
      "3) There are quite a number of grammatical mistakes. Here are some examples but\n",
      "not the complete and exhaustive list:\n",
      "\n",
      "line 210, 212, 213: \"on a displayed image/picture\" -> \"in a displayed\n",
      "image/picture\"\n",
      "\n",
      "line 428: \"Similarly as in\" -> \"Similar to\"\n",
      "\n",
      "A proofread pass on the paper is needed.\n",
      "\n",
      "- General Discussion:\n",
      "- Strengths:\n",
      " * Elaborate evaluation data creation and evaluation scheme.\n",
      " * Range of compared techniques: baseline/simple/complex\n",
      "\n",
      "- Weaknesses:\n",
      " * No in-depth analysis beyond overall evaluation results.\n",
      "\n",
      "- General Discussion:\n",
      "This paper compares several techniques for robust HPSG parsing.\n",
      "\n",
      "Since the main contribution of the paper is not a novel parsing technique but\n",
      "the empirical evaluation, I would like to see a more in-depth analysis of the\n",
      "results summarized in Table 1 and 2.\n",
      "It would be nice to show some representative example sentences and sketches of\n",
      "its analyses, on which the compared methods behaved differently.\n",
      "\n",
      "Please add EDM precision and recall figures to Table 2.\n",
      "The EDM F1 score is a result of a mixed effects of (overall and partial)\n",
      "coverage, parse ranking, efficiency of search, etc.\n",
      "The overall coverage figures in Table 1 are helpful but addition of EDM recall\n",
      "to Table 2 would make the situations clearer.\n",
      "\n",
      "Minor comment:\n",
      "- Is 'pacnv+ut' in Table 1 and 2 the same as 'pacnv' described in 3.4.3?\n",
      "- Strengths:\n",
      "\n",
      "Well-written.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "Although the title and abstract of the paper suggest that robust parsing\n",
      "methods for HPSG are being compared, the actual comparison is limited to only a\n",
      "few techniques applied to a single grammar, the ERG (where in the past the \n",
      "choice has been made to create a treebank for only those sentences that are in\n",
      "the coverage of the grammar). Since the ERG is quite idiosyncratic in this\n",
      "respect, I fear that the paper is not interesting for researchers working in\n",
      "other precision grammar frameworks.\n",
      "\n",
      "The paper lacks comparison with robustness techniques that are routinely\n",
      "applied for systems based on other precision grammars such as various systems\n",
      "based on CCG, LFG, the Alpage system for French, Alpino for Dutch and there is\n",
      "probably more. In the same spirit, there is a reference for supertagging to\n",
      "Dridan 2013 which is about supertagging for ERG whereas supertagging for other\n",
      "precision grammar systems has been proposed at least a decade earlier.\n",
      "\n",
      "The paper lacks enough detail to make the results replicable. Not only are\n",
      "various details not spelled out (e.g. what are those limits on resource\n",
      "allocation), but perhaps more importantly, for some of the techniques that are\n",
      "being compared (eg the robust unification), and for the actual evaluation\n",
      "metric, the paper refers to another paper that is still in preparation.\n",
      "\n",
      "The actual results of the various techniques are somewhat disappointing. With\n",
      "the exception of the csaw-tb method, the resulting parsing speed is extreme -\n",
      "sometimes much slower than the baseline method - where the baseline method is a\n",
      "method in which the standard resource limitations do not apply. The csaw-tb\n",
      "method is faster but not very accurate, and in any case it is not a method\n",
      "introduced in this paper but an existing PCFG approximation technique.\n",
      "\n",
      "It would be (more) interesting to have an idea of the results on a\n",
      "representative dataset (consisting of both sentences that are in the coverage\n",
      "of the grammar and those that are not). In that case, a comparison with the\n",
      "\"real\" baseline system (ERG with standard settings) could be obtained.\n",
      "\n",
      "Methodological issue: the datasets semcor and wsj00ab consist of sentences\n",
      "which an older version of ERG could not parse, but a newer version could. For\n",
      "this reason, the problems in these two datasets are clearly very much biased.\n",
      "It is no suprise therefore that the various techniques obtain much better\n",
      "results on those datasets. But to this reviewer, those results are somewhat\n",
      "meaningless. \n",
      "\n",
      "minor:\n",
      "\n",
      "EDM is used before explained\n",
      "\n",
      "\"reverseability\"\n",
      "\n",
      "- General Discussion:\n",
      "- Strengths:\n",
      "\n",
      "- technique for creating dataset for evaluation of out-of-coverage items, that\n",
      "could possibly be used to evaluation other grammars as well. \n",
      "- the writing in this paper is engaging, and clear (a pleasant surprise, as\n",
      "compared to the typical ACL publication.)\n",
      "\n",
      "- Weaknesses:\n",
      "- The evaluation datasets used are small and hence results are not very\n",
      "convincing (particularly wrt to the alchemy45 dataset on which the best results\n",
      "have been obtained)\n",
      "- It is disappointing to see only F1 scores and coverage scores, but virtually\n",
      "no deeper analysis of the results. For instance, a breakdown by type of\n",
      "error/type of grammatical construction would be interesting. \n",
      "- it is still not clear to this reviewer what is the proportion of out of\n",
      "coverage items due to various factors (running out of resources,  lack of\n",
      "coverage for \"genuine\" grammatical constructions in the long tail, lack of\n",
      "coverage due to extra-grammatical factors like interjections, disfluencies,\n",
      "lack of lexical coverage, etc. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "This paper address the problem of \"robustness\" or lack of coverage for a\n",
      "hand-written HPSG grammar (English Resource Grammar). The paper compares\n",
      "several approaches for increasing coverage, and also presents two creative ways\n",
      "of obtaining evaluation datasets (a non-trivial issue due to the fact that gold\n",
      "standard evaluation data is by definition available only for in-coverage\n",
      "inputs). \n",
      "\n",
      "Although hand-written precision grammars have been very much out of fashion\n",
      "for a long time now and have been superseded by statistical treebank-based\n",
      "grammars, it is important to continue research on these in my opinion. The\n",
      "advantages of high precision and deep semantic analysis provided by these\n",
      "grammars has not been\n",
      "reproduced by non-handwritten grammars as yet. For this reason, I am giving\n",
      "this paper a score of 4, despite the shortcomings mentioned above.\n",
      "This work showed that word representation learning can benefit from sememes\n",
      "when used in an appropriate attention scheme. Authors hypothesized that sememes\n",
      "can act as an essential regularizer for WRL and WSI tasks and proposed SE-WL\n",
      "model which detects word senses and learn representations simultaneously.\n",
      "Though experimental results indicate that WRL benefits, exact gains for WSI are\n",
      "unclear since a qualitative case study of a couple of examples has only been\n",
      "done. Overall, paper is well-written and well-structured.\n",
      "\n",
      "In the last paragraph of introduction section, authors tried to tell three\n",
      "contributions of this work. (1) and (2) are more of novelties of the work\n",
      "rather than contributions. I see the main contribution of the work to be the\n",
      "results which show that we can learn better word representations (unsure about\n",
      "WSI) by modeling sememe information than other competitive baselines. (3) is\n",
      "neither a contribution nor a novelty.\n",
      "\n",
      "The three strategies tried for SE-WRL modeling makes sense and can be\n",
      "intuitively ranked in terms of how well they will work. Authors did a good job\n",
      "explaining that and experimental results supported the intuition but the\n",
      "reviewer also sees MST as a fourth strategy rather than a baseline inspired by\n",
      "Chen et al. 2014 (many WSI systems assume one sense per word given a context).\n",
      "MST many times performed better than SSA and SAC. Unless authors missed to\n",
      "clarify otherwise, MST seems to be exactly like SAT with a difference that\n",
      "target word is represented by the most probable sense rather than taking an\n",
      "attention weighted average over all its senses. MST is still an attention based\n",
      "scheme where sense with maximum attention weight is chosen though it has not\n",
      "been clearly mentioned if target word is represented by chosen sense embedding\n",
      "or some function of it.\n",
      "\n",
      "Authors did not explain the selection of datasets for training and evaluation\n",
      "tasks. Reference page to Sogou-T text corpus did not help as reviewer does not\n",
      "know Chinese language. It was unclear which exact dataset was used as there are\n",
      "several datasets mentioned on that page. Why two word similarity datasets were\n",
      "used and how they are different  (like does one has more rare words than\n",
      "another) since different models performed differently on these datasets. The\n",
      "choice of these datasets did not allow evaluating against results of other\n",
      "works which makes the reviewer wonder about next question.\n",
      "\n",
      "Are proposed SAT model results state of the art for Chinese word similarity? \n",
      "E.g. Schnabel et al. (2015) report a score of 0.640 on WordSim-353 data by\n",
      "using CBOW word embeddings.\n",
      "\n",
      "Reviewer needs clarification on some model parameters like vocabulary sizes for\n",
      "words (Does Sogou-T contains 2.7 billion unique words) and word senses (how\n",
      "many word types from HowNet). Because of the notation used it is not clear if\n",
      "embeddings for senses and sememes for different words were shared. Reviewer\n",
      "hopes that is the case but then why 200 dimensional embeddings were used for\n",
      "only 1889 sememes. It would be better if complexity of model parameters can\n",
      "also be discussed.\n",
      "\n",
      "May be due to lack of space but experiment results discussion lack insight into\n",
      "observations other than SAT performing the best. Also, authors claimed that\n",
      "words with lower frequency were learned better with sememes without evaluating\n",
      "on a rare words dataset.\n",
      "\n",
      "I have read author's response.\n",
      "- Strengths:\n",
      "\n",
      "This paper proposes the use of HowNet to enrich embedings. The idea is\n",
      "interesting and gives good results.\n",
      "\n",
      "- Weaknesses:\n",
      "The paper is interesting, but I am not sure the contibution is important enough\n",
      "for a long paper. Also, the comparision with other works may not be fair:\n",
      "authors should compare to other systems that use manually developed resources.\n",
      "\n",
      "The paper is understandable, but it would help some improvement on the English.\n",
      "\n",
      "- General Discussion:\n",
      "- Strengths:\n",
      "\n",
      "1. The proposed models are shown to lead to rather substantial and consistent\n",
      "improvements over reasonable baselines on two different tasks (word similarity\n",
      "and word analogy), which not only serves to demonstrate the effectiveness of\n",
      "the models but also highlights the potential utility of incorporating sememe\n",
      "information from available knowledge resources for improving word\n",
      "representation learning.\n",
      "2. The paper contributes to ongoing efforts in the community to account for\n",
      "polysemy in word representation learning. It builds nicely on previous work and\n",
      "proposes some new ideas and improvements that could be of interest to the\n",
      "community, such as applying an attention scheme to incorporate a form of soft\n",
      "word sense disambiguation into the learning procedure.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "1. Presentation and clarity: important details with respect to the proposed\n",
      "models are left out or poorly described (more details below). Otherwise, the\n",
      "paper generally reads fairly well; however, the manuscript would need to be\n",
      "improved if accepted.\n",
      "2. The evaluation on the word analogy task seems a bit unfair given that the\n",
      "semantic relations are explicitly encoded by the sememes, as the authors\n",
      "themselves point out (more details below).\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "1. The authors stress the importance of accounting for polysemy and learning\n",
      "sense-specific representations. While polysemy is taken into account by\n",
      "calculating sense distributions for words in particular contexts in the\n",
      "learning procedure, the evaluation tasks are entirely context-independent,\n",
      "which means that, ultimately, there is only one vector per word -- or at least\n",
      "this is what is evaluated. Instead, word sense disambiguation and sememe\n",
      "information are used for improving the learning of word representations. This\n",
      "needs to be clarified in the paper.\n",
      "2. It is not clear how the sememe embeddings are learned and the description of\n",
      "the SSA model seems to assume the pre-existence of sememe embeddings. This is\n",
      "important for understanding the subsequent models. Do the SAC and SAT models\n",
      "require pre-training of sememe embeddings?\n",
      "3. It is unclear how the proposed models compare to models that only consider\n",
      "different senses but not sememes. Perhaps the MST baseline is an example of\n",
      "such a model? If so, this is not sufficiently described (emphasis is instead\n",
      "put on soft vs. hard word sense disambiguation). The paper would be stronger\n",
      "with the inclusion of more baselines based on related work.\n",
      "4. A reasonable argument is made that the proposed models are particularly\n",
      "useful for learning representations for low-frequency words (by mapping words\n",
      "to a smaller set of sememes that are shared by sets of words). Unfortunately,\n",
      "no empirical evidence is provided to test the hypothesis. It would have been\n",
      "interesting for the authors to look deeper into this. This aspect also does not\n",
      "seem to explain the improvements much since, e.g., the word similarity data\n",
      "sets contain frequent word pairs.\n",
      "5. Related to the above point, the improvement gains seem more attributable to\n",
      "the incorporation of sememe information than word sense disambiguation in the\n",
      "learning procedure. As mentioned earlier, the evaluation involves only the use\n",
      "of context-independent word representations. Even if the method allows for\n",
      "learning sememe- and sense-specific representations, they would have to be\n",
      "aggregated to carry out the evaluation task.\n",
      "6. The example illustrating HowNet (Figure 1) is not entirely clear, especially\n",
      "the modifiers of \"computer\".\n",
      "7. It says that the models are trained using their best parameters. How exactly\n",
      "are these determined? It is also unclear how K is set -- is it optimized for\n",
      "each model or is it randomly chosen for each target word observation? Finally,\n",
      "what is the motivation for setting K' to 2?\n",
      "- Strengths:\n",
      "i. Motivation is well described.\n",
      "ii. Provides detailed comparisons with various models across diverse languages\n",
      "\n",
      "- Weaknesses:\n",
      "i.          The conclusion is biased by the selected languages. \n",
      "ii.           The experiments do not cover the claim of this paper completely.\n",
      "\n",
      "- General Discussion:\n",
      "This paper issues a simple but fundamental question about word representation:\n",
      "what subunit of a word is suitable to represent morphologies and how to compose\n",
      "the units. To answer this question, this paper applied word representations\n",
      "with various subunits (characters, character-trigram, and morphs) and\n",
      "composition functions (LSTM, CNN, and a simple addition) to the language\n",
      "modeling task to find the best combination. In addition, this paper evaluated\n",
      "the task for more than 10 languages. This is because languages are\n",
      "typologically diverse and the results can be different according to the word\n",
      "representation and composition function. From their experimental results, this\n",
      "paper concluded that character-level representations are more effective, but\n",
      "they are still imperfective in comparing them with a model with explicit\n",
      "knowledge of morphology. Another conclusion is that character-trigrams show\n",
      "reliable perplexity in the majority of the languages. \n",
      "\n",
      "However, this paper leaves some issues behind.\n",
      "-         First of all, there could be some selection bias of the experimental\n",
      "languages. This paper chose ten languages in four categories (up to three\n",
      "languages per a category). But, one basic question with the languages is “how\n",
      "can it be claimed that the languages are representatives of each category?”\n",
      "All the languages in the same category have the same tendency of word\n",
      "representation and composition function? How can it be proved? For instance,\n",
      "even in this paper, two languages belonging to the same typology\n",
      "(agglutinative) show different results. Therefore, at least to me, it seems to\n",
      "be better to focus on the languages tested in this paper instead of drawing a\n",
      "general conclusions about all languages. \n",
      "-         There is some gap between the claim and the experiments. Is the\n",
      "language modeling the best task to prove the claim of this paper? Isn’t there\n",
      "any chance that the claim of this paper breaks in other tasks? Further\n",
      "explanation on this issue is needed.\n",
      "-         In Section 5.2, this paper evaluated the proposed method only for\n",
      "Arabic. Is there any reason why the experiment is performed only for Arabic?\n",
      "There are plenty of languages with automatic morphological analyzers such as\n",
      "Japanese and Turkish.\n",
      "-         This paper considers only character-trigram among various n-grams. Is\n",
      "there any good reason to choose only character-trigram? Is it always better\n",
      "than character-bigram or character-fourgram? In general, language modeling with\n",
      "n-grams is affected by corpus size and some other factors. \n",
      "\n",
      "Minor typos: \n",
      "- There is a missing reference in Introduction. (88 line in Page 1)\n",
      "- root-and-patter -> root-and-pattern (524 line in Page 6)\n",
      "tldr: The authors compare a wide variety of approaches towards sub-word\n",
      "modelling in language modelling, and show that modelling morphology gives the\n",
      "best results over modelling pure characters. Further, the authors do some\n",
      "precision experiments to show that the biggest benefit towards sub-word\n",
      "modelling is gained after words typically exhibiting rich morphology (nouns and\n",
      "verbs). The paper is comprehensive and the experiments justify the core claims\n",
      "of the paper. \n",
      "\n",
      "- Strengths:\n",
      "\n",
      "1) A comprehensive overview of different approaches and architectures towards\n",
      "sub-word level modelling, with numerous experiments designed to support the\n",
      "core claim that the best results come from modelling morphemes.\n",
      "\n",
      "2) The authors introduce a novel form of sub-word modelling based on character\n",
      "tri-grams and show it outperforms traditional approaches on a wide variety of\n",
      "languages.\n",
      "\n",
      "3) Splitting the languages examined by typology and examining the effects of\n",
      "the models on various typologies is a welcome introduction of linguistics into\n",
      "the world of language modelling.\n",
      "\n",
      "4) The analysis of perplexity reduction after various classes of words in\n",
      "Russian and Czech is particularly illuminating, showing how character-level and\n",
      "morpheme-level models handle rare words much more gracefully. In light of these\n",
      "results, could the authors say something about how much language modelling\n",
      "requires understanding of semantics, and how much it requires just knowing\n",
      "various morphosyntactic effects?\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "1) The character tri-gram LSTM seems a little unmotivated. Did the authors try\n",
      "other character n-grams as well? As a reviewer, I can guess that character\n",
      "tri-grams roughly correspond to morphemes, especially in Semitic languages, but\n",
      "what made the authors report results for 3-grams as opposed to 2- or 4-? In\n",
      "addition, there are roughly 26^3=17576 possible distinct trigrams in the Latin\n",
      "lower-case alphabet, which is enough to almost constitute a word embedding\n",
      "table. Did the authors only consider observed trigrams? How many distinct\n",
      "observed trigrams were there?\n",
      "\n",
      "2) I don't think you can meaningfully claim to be examining the effectiveness\n",
      "of character-level models on root-and-pattern morphology if your dataset is\n",
      "unvocalised and thus doesn't have the 'pattern' bit of 'root-and-pattern'. I\n",
      "appreciate that finding transcribed Arabic and Hebrew with vowels may be\n",
      "challenging, but it's half of the typology.\n",
      "\n",
      "3) Reduplication seems to be a different kind of phenomenon to the other three,\n",
      "which are more strictly morphological typologies. Indonesian and Malay also\n",
      "exhibit various word affixes, which can be used on top of reduplication, which\n",
      "is a more lexical process. I'm not sure splitting it out from the other\n",
      "linguistic typologies is justified.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "1) The paper was structured very clearly and was very easy to read.\n",
      "\n",
      "2) I'm a bit puzzled about why the authors chose to use 200 dimensional\n",
      "character embeddings. Once the dimensionality of the embedding is greater than\n",
      "the size of the vocabulary (here the number of characters in the alphabet),\n",
      "surely you're not getting anything extra?\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Having read the author response, my opinions have altered little. I still think\n",
      "the same strengths and weakness that I have already discussed hold.\n",
      "- Strengths:\n",
      "\n",
      "The paper is well-written and easy to understand. The methods and results are\n",
      "interesting.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "The evaluation and the obtained results might be problematic (see my comments\n",
      "below).\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "This paper proposes a system for end-to-end argumentation mining using neural\n",
      "networks. The authors model the problem using two approaches: (1) sequence\n",
      "labeling (2) dependency parsing. The paper also includes the results of\n",
      "experimenting with a multitask learning setting for the sequence labeling\n",
      "approach. The paper clearly explains the motivation behind the proposed model.\n",
      "Existing methods are based on ILP, manual feature engineering and manual design\n",
      "of ILP constraints. However, the proposed model avoids such manual effort.\n",
      "Moreover, the model jointly learns the subtasks in argumentation mining and\n",
      "therefore, avoids the error back propagation problem in pipeline methods.\n",
      "Except a few missing details (mentioned below), the methods are explained\n",
      "clearly.\n",
      "\n",
      "The experiments are substantial, the comparisons are performed properly, and\n",
      "the results are interesting. My main concern about this paper is the small size\n",
      "of the dataset and the large capacity of the used (Bi)LSTM-based recurrent\n",
      "neural networks (BLC and BLCC). The dataset includes only around 320 essays for\n",
      "training and 80 essays for testing. The size of the development set, however,\n",
      "is not mentioned in the paper (and also the supplementary materials). This is\n",
      "worrying because very few number of essays are left for training, which is a\n",
      "crucial problem. The total number of tags in the training data is probably only\n",
      "a few thousand. Compare it to the standard sequence labeling tasks, where\n",
      "hundreds of thousands (sometimes millions) of tags are available. For this\n",
      "reason, I am not sure if the model parameters are trained properly. The paper\n",
      "also does not analyze the overfitting problem. It would be interesting to see\n",
      "the training and development \"loss\" values during training (after each\n",
      "parameter update or after each epoch). The authors have also provided some\n",
      "information that can be seen as the evidence for overfitting: Line 622 \"Our\n",
      "explanation is that taggers are simpler local models, and thus need less\n",
      "training data and are less prone to overfitting\".\n",
      "\n",
      "For the same reason, I am not sure if the models are stable enough. Mean and\n",
      "standard deviation of multiple runs (different initializations of parameters)\n",
      "need to be included. Statistical significance tests would also provide more\n",
      "information about the stability of the models and the reliability of results.\n",
      "Without these tests, it is hard to say if the better results are because of the\n",
      "superiority of the proposed method or chance.\n",
      "\n",
      "I understand that the neural networks used for modeling the tasks use their\n",
      "regularization techniques. However, since the size of the dataset is too small,\n",
      "the authors need to pay more attention to the regularization methods. The paper\n",
      "does not mention regularization at all and the supplementary material only\n",
      "mentions briefly about the regularization in LSTM-ER. This problem needs to be\n",
      "addressed properly in the paper.\n",
      "\n",
      "Instead of the current hyper-parameter optimization method (described in\n",
      "supplementary materials) consider using Bayesian optimization methods.\n",
      "\n",
      "Also move the information about pre-trained word embeddings and the error\n",
      "analysis from the supplementary material to the paper. The extra one page\n",
      "should be enough for this.\n",
      "\n",
      "Please include some inter-annotator agreement scores. The paper describing the\n",
      "dataset has some relevant information. This information would provide some\n",
      "insight about the performance of the systems and the available room for\n",
      "improvement.\n",
      "\n",
      "Please consider illustrating figure 1 with different colors to make the quality\n",
      "better for black and white prints.\n",
      "\n",
      "Edit:\n",
      "\n",
      "Thanks for answering my questions. I have increased the recommendation score to\n",
      "4. Please do include the F1-score ranges in your paper and also report mean and\n",
      "variance of different settings. I am still concerned about the model stability.\n",
      "For example, the large variance of Kiperwasser setting needs to be analyzed\n",
      "properly. Even the F1 changes in the range [0.56, 0.61] is relatively large.\n",
      "Including these score ranges in your paper helps replicating your work.\n",
      "The work describes a joint neural approach to argumentation mining. There are\n",
      "several approaches explored including:\n",
      " 1) casting the problem as a dependency parsing problem (trying several\n",
      "different parsers)\n",
      " 2) casting the problem as a sequence labeling problem\n",
      "3) multi task learning (based on sequence labeling model underneath)\n",
      "4) an out of the box neural model for labeling entities and relations (LSTM-ER)\n",
      "5) ILP based state-of-the art models\n",
      "All the approaches are evaluated using F1 defined on concepts and relations. \n",
      "Dependency based solutions do not work well, seq. labeling solutions are\n",
      "effective.\n",
      "The out-of-the-box LSTM-ER model performs very well. Especially on paragraph\n",
      "level.\n",
      "The Seq. labeling and LSTM-ER models both outperform the ILP approach.\n",
      "A very comprehensive supplement was given, with all the technicalities of\n",
      "training\n",
      "the models, optimizing hyper-parameters etc.\n",
      "It was also shown that sequence labeling models can be greatly improved by the\n",
      "multitask\n",
      "approach (with the claim task helping more than the relation task).\n",
      "The aper  is a very thorough investigation of neural based approaches to\n",
      "end-to-end argumentation mining.\n",
      "\n",
      "- Major remarks  \n",
      "  - my one concern is with the data set, i'm wondering if it's a problem that\n",
      "essays in the train set and in the test set might\n",
      "   be on the same topics, consequently writers might use the same or similar\n",
      "arguments in both essays, leading to information\n",
      "   leakage from the train to the test set. In turn, this might give overly\n",
      "optimistic performance estimates. Though, i think the same\n",
      "   issues are present for the ILP models, so your model does not have an unfair\n",
      "advantage. Still, this may be something to discuss.\n",
      "\n",
      "  - my other concern is that one of your best models LSTM-ER is acutally just a\n",
      "an out-of-the box application of a model from related\n",
      "    work. However, given the relative success of sequence based models and all\n",
      "the experiments and useful lessons learned, I think this \n",
      "    work deserves to be published.\n",
      "\n",
      "- Minor remarks and questions:\n",
      "222 - 226 - i guess you are arguing that it's possible to reconstruct the full\n",
      "graph once you get a tree as output? Still, this part is not quite clear.\n",
      "443-444 The ordering in this section is seq. tagging -> dependency based -> MTL\n",
      "using seq. tagging, it would be much easier to follow if the order of the first\n",
      "two were\n",
      "                  reversed (by the time I got here i'd forgotten what STag_T\n",
      "stood for)\n",
      "455 - What does it mean that it de-couples them but jointly models them (isn't\n",
      "coupling them required to jointly model them?)\n",
      "         - i checked Miwa and Bansal and I couldn't find it\n",
      "477 - 479 -  It's confusing when you say your system de-couples relation info\n",
      "from entity info, my best guess is that you mean it\n",
      "                        learns some tasks as \"the edges of the tree\" and some\n",
      "other tasks as \"the labels on those edges\", thus decoupling them. \n",
      "                        In any case,  I recommend you make this part clearer\n",
      "\n",
      "Are the F1 scores in the paragraph and essay settings comparable? In particular\n",
      "for the relation tasks. I'm wondering if paragraph based \n",
      "models might miss some cross paragraph relations by default, because they will\n",
      "never consider them.\n",
      "This paper describes a straightforward extension to left-to-right beam search\n",
      "in order to allow it to incorporate lexical constraints in the form of word\n",
      "sequences that must appear in MT output. This algorithm is shown to be\n",
      "effective for interactive translation and domain adaptation.\n",
      "\n",
      "Although the proposed extension is very simple, I think the paper makes a\n",
      "useful contribution by formalizing it. It is also interesting to know that NMT\n",
      "copes well with a set of unordered constraints having no associated alignment\n",
      "information. There seem to be potential applications for this technique beyond\n",
      "the ones investigated here, for example improving NMT’s ability to handle\n",
      "non-compositional constructions, which is one of the few areas where it still\n",
      "might lag traditional SMT.\n",
      "\n",
      "The main weakness of the paper is that the experiments are somewhat limited.\n",
      "The interactive MT simulation shows that the method basically works, but it is\n",
      "difficult to get a sense of how well - for instance, in how many cases the\n",
      "constraint was incorporated in an acceptable manner (the large BLEU score\n",
      "increases are only indirect evidence). Similarly, adaptation should have been \n",
      "compared to the standard “fine-tuning” baseline, which would be relatively\n",
      "inexpensive to run on the 100K Autodesk corpus.\n",
      "\n",
      "Despite this weakness, I think this is a decent contribution that deserves to\n",
      "be published.\n",
      "\n",
      "Further details:\n",
      "\n",
      "422 Given its common usage in PBMT, “coverage vector” is a potentially\n",
      "misleading term. The appropriate data structure seems more likely to be a\n",
      "coverage set.\n",
      "\n",
      "Table 2 should also give some indication of the number of constraints per\n",
      "source sentence in the test corpora, to allow for calibration of the BLEU\n",
      "gains.\n",
      "- Strengths:\n",
      "This paper proposes a novel approach for dialogue state tracking that benefits\n",
      "from representing slot values with pre-trained embeddings and learns to compose\n",
      "them into distributed representations of user utterances and dialogue context.\n",
      "Experiments performed on two datasets show consistent and significant\n",
      "improvements over the baseline of previous delexicalization based approach.\n",
      "Alternative approaches (i.e., XAVIER, GloVe, Program-SL999) for pre-training\n",
      "word embeddings have been investigated.\n",
      "\n",
      "- Weaknesses:\n",
      "Although one of the main motivations for using embeddings is to generalize to\n",
      "more complex dialogue domains where delexicalization may not scale for, the\n",
      "datasets used seem limited.    I wonder how the approach would compare with and\n",
      "without a separate slot tagging component on more complex dialogues. For\n",
      "example, when computing similarity between the utterance and slot value pairs,\n",
      "one can actually limit the estimation to the span of the slot values. This\n",
      "should be applicable even when the values do not match.\n",
      "\n",
      "I think the examples in the intro is misleading, shouldn’t the dialogue state\n",
      "also include “restaurant_name=The House”? This brings another question, how\n",
      "does resolution of coreferences impact this task?\n",
      "\n",
      "- General Discussion:\n",
      "On the overall, use of pre-trained word embeddings is a great idea, and the\n",
      "specific approach for using them is exciting.\n",
      "This paper presents a neural network-based framework for dialogue state\n",
      "tracking.\n",
      "The main contribution of this work is on learning representations of user\n",
      "utterances, system outputs, and also ontology entries, all of which are based\n",
      "on pre-trained word vectors.\n",
      "Particularly for the utterance representation, the authors compared two\n",
      "different neural network models: NBT-DNN and NBT-CNN.\n",
      "The learned representations are combined with each other and finally used in\n",
      "the downstream network to make binary decision for a given slot value pair.\n",
      "The experiment shows that the proposed framework achieved significant\n",
      "performance improvements compared to the baseline with the delexicalized\n",
      "approach.\n",
      "\n",
      "It's generally a quality work with clear goal, reasonable idea, and improved\n",
      "results from previous studies.\n",
      "But the paper itself doesn't seem to be very well organized to effectively\n",
      "deliver the details especially to readers who are not familiar with this area.\n",
      "\n",
      "First of all, more formal definition of DST needs to be given at the beginning\n",
      "of this paper.\n",
      "It is not clear enough and could be more confusing after coupling with SLU.\n",
      "My suggestion is to provide a general architecture of dialogue system described\n",
      "in Section 1 rather than Section 2, followed by the problem definition of DST\n",
      "focusing on its relationships to other components including ASR, SLU, and\n",
      "policy learning.\n",
      "\n",
      "And it would also help to improve the readability if all the notations used\n",
      "throughout the paper are defined in an earlier section.\n",
      "Some symbols (e.g. t_q, t_s, t_v) are used much earlier than their\n",
      "descriptions.\n",
      "\n",
      "Below are other comments or questions:\n",
      "\n",
      "- Would it be possible to perform the separate SLU with this model? If no, the\n",
      "term 'joint' could be misleading that this model is able to handle both tasks.\n",
      "\n",
      "- Could you please provide some statistics about how many errors were corrected\n",
      "from the original DSTC2 dataset?\n",
      "If it is not very huge, the experiment could include the comparisons also with\n",
      "other published work including DSTC2 entries using the same dataset.\n",
      "\n",
      "- What do you think about using RNNs or LSTMs to learn the sequential aspects\n",
      "in learning utterance representations?\n",
      "Considering the recent successes of these recurrent networks in SLU problems,\n",
      "it could be effective to DST as well.\n",
      "\n",
      "- Some more details about the semantic dictionary used with the baseline would\n",
      "help to imply the cost for building this kind of resources manually.\n",
      "\n",
      "- It would be great if you could give some samples which were not correctly\n",
      "predicted by the baseline but solved with your proposed models.\n",
      "This paper presents a purpose-built neural network architecture for textual\n",
      "entailment/NLI based on a three step process of encoding, attention-based\n",
      "matching, and aggregation. The model has two variants, one based on TreeRNNs\n",
      "and the other based on sequential BiLSTMs. The sequential model outperforms all\n",
      "published results, and an ensemble with the tree model does better still.\n",
      "\n",
      "The paper is clear, the model is well motivated, and the results are\n",
      "impressive. Everything in the paper is solidly incremental, but I nonetheless\n",
      "recommend acceptance. \n",
      "\n",
      "Major issues that I'd like discussed in the response:\n",
      "– You suggest several times that your system can serve as a new baseline for\n",
      "future work on NLI. This isn't an especially helpful or meaningful claim—it\n",
      "could be said of just about any model for any task. You could argue that your\n",
      "model is unusually simple or elegant, but I don't think that's really a major\n",
      "selling point of the model.\n",
      "– Your model architecture is symmetric in some ways that seem like\n",
      "overkill—you compute attention across sentences in both directions, and run a\n",
      "separate inference composition (aggregation) network for each direction. This\n",
      "presumably nearly doubles the run time of your model. Is this really necessary\n",
      "for the very asymmetric task of NLI? Have you done ablation studies on this?**\n",
      "– You present results for the full sequential model (ESIM) and the ensemble\n",
      "of that model and the tree-based model (HIM). Why don't you present results for\n",
      "the tree-based model on its own?**\n",
      "\n",
      "Minor issues:\n",
      "– I don't think the Barker and Jacobson quote means quite what you want it to\n",
      "mean. In context, it's making a specific and not-settled point about *direct*\n",
      "compositionality in formal grammar. You'd probably be better off with a more\n",
      "general claim about the widely accepted principle of compositionality.\n",
      "– The vector difference feature that you use (which has also appeared in\n",
      "prior work) is a bit odd, since it gives the model redundant parameters. Any\n",
      "model that takes vectors a, b, and (a - b) as input to some matrix\n",
      "multiplication is exactly equivalent to some other model that takes in just a\n",
      "and b and has a different matrix parameter. There may be learning-related\n",
      "reasons why using this feature still makes sense, but it's worth commenting on.\n",
      "– How do you implement the tree-structured components of your model? Are\n",
      "there major issues with speed or scalability there?\n",
      "\u001f– Typo: (Klein and D. Manning, 2003) \n",
      "– Figure 3: Standard tree-drawing packages like (tikz-)qtree produce much\n",
      "more readable parse trees without crossing lines. I'd suggest using them.\n",
      "\n",
      "---\n",
      "\n",
      "Thanks for the response! I still solidly support publication. This work is not\n",
      "groundbreaking, but it's novel in places, and the results are surprising enough\n",
      "to bring some value to the conference.\n",
      "The paper proposes a model for the Stanford Natural Language Inference (SNLI)\n",
      "dataset, that builds on top of sentence encoding models and the decomposable\n",
      "word level alignment model by Parikh et al. (2016). The proposed improvements\n",
      "include performing decomposable attention on the output of a BiLSTM and feeding\n",
      "the attention output to another BiLSTM, and augmenting this network with a\n",
      "parallel tree variant.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "This approach outperforms several strong models previously proposed for the\n",
      "task. The authors have tried a large number of experiments, and clearly report\n",
      "the ones that did not work, and the hyperparameter settings of the ones that\n",
      "did. This paper serves as a useful empirical study for a popular problem.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "Unfortunately, there are not many new ideas in this work that seem useful\n",
      "beyond the scope the particular dataset used. While the authors claim that the\n",
      "proposed network architecture is simpler than many previous models, it is worth\n",
      "noting that the model complexity (in terms of the number of parameters) is\n",
      "fairly high. Due to this reason, it would help to see if the empirical gains\n",
      "extend to other datasets as well. In terms of ablation studies, it would help\n",
      "to see 1) how well the tree-variant of the model does on its own and 2) the\n",
      "effect of removing inference composition from the model.\n",
      "\n",
      "Other minor issues:\n",
      "1) The method used to enhance local inference (equations 14 and 15) seem very\n",
      "similar to the heuristic matching function used by Mou et al., 2015 (Natural\n",
      "Language Inference by Tree-Based Convolution and Heuristic Matching). You may\n",
      "want to cite them.\n",
      "\n",
      "2) The first sentence in section 3.2 is an unsupported claim. This either needs\n",
      "a citation, or needs to be stated as a hypothesis.\n",
      "\n",
      "While the work is not very novel, the the empirical study is rigorous for the\n",
      "most part, and could be useful for researchers working on similar problems.\n",
      "Given these strengths, I am changing my recommendation score to 3. I have read\n",
      "the authors' responses.\n",
      "This work proposes to apply dilated convolutions for sequence tagging\n",
      "(specifically, named entity recognition). It also introduces some novel ideas\n",
      "(sharing the dilated convolution block, predicting the tags at each convolution\n",
      "level), which I think will prove useful to the community. The paper performs\n",
      "extensive ablation experiments to show the effectiveness of their approach.\n",
      "I found the writing to be very clear, and the experiments were exceptionally\n",
      "thorough.\n",
      "\n",
      "Strengths:  \n",
      "- Extensive experiments against various architectures (LSTM, LSTM + CRF)       \n",
      "- Novel architectural/training ideas (sharing blocks)  \n",
      "\n",
      "Weaknesses:  \n",
      "- Only applied to English NER--this is a big concern since the title of the\n",
      "paper seems to reference sequence-tagging directly.  \n",
      "- Section 4.1 could be clearer. For example, I presume there is padding to make\n",
      "sure the output resolution after each block is the same as the input\n",
      "resolution.  Might be good to mention this.  \n",
      "- I think an ablation study of number of layers vs perf might be interesting.\n",
      "\n",
      "RESPONSE TO AUTHOR REBUTTAL:\n",
      "\n",
      "Thank you very much for a thoughtful response. Given that the authors have\n",
      "agreed to make the content be more specific to NER as opposed to\n",
      "sequence-tagging, I have revised my score upward.\n",
      "- Strengths:\n",
      "\n",
      "The main strength promised by the paper is the speed advantage at the same\n",
      "accuracy level.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "Presentation of the approach leaves a lot to be desired. Sections 3 and 4 need\n",
      "to be much clearer, from concept definition to explaining the architecture and\n",
      "parameterization. In particular Section 4.1 and the parameter tieing used need\n",
      "to be crystal clear, since that is one of the main contributions of the paper.\n",
      "\n",
      "More experiments supporting the vast speed improvements promised need to be\n",
      "presented. The results in Table 2 are good but not great. A speed-up of 4-6X is\n",
      "nothing all that transformative.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "What exactly is \"Viterbi prediction\"? The term/concept is far from established;\n",
      "the reader could guess but there must be a better way to phrase it.\n",
      "\n",
      "Reference Weiss et al., 2015 has a typo.\n",
      "This paper compares different ways of inducing embeddings for the task of\n",
      "polarity classification. The authors focus on different types of corpora and\n",
      "find that not necessarily the largest corpus provides the most appropriate\n",
      "embeddings for their particular task but it is more effective to consider a\n",
      "corpus (or subcorpus) in which a higher concentration of subjective content can\n",
      "be found. The latter type of data are also referred to as \"task-specific data\".\n",
      "Moreover, the authors compare different embeddings that combine information\n",
      "from \"task-specific\" corpora and generic corpora. A combination outperforms\n",
      "embeddings just drawn from a single corpus. This combination is not only\n",
      "evaluated on English but also on a less resourced language (i.e. Catalan).\n",
      "\n",
      "- Strengths:\n",
      "The paper addresses an important aspect of sentiment analysis, namely how to\n",
      "appropriately induce embeddings for training supervised classifers for polarity\n",
      "classification. The paper is well-structured and well-written. The major claims\n",
      "made by the authors are sufficiently supported by their experiments.\n",
      "\n",
      "- Weaknesses:\n",
      "The outcome of the experiments is very predictable. The methods that are\n",
      "employed are very simple and ad-hoc. I found hardly any new idea in\n",
      "that paper. Neither are there any significant lessons that the reader learns\n",
      "about embeddings or sentiment analysis. The main idea (i.e. focusing on more\n",
      "task-specific data for training more accurate embeddings) was already published\n",
      "in the context of named-entity recognition by Joshi et al. (2015). The\n",
      "additions made in this paper are very incremental in nature.\n",
      "\n",
      "I find some of the experiments inconclusive as (apparently) no statistical\n",
      "signficance testing between different classifiers has been carried out. In\n",
      "Tables\n",
      "2, 3 and 6, various classifier configurations produce very similar scores. In\n",
      "such cases, only statistical signficance testing can really give a proper\n",
      "indication whether these difference are meaningful. For instance, in Table 3 on\n",
      "the left half reporting results on RT, one may wonder whether there is a\n",
      "significant difference between \"Wikipedia Baseline\" and any of the\n",
      "combinations. Furthermore, one doubts whether there is any signficant\n",
      "difference between the different combinations (i.e. either using \"subj-Wiki\",\n",
      "\"subj-Multiun\" or \"subj-Europarl\") in that table.\n",
      "The improvement by focusing on subjective subsets is plausible in general.\n",
      "However, I wonder whether in real life, in particular, a situation in which\n",
      "resources are sparse this is very helpful. Doing a pre-selection with\n",
      "OpinionFinder is some pre-processing step which will not be possible in most\n",
      "languages other than English. There are no equivalent tools or fine-grained\n",
      "datasets on which such functionality could be learnt. The fact that in the\n",
      "experiments\n",
      "for Catalan, this information is not considered proves that. \n",
      "\n",
      "Minor details:\n",
      "\n",
      "- lines 329-334: The discussion of this dataset is confusing. I thought the\n",
      "task is plain polarity classification but the authors here also refer to\n",
      "\"opinion holder\" and \"opinion targets\". If these information are not relevant\n",
      "to the experiments carried out in this paper, then they should not be mentioned\n",
      "here.\n",
      "\n",
      "- lines 431-437: The variation of \"splicing\" that the authors explain is not\n",
      "very well motivated. First, why do we need this? In how far should this be more\n",
      "effective than simple \"appending\"?\n",
      "\n",
      "- lines 521-522: How is the subjective information isolated for these\n",
      "configurations? I assume the authors here again employ OpinionFinder? However,\n",
      "there is no explicit mention of this here.\n",
      "\n",
      "- lines 580-588: The definitions of variables do not properly match the\n",
      "formula (i.e. Equation 3). I do not find n_k in Equation 3.\n",
      "\n",
      "- lines 689-695: Similar to lines 329-334 it is unclear what precise task is\n",
      "carried out. Do the authors take opinion holders and targets in consideration?\n",
      "\n",
      "***AFTER AUTHORS' RESPONSE***\n",
      "Thank you very much for these clarifying remarks.\n",
      "I do not follow your explanations regarding the incorporation of opinion\n",
      "holders and targets, though.\n",
      "\n",
      "Overall, I will not change my scores since I think that this work lacks\n",
      "sufficient novelty (the things the authors raised in their response are just\n",
      "insufficient to me). This submission is too incremental in nature.\n",
      "- Strengths: An interesting and comprehensive study of the effect of using\n",
      "special-domain corpora for training word embeddings.  Clear explanation of the\n",
      "assumptions, contributions, methodology, and results.  Thorough evaluation of\n",
      "various aspects of the proposal.\n",
      "\n",
      "- Weaknesses: Some conclusions are not fully backed up by the numerical\n",
      "results.  E.g., the authors claim that for Catalan, the improvements of using\n",
      "specific corpora for training word vectors is more pronounced than English.  I\n",
      "am not sure why this conclusion is made based on the results.  E.g., in Table\n",
      "6, none of the combination methods outperform the baseline for the\n",
      "300-dimension vectors.\n",
      "\n",
      "- General Discussion: The paper presents a set of simple, yet interesting\n",
      "experiments that suggest word vectors (here trained using the skip-gram method)\n",
      "largely benefit from the use of relevant (in-domain) and subjective corpora. \n",
      "The paper answers important questions that are of benefit to practitioners of\n",
      "natural language processing.  The paper is also very well-written, and very\n",
      "clearly organized.\n",
      "This paper presents a new dataset with annotations of products coming from\n",
      "online cybercrime forums. The paper is clear and well-written and the\n",
      "experiments are good. Every hypothesis is tested and compared to each other.\n",
      "\n",
      "However, I do have some concerns about the paper:\n",
      "\n",
      "1. The authors took the liberty to change the font size and the line spacing of\n",
      "the abstract, enabling them to have a longer abstract and to fit the content\n",
      "into the 8 pages requirement.\n",
      "\n",
      "2. I don't think this paper fits the tagging, chunking, parsing area, as it is\n",
      "more an information extraction problem.\n",
      "\n",
      "3. I have difficulties to see why some annotations such as sombody in Fig. 1\n",
      "are related to a product.\n",
      "\n",
      "4. The basic results are very basic indeed and - with all the tools available\n",
      "nowadays in NLP -, I am sure that it would have been possible to have more\n",
      "elaborate baselines without too much extra work.\n",
      "\n",
      "5. Domain adaptation experiments corroborate what we already know about\n",
      "user-generated data where two forums on video games, e.g., may have different\n",
      "types of users (age, gender, etc.) leading to very different texts. So this\n",
      "does not give new highlights on this specific problem.\n",
      "- Strengths:\n",
      "\n",
      "This is the first neural network-based approach to argumentation\n",
      "mining. The proposed method used a Pointer Network (PN) model with\n",
      "multi-task learning and outperformed previous methods in the\n",
      "experiments on two datasets.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "This is basically an application of PN to argumentation\n",
      "mining. Although the combination of PN and multi-task learning for\n",
      "this task is novel, its novelty is not enough for ACL long\n",
      "publication. The lack of qualitative analysis and error analysis is\n",
      "also a major concern.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "Besides the weaknesses mentioned above, the use of PN is not\n",
      "well-motivated. Although three characteristics of PN were described in\n",
      "l.138-143, these are not a strong motivation against the use of\n",
      "bi-directional LSTMs and the attention mechanism. The authors should\n",
      "describe what problems are solved by PN and discuss in the experiments\n",
      "how much these problems are solved.\n",
      "\n",
      "Figures 2 and 3 are difficult to understand. What are the self link to\n",
      "D1 and the links from D2 to E1 and D3/D4 to E2? These are just the\n",
      "outputs from the decoder and not links. The decoder LSTM does not have\n",
      "an input from e_j in these figures, but it does in Equation (3). Also,\n",
      "in Figure 3, the abbreviation \"FC\" is not defined.\n",
      "\n",
      "Equation (8) is strange. To calculate the probability of each\n",
      "component type, the probability of E_i is calculated.\n",
      "\n",
      "In the experiments, I did not understand why only \"PN\", which is not a\n",
      "joint model, was performed for the microtext corpus.\n",
      "\n",
      "It is not clear whether the BLSTM model is trained with the joint-task\n",
      "objective.\n",
      "\n",
      "There are some studies on discourse parsing using the attention\n",
      "mechanism. The authors should describe the differences from these studies.\n",
      "\n",
      "Minor issues:\n",
      "\n",
      "l.128: should related -> should be related\n",
      "\n",
      "l.215: (2015) is floating\n",
      "\n",
      "l.706: it able -> it is able\n",
      "\n",
      "I raised my recommendation score after reading the convincing author responses.\n",
      "I strongly recommend that the authors should discuss improved examples by PN as\n",
      "well as the details of feature ablation.\n",
      "The paper presents an application of Pointer Networks, a recurrent neural\n",
      "network model original used for solving algorithmic tasks, to two subtasks of\n",
      "Argumentation Mining: determining the types of Argument Components, and finding\n",
      "the links between them. The model achieves state-of-the-art results.\n",
      "\n",
      "Strengths:\n",
      "\n",
      "- Thorough review of prior art in the specific formulation of argument mining\n",
      "handled in this paper.\n",
      "- Simple and effective modification of an existing model to make it suitable\n",
      "for\n",
      "the task. The model is mostly explained clearly.\n",
      "- Strong results as compared to prior art in this task.\n",
      "\n",
      "Weaknesses:\n",
      "\n",
      "- 071: This formulation of argumentation mining is just one of several proposed\n",
      "subtask divisions, and this should be mentioned. For example, in [1], claims\n",
      "are detected and classified before any supporting evidence is detected.\n",
      "Furthermore, [2] applied neural networks to this task, so it is inaccurate to\n",
      "say (as is claimed in the abstract of this paper) that this work is the first\n",
      "NN-based approach to argumentation mining.\n",
      "- Two things must be improved in the presentation of the model: (1) What is the\n",
      "pooling method used for embedding features (line 397)? and (2) Equation (7) in\n",
      "line 472 is not clear enough: is E_i the random variable representing the\n",
      "*type* of AC i, or its *identity*? Both are supposedly modeled (the latter by\n",
      "feature representation), and need to be defined. Furthermore, it seems like the\n",
      "LHS of equation (7) should be a conditional probability.\n",
      "- There are several unclear things about Table 2: first, why are the three\n",
      "first\n",
      "baselines evaluated only by macro f1 and the individual f1 scores are missing?\n",
      "This is not explained in the text. Second, why is only the \"PN\" model\n",
      "presented? Is this the same PN as in Table 1, or actually the Joint Model? What\n",
      "about the other three?\n",
      "- It is not mentioned which dataset the experiment described in Table 4 was\n",
      "performed on.\n",
      "\n",
      "General Discussion:\n",
      "\n",
      "- 132: There has to be a lengthier introduction to pointer networks, mentioning\n",
      "recurrent neural networks in general, for the benefit of readers unfamiliar\n",
      "with \"sequence-to-sequence models\". Also, the citation of Sutskever et al.\n",
      "(2014) in line 145 should be at the first mention of the term, and the\n",
      "difference with respect to recursive neural networks should be explained before\n",
      "the paragraph starting in line 233 (tree structure etc.).\n",
      "- 348: The elu activation requires an explanation and citation (still not\n",
      "enough\n",
      "well-known).\n",
      "- 501: \"MC\", \"Cl\" and \"Pr\" should be explained in the label.\n",
      "- 577: A sentence about how these hyperparameters were obtained would be\n",
      "appropriate.\n",
      "- 590: The decision to do early stopping only by link prediction accuracy\n",
      "should\n",
      "be explained (i.e. why not average with type accuracy, for example?).\n",
      "- 594: Inference at test time is briefly explained, but would benefit from more\n",
      "details.\n",
      "- 617: Specify what the length of an AC is measured in (words?).\n",
      "- 644: The referent of \"these\" in \"Neither of these\" is unclear.\n",
      "- 684: \"Minimum\" should be \"Maximum\".\n",
      "- 694: The performance w.r.t. the amount of training data is indeed surprising,\n",
      "but other models have also achieved almost the same results - this is\n",
      "especially surprising because NNs usually need more data. It would be good to\n",
      "say this.\n",
      "- 745: This could alternatively show that structural cues are less important\n",
      "for\n",
      "this task.\n",
      "- Some minor typos should be corrected (e.g. \"which is show\", line 161).\n",
      "\n",
      "[1] Rinott, Ruty, et al. \"Show Me Your Evidence-an Automatic Method for Context\n",
      "Dependent Evidence Detection.\" EMNLP. 2015.\n",
      "\n",
      "[2] Laha, Anirban, and Vikas Raykar. \"An Empirical Evaluation of various Deep\n",
      "Learning Architectures for Bi-Sequence Classification Tasks.\" COLING. 2016.\n",
      "The paper is clearly written, and the claims are well-supported.  The Related\n",
      "Work in particular is very thorough, and clearly establishes where the proposed\n",
      "work fits in the field.\n",
      "\n",
      "I had two main questions about the method: (1) phrases are mentioned in section\n",
      "3.1, but only word representations are discussed.  How are phrase\n",
      "representations derived?\n",
      "(2) There is no explicit connection between M^+ and M^- in the model, but they\n",
      "are indirectly connected through the tanh scoring function.  How do the learned\n",
      "matrices compare to one another (e.g., is M^- like -1*M^+?)?  Furthermore, what\n",
      "would be the benefits/drawbacks of linking the two together directly, by\n",
      "enforcing some measure of dissimilarity?\n",
      "\n",
      "Additionally, statistical significance of the observed improvements would be\n",
      "valuable.\n",
      "\n",
      "Typographical comments:\n",
      "- Line 220: \"word/phase pair\" should be \"word/phrase pair\"\n",
      "- Line 245: I propose an alternate wording: instead of \"entities are translated\n",
      "to,\" say \"entities are mapped to\".  At first, I read that as a translation\n",
      "operation in the vector space, which I think isn't exactly what's being\n",
      "described.\n",
      "- Line 587: \"slightly improvement in F-measure\" should be \"slight improvement\n",
      "in F-measure\"\n",
      "- Line 636: extraneous commas in citation\n",
      "- Line 646: \"The most case\" should be \"The most likely case\" (I'm guessing)\n",
      "- Line 727: extraneous period and comma in citation\n",
      "- Strengths:\n",
      "\n",
      "1. Interesting research problem\n",
      "2. The method in this paper looks quite formal.\n",
      "3. The authors have released their dataset with the submission.\n",
      "4. The design of experiments is good.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "1. The advantage and disadvantage of the transductive learning has not yet\n",
      "discussed.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "In this paper, the authors introduce a transductive learning approach for\n",
      "Chinese hypernym prediction, which is quite interesting problem. The authors\n",
      "establish mappings from entities to hypernyms in the embedding space directly,\n",
      "which sounds also quite novel. This paper is well written and easy to follow.\n",
      "The first part of their method, preprocessing using embeddings, is widely used\n",
      "method for the initial stage. But it's still a normal way to preprocess the\n",
      "input data. The transductive model is an optimization framework for non-linear\n",
      "mapping utilizing both labeled and unlabeled data. The attached supplementary\n",
      "notes about the method makes it more clear. The experimental results have shown\n",
      "the effectiveness of the proposed method in this paper. The authors also\n",
      "released dataset, which contributes to similar research for other researchers\n",
      "in future.\n",
      "- Strengths:\n",
      "This paper presents an extension to A* CCG parsing to include dependency\n",
      "information.  Achieving this while maintaining speed and tractability is a very\n",
      "impressive feature of this approach.  The ability to precompute attachments is\n",
      "a nice trick.                  I also really appreciated the evaluation of the\n",
      "effect of\n",
      "the\n",
      "head-rules on normal-form violations and would love to see more details on the\n",
      "remaining cases.\n",
      "\n",
      "- Weaknesses:\n",
      "I'd like to see more analysis of certain dependency structures.  I'm\n",
      "particularly interested in how coordination and relative clauses are handled\n",
      "when the predicate argument structure of CCG is at odds with the dependency\n",
      "structures normally used by other dependency parsers.\n",
      "\n",
      "- General Discussion:\n",
      "I'm very happy with this work and feel it's a very nice contribution to the\n",
      "literature.  The only thing missing for me is a more in-depth analysis of the\n",
      "types of constructions which saw the most improvement (English and Japanese)\n",
      "and a discussion (mentioned above) reconciling Pred-Arg dependencies with those\n",
      "of other parsers.\n",
      "This paper describes a state-of-the-art CCG parsing model that decomposes into\n",
      "tagging and dependency scores, and has an efficient A* decoding algorithm.\n",
      "Interestingly, the paper slightly outperforms Lee et al. (2016)'s more\n",
      "expressive global parsing model, presumably because this factorization makes\n",
      "learning easier. It's great that they also report results on another language,\n",
      "showing large improvements over existing work on Japanese CCG parsing. One\n",
      "surprising original result is that modeling the first word of a constituent as\n",
      "the head substantially outperforms linguistically motivated head rules. \n",
      "\n",
      "Overall this is a good paper that makes a nice contribution. I only have a few\n",
      "suggestions:\n",
      "- I liked the way that the dependency and supertagging models interact, but it\n",
      "would be good to include baseline results for simpler variations (e.g. not\n",
      "conditioning the tag on the head dependency).\n",
      "- The paper achieves new state-of-the-art results on Japanese by a large\n",
      "margin. However, there has been a lot less work on this data - would it also be\n",
      "possible to train the Lee et al. parser on this data for comparison?\n",
      "- Lewis, He and Zettlemoyer (2015) explore combined dependency and supertagging\n",
      "models for CCG and SRL, and may be worth citing.\n",
      "This paper proposes a method for building dialogue agents involved in a\n",
      "symmetric collaborative task, in which the agents need to strategically\n",
      "communicate to achieve a common goal.  \n",
      "\n",
      "I do like this paper.  I am very interested in how much data-driven techniques\n",
      "can be used for dialogue management.  However, I am concerned that the approach\n",
      "that this paper proposes, is actually not specific to symmetric collaborative\n",
      "tasks, but to tasks that can be represented as graph operations, such as\n",
      "finding an intersection between objects that the two people know about.\n",
      "\n",
      "In Section 2.1, the authors introduce symmetric collaborative dialogue setting.\n",
      " However, such dialogs have been studied before, such as Clark and Wilkes-Gibbs\n",
      "explored (Cognition '86), and Walker's furniture layout task (Journal of\n",
      "Artificial Research '00).\n",
      "\n",
      "On line 229, the authors say that this domain is too rich for slot-value\n",
      "semantics.  However, their domain is based on attribute value pairs, so their\n",
      "domain could use a semantics represenation based on attribute value-pairs, such\n",
      "as first order logic.\n",
      "\n",
      "Section 3.2 is hard to follow.        The authors often refer to Figure 2, but I\n",
      "didn't find this example that helpful.        For example, for section 3.1, at what\n",
      "point of the dialogue does this represent?  Is this the same after `anyone went\n",
      "to columbia?'\n",
      "This paper presents a novel framework for modelling symmetric collaborative\n",
      "dialogue agents by dynamically extending knowledge graphs embeddings. The task\n",
      "is rather simple: two dialogue agents (bot-bot, human-human or human-bot) talk\n",
      "about their mutual friends. There is an underlying knowledge base for each\n",
      "party in the dialogue and an associated knowledge graph. Items in the knowledge\n",
      "graph have embeddings that are dynamically updated during the conversation and\n",
      "used to generate the answers.\n",
      "\n",
      "- Strengths: This model is very novel for both goal-directed and open ended\n",
      "dialogue. The presented evaluation metrics show clear advantage for the\n",
      "presented model.\n",
      "\n",
      "- Weaknesses: In terms of the presentation, mathematical details of how the\n",
      "embeddings are computed are not sufficiently clear. While the authors have done\n",
      "an extensive evaluation, they haven't actually compared the system with an\n",
      "RL-based dialogue manager which is current state-of-the-art in goal-oriented\n",
      "systems. Finally, it is not clear how this approach scales to more complex\n",
      "problems. The authors say that the KB is 3K, but actually what the agent\n",
      "operates is about 10 (judging from Table 6).\n",
      "\n",
      "- General Discussion: Overall, I think this is a good paper. Had the\n",
      "theoretical aspects of the paper been better presented I would give this paper\n",
      "an accept.\n",
      "This is a nice paper on morphological segmentation utilizing word \n",
      "embeddings. The paper presents a system which uses word embeddings to \n",
      "both measure local semantic similarity of word pairs with a potential \n",
      "morphological relation, and global information about the semantic validity\n",
      "of potential morphological segment types. The paper is well written and \n",
      "represents a nice extension to earlier approaches on semantically driven \n",
      "morphological segmentation.\n",
      "\n",
      "The authors present experiments on Morpho Challenge data for three \n",
      "languages: English, Turkish and Finnish. These languages exhibit varying \n",
      "degrees of morphological complexity. All systems are trained on Wikipedia \n",
      "text. \n",
      "\n",
      "The authors show that the proposed MORSE system delivers clear \n",
      "improvements w.r.t. F1-score for English and Turkish compared to the well \n",
      "known Morfessor system which was used as baseline. The system fails to \n",
      "reach the performance of Morfessor for Finnish. As the authors note, this \n",
      "is probably a result of the richness of Finnish morphology which leads to \n",
      "data sparsity and, therefore, reduced quality of word embeddings. To \n",
      "improve the performance for Finnish and other languages with a similar \n",
      "degree of morphological complexity, the authors could consider word \n",
      "embeddings which take into account sub-word information. For example,\n",
      "\n",
      "@article{DBLP:journals/corr/CaoR16,\n",
      "  author    = {Kris Cao and\n",
      "               Marek Rei},\n",
      "  title     = {A Joint Model for Word Embedding and Word Morphology},\n",
      "  journal   = {CoRR},\n",
      "  volume    = {abs/1606.02601},\n",
      "  year                  = {2016},\n",
      "  url                 = {http://arxiv.org/abs/1606.02601},\n",
      "  timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},\n",
      "  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/CaoR16},\n",
      "  bibsource = {dblp computer science bibliography, http://dblp.org}\n",
      "}\n",
      "\n",
      "@article{DBLP:journals/corr/BojanowskiGJM16,\n",
      "  author    = {Piotr Bojanowski and\n",
      "               Edouard Grave and\n",
      "               Armand Joulin and\n",
      "               Tomas Mikolov},\n",
      "  title     = {Enriching Word Vectors with Subword Information},\n",
      "  journal   = {CoRR},\n",
      "  volume    = {abs/1607.04606},\n",
      "  year                  = {2016},\n",
      "  url                 = {http://arxiv.org/abs/1607.04606},\n",
      "  timestamp = {Tue, 02 Aug 2016 12:59:27 +0200},\n",
      "  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BojanowskiGJM16},\n",
      "  bibsource = {dblp computer science bibliography, http://dblp.org}\n",
      "}\n",
      "\n",
      "The authors critique the existing Morpho Challenge data sets. \n",
      "For example, there are many instances of incorrectly segmented words in \n",
      "the material. Moreover, the authors note that, while some segmentations \n",
      "in the the data set may be historically valid (for example the \n",
      "segmentation of business into busi-ness), these segmentations are no \n",
      "longer semantically motivated. The authors provide a new data set \n",
      "consisting of 2000 semantically motivated segmentation of English word \n",
      "forms from the English Wikipedia. They show that MORSE deliver highly \n",
      "substantial improvements compared to Morfessor on this data set.\n",
      "\n",
      "In conclusion, I think this is a well written paper which presents \n",
      "competitive results on the interesting task of semantically driven \n",
      "morphological segmentation. The authors accompany the submission with \n",
      "code and a new data set which definitely add to the value of the \n",
      "submission.\n",
      "This paper continues the line of work for applying word embeddings for the\n",
      "problem of unsupervised morphological segmentation (e.g. Soricut & Och, 2015;\n",
      "Üstün & Can, 2016). The proposed method, MORSE, applies a local optimization\n",
      "for segmentation of each word, based on a set of orthographic and semantic\n",
      "rules and a few heuristic threshold values associated with them.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "The paper presents multiple ways to evaluate segmentation hypothesis on word\n",
      "embeddings, and these may be useful also in other type of methods. The results\n",
      "on English and Turkish data sets are convincing.\n",
      "\n",
      "The paper is clearly written and organized, and the biliography is extensive.\n",
      "\n",
      "The submission includes software for testing the English MORSE model and three\n",
      "small data sets used in the expriments.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "The ideas in the paper are quite incremental, based mostly on the work by\n",
      "Soricut & Och (2015). However, the main problems of the paper concern\n",
      "meaningful comparison to prior work and analysis of the method's limitations.\n",
      "\n",
      "First, the proposed method does not provide any sensible way for segmenting\n",
      "compounds. Based on Section 5.3, the method does segment some of the compounds,\n",
      "but using the terminology of the method, it considers either of the\n",
      "constituents as an affix. Unsuprisingly, the limitation shows up especially in\n",
      "the results of a highly-compounding language, Finnish. While the limitation is\n",
      "indicated in the end of the discussion section, the introduction and\n",
      "experiments seem to assume otherwise.\n",
      "\n",
      "In particular, the limitation on modeling compounds makes the evaluation of\n",
      "Section 4.4/5.3 quite unfair: Morfessor is especially good at segmenting\n",
      "compounds (Ruokolainen et al., 2014), while MORSE seems to segment them only\n",
      "\"by accident\". Thus it is no wonder that Morfessor segments much larger\n",
      "proportion of the semantically non-compositional compounds. A fair experiment\n",
      "would include an equal number of compounds that _should_ be segmented to their\n",
      "constituents.\n",
      "\n",
      "Another problem in the evaluations (in 4.2 and 4.3) concerns hyperparameter\n",
      "tuning. The hyperparameters of MORSE are optimized on a tuning data, but\n",
      "apparently the hyperparameters of Morfessor are not. The recent versions of\n",
      "Morfessor (Kohonen et al. 2010, Grönroos et al. 2014) have a single\n",
      "hyperparameter that can be used to balance precision and recall of the\n",
      "segmentation. Given that the MORSE outperforms Morfessor both in precision and\n",
      "recall in many cases, this does not affect the conclusions, but should at least\n",
      "be mentioned.\n",
      "\n",
      "Some important details of the evaluations and results are missing: The\n",
      "\"morpheme-level evaluation\" method in 5.2 should be described or referred to.\n",
      "Moreover, Table 7 seems to compare results from different evaluation sets: the\n",
      "Morfessor and Base Inference methods seem to be from official Morpho Challenge\n",
      "evaluations, LLSM is from Narasimhan et al. (2015), who uses aggregated data\n",
      "from Morpho Challenges (probably including both development and training sets),\n",
      "and MORSE is evaluated Morpho Challenges 2010 development set. This might not\n",
      "affect the conclusions, as the differences in the scores are rather large, but\n",
      "it should definitely be mentioned.\n",
      "\n",
      "The software package does not seem to support training, only testing an\n",
      "included model for English.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The paper puts a quite lot of focus on the issue of segmenting semantically\n",
      "non-compositional compounds. This is problematic in two ways: First, as\n",
      "mentioned above, the proposed method does not seem to provide sensible way of\n",
      "segmenting _any_ compound. Second, finding the level of lexicalized base forms\n",
      "(e.g. freshman) and the morphemes as smallest meaning-bearing units (fresh,\n",
      "man) are two different tasks with different use cases (for example, the former\n",
      "would be more sensible for phrase-based SMT and the latter for ASR). The\n",
      "unsupervised segmentation methods, such as Morfessor, typically target at the\n",
      "latter, and critizing the method for a different goal is confusing.\n",
      "\n",
      "Finally, there is certainly a continuum on the (semantic) compositionality of\n",
      "the compound, and the decision is always somewhat arbitrary. (Unfortunately\n",
      "many gold standards, including the Morpho Challenge data sets, tend to be also\n",
      "inconsistent with their decisions.)\n",
      "\n",
      "Sections 4.1 and 5.1 mention the computational efficiency and limitation to one\n",
      "million input word forms, but does not provide any details: What is the\n",
      "bottleneck here? Collecting the transformations, support sets, and clusters? Or\n",
      "the actual optimization problem? What were the computation times and how do\n",
      "these scale up?\n",
      "\n",
      "The discussion mentions a few benefits of the MORSE approach: Adaptability as a\n",
      "stemmer, ability to control precision and recall, and need for only a small\n",
      "number of gold standard segmentations for tuning. As far as I can see, all or\n",
      "some of these are true also for many of the Morfessor variants (Creutz and\n",
      "Lagus, 2005; Kohonen et al., 2010; Grönroos et al., 2014), so this is a bit\n",
      "misleading. It is true that Morfessor works usually fine as a completely\n",
      "unsupervised method, but the extensions provide at least as much flexibility as\n",
      "MORSE has.\n",
      "\n",
      "(Ref: Mathias Creutz and Krista Lagus. 2005. Inducing the Morphological Lexicon\n",
      "of a Natural Language from Unannotated Text. In Proceedings of the\n",
      "International and Interdisciplinary Conference on Adaptive Knowledge\n",
      "Representation and Reasoning (AKRR'05), Espoo, Finland, June 15-17.)\n",
      "\n",
      "- Miscellaneous:\n",
      "\n",
      "Abstract should maybe mention that this is a minimally supervised method\n",
      "(unsupervised to the typical extent, i.e. excluding hyperparameter tuning).\n",
      "\n",
      "In section 3, it should be mentioned somewhere that phi is an empty string.\n",
      "\n",
      "In section 5, it should be mentioned what specific variant (and implementation)\n",
      "of Morfessor is applied in the experiments.\n",
      "\n",
      "In the end of section 5.2, I doubt that increasing the size of the input\n",
      "vocabulary would alone improve the performance of the method for Finnish. For a\n",
      "language that is morphologically as complex, you never encounter even all the\n",
      "possible inflections of the word forms in the data, not to mention derivations\n",
      "and compounds.\n",
      "\n",
      "I would encourage improving the format of the data sets (e.g.  using something\n",
      "similar to the MC data sets): For example using \"aa\" as a separator for\n",
      "multiple analyses is confusing and makes it impossible to use the format for\n",
      "other languages.\n",
      "\n",
      "In the references, many proper nouns and abbreviations in titles are written in\n",
      "lowercase letters. Narasimhan et al. (2015) is missing all the publication\n",
      "details.\n",
      "- Strengths:\n",
      " I find the idea of using morphological compositionality to make decisions on\n",
      "segmentation quite fruitful.\n",
      "\n",
      "Motivation is quite clear\n",
      "\n",
      "The paper is well-structured\n",
      "\n",
      "- Weaknesses:\n",
      "Several points are still unclear: \n",
      "  -- how the cases of rule ambiguity are treated (see \"null->er\" examples in\n",
      "general discussion)\n",
      "  -- inference stage seems to be suboptimal\n",
      "  -- the approach is limited to known words only\n",
      "\n",
      "- General Discussion:\n",
      "The paper presents semantic-aware method for morphological segmentation. The\n",
      "method considers sets of simple morphological composition rules, mostly\n",
      "appearing as 'stem plus suffix or prefix'. The approach seems to be quite\n",
      "plausible and the motivation behind is clear and well-argumented.\n",
      "\n",
      "The method utilizes the idea of vector difference to evaluate semantic\n",
      "confidence score for a proposed transformational rule. It's been previously\n",
      "shown by various studies that morpho-syntactic relations are captured quite\n",
      "well by doing word analogies/vector differences. But, on the other hand, it has\n",
      "also been shown that in case of derivational morphology (which has much less\n",
      "regularity than inflectional) the performance substantially drops (see\n",
      "Gladkova, 2016; Vylomova, 2016). \n",
      "\n",
      " The search space in the inference stage although being tractable, still seems\n",
      "to be far from optimized (to get a rule matching \"sky->skies\" the system first\n",
      "needs to searhc though the whole R_add set and, probably, quite huge set of\n",
      "other possible substitutions) and limited to known words only (for which we can\n",
      "there exist rules). \n",
      "\n",
      " It is not clear how the rules for the transformations which are\n",
      "orthographically the same, but semantically completely different are treated.\n",
      "For instance, consider \"-er\" suffix. On one hand, if used with verbs, it\n",
      "transforms them into agentive nouns, such as \"play->player\". On the other hand,\n",
      "it could also be used with adjectives for producing comparative form, for\n",
      "instance, \"old->older\". Or consider \"big->bigger\" versus \"dig->digger\".\n",
      "More over, as mentioned before, there is quite a lot of irregularity in\n",
      "derivational morphology. The same suffix might play various roles. For\n",
      "instance, \"-er\" might also represent patiental meanings (like in \"looker\"). Are\n",
      "they merged into a single rule/cluster? \n",
      "\n",
      " No exploration of how the similarity threshold and measure may affect the\n",
      "performance is presented.\n",
      "This paper presents a dialogue agent where the belief tracker and the dialogue\n",
      "manager are jointly optimised using the reinforce algorithm. It learns from\n",
      "interaction with a user simulator. There are two training phases. The first is\n",
      "an imitation learning phase where the system is initialised using supervising\n",
      "learning from a rule-based model. Then there is a reinforcement learning phase\n",
      "where the system has jointly been optimised using the RL objective.\n",
      "\n",
      "- Strengths: This paper presents a framework where a differentiable access to\n",
      "the KB is integrated in the joint optimisation. This is the biggest\n",
      "contribution of the paper. \n",
      "\n",
      "- Weaknesses: Firstly, this is not a truly end-to-end system considering the\n",
      "response generation was handcrafted rather than learnt. Also, their E2E model\n",
      "actually overfits to the simulator and performs poorly in human evaluation.\n",
      "This begs the question whether the authors are actually selling the idea of E2E\n",
      "learning or the soft-KB access. The soft-KB access actually brings consistent\n",
      "improvement, however the idea of end-to-end learning not so much. The authors\n",
      "tried to explain the merits of E2E in Figure 5 but I also fail to see the\n",
      "difference. In addition, the authors didn't motivate the reason for using the\n",
      "reinforce algorithm which is known to suffer from high variance problem. They\n",
      "didn't attempt to improve it by using a baseline or perhaps considering the\n",
      "natural actor-critic algorithm which is known to perform better.\n",
      "\n",
      "- General Discussion: Apart from the mentioned weaknesses, I think the\n",
      "experiments are solid and this is generally an acceptable paper. However, if\n",
      "they crystallised the paper around the idea which actually improves the\n",
      "performance (the soft KB access) but not the idea of E2E learning the paper\n",
      "would be better.\n",
      "- Strengths: Great paper: Very well-written, interesting results, creative\n",
      "method, good and enlightening comparisons with earlier approaches. In addition,\n",
      "the corpus, which is very carefully annotated, will prove to be a valuable\n",
      "resource for other researchers. I appreciated the qualitative discussion in\n",
      "section 5. Too many ML papers just give present a results table without much\n",
      "further ado, but the discussion in this paper really provides insights for the\n",
      "reader. \n",
      "\n",
      "- Weaknesses: In section 4.1, the sentence \"The rest of the model’s input is\n",
      "set to zeroes...\" is quite enigmatic until you look at Figure 2. Some extra\n",
      "sentence here explaining what is going on would be helpful. Furthermore, in\n",
      "Figure 2, in the input layers to the LSTMs it says \"5*Embeddings(50D)\" also for\n",
      "the networks taking dependency labels as input. Surely this is wrong? (Or if it\n",
      "is correct, please explain what you mean). \n",
      "\n",
      "- General Discussion: Concerning the comment in 4.2 \"LSTMs are excellent at\n",
      "modelling language sequences ... which is why we use this type of model.\". This\n",
      "comment seems strange to me. This is not a sequential problem in that sense.\n",
      "For each datapoint, you feed the network all 5 words in an example in one go,\n",
      "and the next example has nothing to do with the preceding one. The LSTM\n",
      "architecture could still be superior, of course, but not for the reason you\n",
      "state. Or have I misunderstood something? I'd be interested to hear the\n",
      "authors' comments on this point.\n",
      "- Update after rebuttal\n",
      "\n",
      "I appreciate the authors taking the time to clarify their implementation of the\n",
      "baseline and to provide some evidence of the significance of the improvements\n",
      "they report. These clarifications should definitely be included in the\n",
      "camera-ready version. I very much like the idea of using visual features for\n",
      "these languages, and I am looking forward to seeing how they help more\n",
      "difficult tasks in future work.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "- Thinking about Chinese/Japanese/Korean characters visually is a great idea!\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "- Experimental results show only incremental improvement over baseline, and the\n",
      "choice of evaluation makes it hard to verify one of the central arguments: that\n",
      "visual features improve performance when processing rare/unseen words.\n",
      "\n",
      "- Some details about the baseline are missing, which makes it difficult to\n",
      "interpret the results, and would make it hard to reproduce the work.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The paper proposes the use of computer vision techniques (CNNs applied to\n",
      "images of text) to improve language processing for Chinese, Japanese, and\n",
      "Korean, languages in which characters themselves might be compositional. The\n",
      "authors evaluate their model on a simple text-classification task (assigning\n",
      "Wikipedia page titles to categories). They show that a simple one-hot\n",
      "representation of the characters outperforms the CNN-based representations, but\n",
      "that the combination of the visual representations with standard one-hot\n",
      "encodings performs better than the visual or the one-hot alone. They also\n",
      "present some evidence that the visual features outperform the one-hot encoding\n",
      "on rare words, and present some intuitive qualitative results suggesting the\n",
      "CNN learns good semantic embeddings of the characters.\n",
      "\n",
      "I think the idea of processing languages like Chinese and Japanese visually is\n",
      "a great one, and the motivation for this paper makes a lot of sense. However, I\n",
      "am not entirely convinced by the experimental results. The evaluations are\n",
      "quite weak, and it is hard to say whether these results are robust or simply\n",
      "coincidental. I would prefer to see some more rigorous evaluation to make the\n",
      "paper publication-ready. If the results are statistically significant (if the\n",
      "authors can indicate this in the author response), I would support accepting\n",
      "the paper, but ideally, I would prefer to see a different evaluation entirely.\n",
      "\n",
      "More specific comments below:\n",
      "\n",
      "- In Section 3, paragraph \"lookup model\", you never explicitly say which\n",
      "embeddings you use, or whether they are tuned via backprop the way the visual\n",
      "embeddings are. You should be more clear about how the baseline was\n",
      "implemented. If the baseline was not tuned in a task-specific way, but the\n",
      "visual embeddings were, this is even more concerning since it makes the\n",
      "performances substantially less comparable.\n",
      "\n",
      "- I don't entirely understand why you chose to evaluate on classifying\n",
      "wikipedia page titles. It seems that the only real argument for using the\n",
      "visual model is its ability to generalize to rare/unseen characters. Why not\n",
      "focus on this task directly? E.g. what about evaluating on machine translation\n",
      "of OOV words? I agree with you that some languages should be conceptualized\n",
      "visually, and sub-character composition is important, but the evaluation you\n",
      "use does not highlight weaknesses of the standard approach, and so it does not\n",
      "make a good case for why we need the visual features. \n",
      "\n",
      "- In Table 5, are these improvements statistically significant?\n",
      "\n",
      "- It might be my fault, but I found Figure 4 very difficult to understand.\n",
      "Since this is one of your main results, you probably want to present it more\n",
      "clearly, so that the contribution of your model is very obvious. As I\n",
      "understand it, \"rank\" on the x axis is a measure of how rare the word is (I\n",
      "think log frequency?), with the rarest word furthest to the left? And since the\n",
      "visual model intersects the x axis to the left of the lookup model, this means\n",
      "the visual model was \"better\" at ranking rare words? Why don't both models\n",
      "intersect at the same point on the x axis, aren't they being evaluated on the\n",
      "same set of titles and trained with the same data? In the author response, it\n",
      "would be helpful if you could summarize the information this figure is supposed\n",
      "to show, in a more concise way. \n",
      "\n",
      "- On the fallback fusion, why not show performance for for different\n",
      "thresholds? 0 seems to be an edge-case threshold that might not be\n",
      "representative of the technique more generally.\n",
      "\n",
      "- The simple/traditional experiment for unseen characters is a nice idea, but\n",
      "is presented as an afterthought. I would have liked to see more eval in this\n",
      "direction, i.e. on classifying unseen words\n",
      "\n",
      "- Maybe add translations to Figure 6, for people who do not speak Chinese?\n",
      "This paper proposed to explore discourse structure, as defined by Rhetorical\n",
      "Structure Theory (RST) to improve text categorization. A RNN with attention\n",
      "mechanism is employed to compute a representation of text. The experiments on\n",
      "various of dataset shows the effectiveness of the proposed method. Below are my\n",
      "comments:\n",
      "\n",
      "(1) From Table 2, it shows that “UNLABELED” model performs better on four\n",
      "out of five datasets than the “FULL” model. The authors should explain more\n",
      "about this, because intuitively, incorporating additional relation labels\n",
      "should bring some benefits. Is the performance of relation labelling so bad and\n",
      "it hurts the performance instead?\n",
      "\n",
      "(2) The paper also transforms the RST tree into a dependency structure as a\n",
      "pre-process step. Instead of transforming, how about keep the original tree\n",
      "structure and train a hierarchical model on that?\n",
      "\n",
      "(3) For the experimental datasets, instead of comparing with only one dataset\n",
      "with each of the previous work, the authors may want to run experiments on more\n",
      "common datasets used by previous work.\n",
      "- Strengths:\n",
      "\n",
      "The main strength of this paper is the incorporation of discourse structure in\n",
      "the DNN's attention model, which allows the model to learn the weights given to\n",
      "different EDUs.\n",
      "\n",
      "Also the paper is very clear, and provides a good explanation of both RST and\n",
      "how it is used in the model.\n",
      "Finally, the evaluation experiments are conducted thoroughly with strong,\n",
      "state-of-the-art baselines.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "The main weakness of the paper is that the results do not strongly support the\n",
      "main claim that discourse structure can help text classification. Even the\n",
      "UNLABELED variant, which performs best and does outperform the state of the\n",
      "art, only provides minimal gains (and hurts in the legal/bills domain). The\n",
      "approach (particularly the FULL variant) seems to be too data greedy but no\n",
      "real solution is provided to address this beyond the simpler UNLABELED and ROOT\n",
      "variants.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "In general, this paper feels like a good first shot at incorporating discourse\n",
      "structure into DNN-based classification, but does not fully convince that\n",
      "RST-style structure will significantly boost performance on most tasks (given\n",
      "that it is also very costly to build a RST parser for a new domain, as would be\n",
      "needed in the legal/bill domains described in this paper). I wish the authors\n",
      "had explored or at least mentioned next steps in making this approach work, in\n",
      "particular in the face of data sparsity. For example, how about defining\n",
      "(task-independent) discourse embeddings? Would it be possible to use a DNN for\n",
      "discourse parsing that could be incorporated in the main task DNN and optimized\n",
      "jointly  end-to-end? Again, this is good work, I just wish the authors had\n",
      "pushed it a little further given the mixed results.\n",
      "This paper details a method of achieving translation from morphologically\n",
      "impoverished languages (e.g. Chinese) to morphologically rich ones (e.g.\n",
      "Spanish) in a two-step process. First, a system translates into a simplified\n",
      "version of the target language. Second, a system chooses morphological features\n",
      "for each generated target word, and inflects the words based on those features.\n",
      "\n",
      "While I wish the authors would apply the work to more than one language pair, I\n",
      "believe the issue addressed by this work is one of the most important and\n",
      "under-addressed problems with current MT systems. The approach taken by the\n",
      "authors is very different than many modern approaches based on BPE and\n",
      "character-level models, and instead harkens back to approaches such as\n",
      "\"Factored Translation Models\" (Koehn and Hoang, 2007) and \"Translating into\n",
      "Morphologically Rich Languages with Synthetic Phrases\" (Chahuneau et a. 2013),\n",
      "both of which are unfortunately uncited.\n",
      "\n",
      "I am also rather suspicious of the fact that the authors present only METEOR\n",
      "results and no BLEU or qualitative improvements. If BLEU scores do not rise,\n",
      "perhaps the authors could argue why they believe their approach is still a net\n",
      "plus, and back the claim up with METEOR and example sentences.\n",
      "\n",
      "Furthermore, the authors repeatedly talk about gender and number as the two\n",
      "linguistic features they seek to correctly handle, but seem to completely\n",
      "overlook person. Perhaps this is because first and second person pronouns and\n",
      "verbs rarely occur in news, but certainly this point at least merits brief\n",
      "discussion. I would also like to see some discussion of why rescoring hurts\n",
      "with gender. If the accuracy is very good, shouldn the reranker learn to just\n",
      "keep the 1-best?\n",
      "\n",
      "Finally, while the content of this paper is good overall, it has a huge amount\n",
      "of spelling, grammar, word choice, and style errors that render it unfit for\n",
      "publication in its current form. Below is dump of some errors that I found.\n",
      "\n",
      "Overall, I would like to this work in a future conference, hopefully with more\n",
      "than one language pair, more evaluation metrics, and after further\n",
      "proofreading.\n",
      "\n",
      "General error dump:\n",
      "Line 062: Zhand --> Zhang\n",
      "Line 122: CFR --> CRF\n",
      "Whole related work section: consistent use of \\cite when \\newcite is\n",
      "appropriate\n",
      "It feels like there's a lot of filler: \"it is important to mention that\", \"it\n",
      "is worth mentioning that\", etc\n",
      "Line 182, 184: \"The popular phrase-based MT system\" = moses? or PBMT systems in\n",
      "general?\n",
      "Line 191: \"a software\"\n",
      "Line 196: \"academic and commercial level\" -- this should definitely be\n",
      "pluralized, but are these even levels?\n",
      "Line 210: \"a morphology-based simplified target\" makes it sound like this\n",
      "simplified target uses morphology. Perhaps the authors mean \"a morphologically\n",
      "simplified target\"?\n",
      "Line 217: \"decide on the morphological simplifications\"?\n",
      "Table 1: extra space in \"cuestión\" on the first line and \"titulado\" in the\n",
      "last line.\n",
      "Table 1: Perhaps highlight differences between lines in this table somehow?\n",
      "How is the simplification carried out? Is this simplifier hand written by the\n",
      "authors, or does it use an existing tool?\n",
      "Line 290: i.e. --> e.g.\n",
      "Line 294: \"train on\" or \"train for\"\n",
      "Line 320: \"our architecture is inspired by\" or \"Collobert's proposal inspires\n",
      "our architecture\"\n",
      "Line 324: drop this comma\n",
      "Line 338: This equation makes it look like all words share the same word vector\n",
      "W\n",
      "Line 422: This could also be \"casas blancas\", right? How does the system choose\n",
      "between the sg. and pl. forms? Remind the reader of the source side\n",
      "conditioning here.\n",
      "Line 445: This graph is just a lattice, or perhaps more specifically a \"sausage\n",
      "lattice\"\n",
      "Line 499: Insert \"e.g.\" or similiar: (e.g. producirse)\n",
      "Line 500: misspelled \"syllable\"\n",
      "Line 500/503: I'd like some examples or further clarity on what palabras llanas\n",
      "and palabras estrújulas are and how you handle all three of these special\n",
      "cases.\n",
      "Line 570: \"and sentences longer than 50 words\"\n",
      "Line 571: \"by means of zh-seg\" (no determiner) or \"by means of the zh-seg tool\"\n",
      "Line 574: are you sure this is an \"and\" and not an \"or\"?\n",
      "Line 596: \"trained for\" instead of \"trained on\"\n",
      "Line 597: corpus --> copora\n",
      "Line 604: size is --> sizes are\n",
      "Line 613: would bigger embedding sizes help? 1h and 12h are hardly unreasonable\n",
      "training times.\n",
      "Line 615: \"seven and five being the best values\"\n",
      "Line 617: Why 70? Increased from what to 70?\n",
      "Table 3: These are hyperparameters and not just ordinary parameters of the\n",
      "model\n",
      "Line 650: \"coverage exceeds 99%\"?\n",
      "Line 653: \"descending\"\n",
      "Line 666: \"quadratic\"\n",
      "Line 668: space before \\cites\n",
      "Line 676: \"by far\" or \"by a large margin\" instead of \"by large\"\n",
      "Line 716: below\n",
      "Line 729: \"The standard phrase-based ...\"\n",
      "zh-seg citation lists the year as 2016, but the tool actually was released in\n",
      "2009\n",
      "The paper describes a method for improving two-step translation using deep\n",
      "learning. Results are presented for Chinese->Spanish translation, but the\n",
      "approach seems to be largely language-independent.\n",
      "\n",
      "The setting is fairly typical for two-step MT. The first step translates into a\n",
      "morphologically underspecified version of the target language. The second step\n",
      "then uses machine learning to fill in the missing morphological categories and\n",
      "produces the final system output by inflecting the underspecified forms (using\n",
      "a morphological generator). The main novelty of this work is the choice of deep\n",
      "NNs as classifiers in the second step. The authors also propose a rescoring\n",
      "step which uses a LM to select the best variant.\n",
      "\n",
      "Overall, this is solid work with good empirical results: the classifier models\n",
      "reach a high accuracy (clearly outperforming baselines such as SVMs) and the\n",
      "improvement is apparent even in the final translation quality.\n",
      "\n",
      "My main problem with the paper is the lack of a comparison with some\n",
      "straightforward deep-learning baselines. Specifically, you have a structured\n",
      "prediction problem and you address it with independent local decisions followed\n",
      "by a rescoring step. (Unless I misunderstood the approach.) But this is a\n",
      "sequence labeling task which RNNs are well suited for. How would e.g. a\n",
      "bidirectional LSTM network do when trained and used in the standard sequence\n",
      "labeling setting? After reading the author response, I still think that\n",
      "baselines (including the standard LSTM) are run in the same framework, i.e.\n",
      "independently for each local label. If that's not the case, it should have been\n",
      "clarified better in the response. This is a problem because you're not using\n",
      "the RNNs in the standard way and yet you don't justify why your way is better\n",
      "or compare the two approaches.\n",
      "\n",
      "The final re-scoring step is not entirely clear to me. Do you rescore n-best\n",
      "sentences? What features do you use? Or are you searching a weighted graph for\n",
      "the single optimal path? This needs to be explained more clearly in the paper.\n",
      "(My current impression is that you produce a graph, then look for K best paths\n",
      "in it, generate the inflected sentences from these K paths and *then* use a LM\n",
      "-- and nothing else -- to select the best variant. But I'm not sure from\n",
      "reading the paper.) This was not addressed in the response.\n",
      "\n",
      "You report that larger word embeddings lead to a longer training time. Do they\n",
      "also influence the final results?\n",
      "\n",
      "Can you attempt to explain why adding information from the source sentence\n",
      "hurts? This seems a bit counter-intuitive -- does e.g. the number information\n",
      "not get entirely lost sometimes because of this? I would appreciate a more\n",
      "thorough discussion on this in the final version, perhaps with a couple of\n",
      "convincing examples.\n",
      "\n",
      "The paper contains a number of typos and the general level of English may not\n",
      "be sufficient for presentation at ACL.\n",
      "\n",
      "Minor corrections:\n",
      "\n",
      "context of the application of MT -> context of application for MT\n",
      "\n",
      "In this cases, MT is faced in two-steps -> In this case, MT is divided into two\n",
      "steps\n",
      "\n",
      "markov -> Markov\n",
      "\n",
      "CFR -> CRF\n",
      "\n",
      "task was based on a direct translation -> task was based on direct translation\n",
      "\n",
      "task provided corpus -> task provided corpora\n",
      "\n",
      "the phrase-based system has dramatically -> the phrase-based approach...\n",
      "\n",
      "investigated different set of features -> ...sets of features\n",
      "\n",
      "words as source of information -> words as the source...\n",
      "\n",
      "correspondant -> corresponding\n",
      "\n",
      "Classes for gender classifier -> Classes for the...\n",
      "\n",
      "for number classifier -> for the...\n",
      "\n",
      "This layer's input consists in -> ...consists of\n",
      "\n",
      "to extract most relevant -> ...the most...\n",
      "\n",
      "Sigmoid does not output results in [-1, 1] but rather (0, 1). A tanh layer\n",
      "would produce (-1, 1).\n",
      "\n",
      "information of a word consists in itself -> ...of itself\n",
      "\n",
      "this $A$ set -> the set $A$\n",
      "\n",
      "empty sentences and longer than 50 words -> empty sentences and sentences\n",
      "longer than...\n",
      "\n",
      "classifier is trained on -> classifier is trained in\n",
      "\n",
      "aproximately -> approximately\n",
      "\n",
      "coverage raises the 99% -> coverage exceeds 99% (unless I misunderstand)\n",
      "\n",
      "in descendant order -> in descending order\n",
      "\n",
      "cuadratic -> quadratic (in multiple places)\n",
      "\n",
      "but best results -> but the best results\n",
      "\n",
      "Rescoring step improves -> The rescoring step...\n",
      "\n",
      "are not be comparable -> are not comparable\n",
      "This paper presents a method for generating morphology, focusing on gender and\n",
      "number, using deep learning techniques. From a morphologically simplified\n",
      "Spanish text, the proposed approach uses a classifier to reassign the gender\n",
      "and number for each token, when necessary. The authors compared their approach\n",
      "with other learning algorithms, and evaluated it in machine translation on the\n",
      "Chinese-to-Spanish (Zh->Es) translation direction.\n",
      "\n",
      "Recently, the task of generating gender and number has been rarely tackled,\n",
      "morphology generation methods usually target, and are evaluated on,\n",
      "morphologically-rich languages like German or Finnish.\n",
      "However, calling the work presented in this paper “morphology\n",
      "generation“ is a bit overselling as the proposed method clearly deals only\n",
      "with\n",
      "gender and number. And given the fact that some rules are handcrafted for this\n",
      "specific task, I do not think this method can be straightforwardly applied to\n",
      "do more complex morphology generation for morphologically-rich languages.\n",
      "\n",
      "This paper is relatively clear in the sections presenting the proposed method.\n",
      "A\n",
      "lot of work has been done to design the method and I think it can have some\n",
      "interesting impact on various NLP tasks. However the evaluation part of\n",
      "this work is barely understandable as many details of what is done, or why it\n",
      "is done, are missing. From this evaluation, we cannot know if the proposed\n",
      "method brings improvements over state-of-the-art methods while the experiments\n",
      "cannot be replicated. Furthermore, no analysis of the results obtained is\n",
      "provided. Since half a page is still available, there was the possibility\n",
      "to provide more information to make more clear the evaluation. This work lacks\n",
      "of motivation. Why do you think deep learning can especially improve gender and\n",
      "number generation over state-of-the-art methods?\n",
      "\n",
      "In your paper, the word “contribution“ should be used more wisely, as it is\n",
      "now in the paper, it is not obvious what are the real contributions (more\n",
      "details below). \n",
      "\n",
      "abstract:\n",
      "what do you mean by unbalanced languages?\n",
      "\n",
      "section 1:\n",
      "You claim that your main contribution is the use of deep learning. Just the use\n",
      "of deep learning in some NLP task is not a contribution.\n",
      "\n",
      "section 2:\n",
      "You claim that neural machine translation (NMT), mentioned as “neural\n",
      "approximations“,  does not achieve state-of-the-art results for Zh->Es. I\n",
      "recommend to remove this claim from the paper, or to discuss it more, since\n",
      "Junczys-Dowmunt et al. (2016), during the last IWSLT, presented some results\n",
      "for Zh->Es with the UN corpus, showing that NMT outperforms SMT by around 10\n",
      "BLEU points.\n",
      "\n",
      "section 5.1:\n",
      "You wrote that using the Zh->Es language pair is one of your main\n",
      "contributions. Just using a language pair is not a contribution. Nonetheless, I\n",
      "think it is nice to see a paper on machine translation that does not focus of\n",
      "improving machine translation for English.\n",
      "The numbers provided in Table 2 were computed before or after preprocessing?\n",
      "Why did you remove the sentences longer than 50 tokens?\n",
      "Precise how did you obtain development and test sets, or provide them. Your\n",
      "experiments are currently no replicable especially because of that.\n",
      "\n",
      "section 5.2:\n",
      "You wrote that you used Moses and its default parameters, but the default\n",
      "parameters of Moses are not the same depending on the version, so you should\n",
      "provide the number of the version used.\n",
      "\n",
      "section 5.3:\n",
      "What do you mean by “hardware cost“?\n",
      "Table 3: more details should be provided regarding how did you obtain these\n",
      "values. You chose these values given the classifier accuracy, but how precisely\n",
      "and on what data did you train and test the classifiers? On the same data used\n",
      "in section 6?\n",
      "If I understood the experiments properly, you used simplified Spanish. But I\n",
      "cannot find in the text how do you simplify Spanish. And how do you use it to\n",
      "train the classifier and the SMT system? \n",
      "\n",
      "section 6:\n",
      "Your method is better than other classification\n",
      "algorithms, but it says nothing about how it performs compared to the\n",
      "state-of-the-art methods. You should at least precise why you chose these\n",
      "classifications algorithms for comparison. Furthermore, how your rules impact\n",
      "these results? And more generally, how do you explain such a high accuracy for\n",
      "you method?\n",
      "Did you implement all these classification algorithms by yourselves? If not,\n",
      "you must provide the URL or cite the framework you used.\n",
      "For the SMT experiments, I guess you trained your phrase table on simplified\n",
      "Spanish. You must precise it.\n",
      "You chose METEOR over other metrics like BLEU to evaluate your results. You\n",
      "must provide some explanation for this choice. I particularly appreciate when I\n",
      "see a MT paper that does not use BLEU for evaluation, but if you use METEOR,\n",
      "you must mention which version you used. METEOR has largely changed since 2005.\n",
      "You cited the paper of 2005, did you use the 2005 version? Or did you use the\n",
      "last one with paraphrases? \n",
      "Are your METEOR scores statistically significant?\n",
      "\n",
      "section 7:\n",
      "As future work you mentioned “further simplify morphology“. In this paper,\n",
      "you do not present any simplification of morphology, so I think that choosing\n",
      "the word\n",
      "“further“ is misleading.\n",
      "\n",
      "some typos:\n",
      "femenine\n",
      "ensambling\n",
      "cuadratic\n",
      "\n",
      "style:\n",
      "plain text citations should be rewritten like this: “(Toutanova et al, 2008)\n",
      "built “ should be “Toutanova et al. (2008) built “\n",
      "place the caption of your tables below the table and not above, and with more\n",
      "space between the table and its caption.\n",
      "You used the ACL 2016 template. You must use the new one prepared for ACL 2017.\n",
      "More generally, I suggest that you read again the FAQ and the submission\n",
      "instructions provided on the ACL 2017 website. It will greatly help you to\n",
      "improve the paper. There are also important information regarding references:\n",
      "you must provide DOI or URL of all ACL papers in your references.\n",
      "\n",
      "-----------------------\n",
      "\n",
      "After authors response:\n",
      "\n",
      "Thank you for your response.\n",
      "\n",
      "You wrote that rules are added just as post-processing, but does it mean that\n",
      "you do not apply them to compute your classification results? Or if you do\n",
      "apply them before computing these results, I'm still wondering about their\n",
      "impact on these results.\n",
      "\n",
      "You wrote that Spanish is simplified as shown in Table 1, but it does not\n",
      "answer my question: how did you obtain these simplifications exactly? (rules?\n",
      "software? etc.) The reader need to now that to reproduce your approach.\n",
      "\n",
      "The classification algorithms presented in Table 5 are not state-of-the-art, or\n",
      "if they are you need to cite some paper. Furthermore, this table only tells\n",
      "that deep learning gives the best results for classification, but it does not\n",
      "tell at all if your approach is better than state-of-the-art approach for\n",
      "machine translation. You need to compare your approach with other\n",
      "state-of-the-art morphology generation approaches (described in related work)\n",
      "designed for machine translation. If you do that your paper will be much more\n",
      "convincing in my opinion.\n",
      "- Strengths:\n",
      "The idea of hard monotonic attention is new and substantially different from\n",
      "others.\n",
      "\n",
      "- Weaknesses:\n",
      "The experiment results on morphological inflection generation is somewhat\n",
      "mixed. The proposed model is effective if the amount of training data is small\n",
      "(such as CELEX). It is also effective if the alignment is mostly monotonic and\n",
      "less context sensitive (such as Russian, German and Spanish).\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The authors proposed a novel neural model for morphological inflection\n",
      "generation which uses \"hard attention\", character alignments separately\n",
      "obtained by using a Bayesian method for transliteration. It is substantially\n",
      "different from the previous state of the art neural model for the task which\n",
      "uses \"soft attention\", where character alignment and conversion are solved\n",
      "jointly in the probabilistic model.\n",
      "\n",
      "The idea is novel and sound. The paper is clearly written. The experiment is\n",
      "comprehensive. The only concern is that the proposed method is not necessarily\n",
      "the state of the art in all conditions. It is suitable for the task with mostly\n",
      "monotonic alignment and with less context sensitive phenomena. The paper would\n",
      "be more convincing if it describe the practical merits of the proposed method,\n",
      "such as the ease of implementation and computational cost.\n",
      "- Strengths: A new encoder-decoder model is proposed that explicitly takes \n",
      "into account monotonicity.\n",
      "\n",
      "- Weaknesses: Maybe the model is just an ordinary BiRNN with alignments\n",
      "de-coupled.\n",
      "Only evaluated on morphology, no other monotone Seq2Seq tasks.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The authors propose a novel encoder-decoder neural network architecture with\n",
      "\"hard monotonic attention\". They evaluate it on three morphology datasets.\n",
      "\n",
      "This paper is a tough one. One the one hand it is well-written, mostly very\n",
      "clear and also presents a novel idea, namely including monotonicity in\n",
      "morphology tasks. \n",
      "\n",
      "The reason for including such monotonicity is pretty obvious: Unlike machine\n",
      "translation, many seq2seq tasks are monotone, and therefore general\n",
      "encoder-decoder models should not be used in the first place. That they still\n",
      "perform reasonably well should be considered a strong argument for neural\n",
      "techniques, in general. The idea of this paper is now to explicity enforce a\n",
      "monotonic output character generation. They do this by decoupling alignment and\n",
      "transduction and first aligning input-output sequences monotonically and\n",
      "then training to generate outputs in agreement with the monotone alignments.\n",
      "However, the authors are unclear on this point. I have a few questions:\n",
      "\n",
      "1) How do your alignments look like? On the one hand, the alignments seem to\n",
      "be of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input\n",
      "character can be aligned with zero, 1, or several output characters. However,\n",
      "this seems to contrast with the description given in lines 311-312 where the\n",
      "authors speak of several input characters aligned to 1 output character. That\n",
      "is, do you use 1-to-many, many-to-1 or many-to-many alignments?\n",
      "\n",
      "2) Actually, there is a quite simple approach to monotone Seq2Seq. In a first\n",
      "stage, align input and output characters monotonically with a 1-to-many\n",
      "constraint (one can use any monotone aligner, such as the toolkit of\n",
      "Jiampojamarn and Kondrak). Then one trains a standard sequence tagger(!) to\n",
      "predict exactly these 1-to-many alignments. For example, flog->fliege (your\n",
      "example on l.613): First align as in \"f-l-o-g / f-l-ie-ge\". Now use any tagger\n",
      "(could use an LSTM, if you like) to predict \"f-l-ie-ge\" (sequence of length 4)\n",
      "from \"f-l-o-g\" (sequence of length 4). Such an approach may have been suggested\n",
      "in multiple papers, one reference could be [*, Section 4.2] below. \n",
      "My two questions here are: \n",
      "\n",
      "2a) How does your approach differ from this rather simple idea?\n",
      "\n",
      "2b) Why did you not include it as a baseline?\n",
      "\n",
      "Further issues:\n",
      "\n",
      "3) It's really a pitty that you only tested on morphology, because there are\n",
      "many other interesting monotonic seq2seq tasks, and you could have shown your\n",
      "system's superiority by evaluating on these, given that you explicitly model\n",
      "monotonicity (cf. also [*]).\n",
      "\n",
      "4) You perform \"on par or better\" (l.791). There seems to be a general\n",
      "cognitive bias among NLP researchers to map instances where they perform worse\n",
      "to\n",
      "\"on par\" and all the rest to \"better\". I think this wording should be\n",
      "corrected, but otherwise I'm fine with the experimental results.\n",
      "\n",
      "5) You say little about your linguistic features: From Fig. 1, I infer that\n",
      "they include POS, etc. \n",
      "\n",
      "5a) Where did you take these features from?\n",
      "\n",
      "5b) Is it possible that these are responsible for your better performance in\n",
      "some cases, rather than the monotonicity constraints?\n",
      "\n",
      "Minor points:\n",
      "\n",
      "6) Equation (3): please re-write $NN$ as $\\text{NN}$ or similar\n",
      "\n",
      "7) l.231 \"Where\" should be lower case\n",
      "\n",
      "8) l.237 and many more: $x_1\\ldots x_n$. As far as I know, the math community\n",
      "recommends to write $x_1,\\ldots,x_n$ but $x_1\\cdots x_n$. That is, dots should\n",
      "be on the same level as surrounding symbols.\n",
      "\n",
      "9) Figure 1: is it really necessary to use cyrillic font? I can't even address\n",
      "your example here, because I don't have your fonts.\n",
      "\n",
      "10) l.437: should be \"these\"\n",
      "\n",
      "[*] \n",
      "\n",
      "@InProceedings{schnober-EtAl:2016:COLING, \n",
      "\n",
      "  author    = {Schnober, Carsten  and  Eger, Steffen  and  Do Dinh,\n",
      "Erik-L\\^{a}n  and  Gurevych, Iryna},\n",
      "  title     = {Still not there? Comparing Traditional Sequence-to-Sequence\n",
      "Models to Encoder-Decoder Neural Networks on Monotone String Translation\n",
      "Tasks},\n",
      "  booktitle = {Proceedings of COLING 2016, the 26th International Conference on\n",
      "Computational Linguistics: Technical Papers},\n",
      "  month     = {December},\n",
      "  year                                                      = {2016},\n",
      "  address   = {Osaka, Japan},\n",
      "  publisher = {The COLING 2016 Organizing Committee},\n",
      "  pages     = {1703--1714},\n",
      "  url                                               =\n",
      "{http://aclweb.org/anthology/C16-1160}\n",
      "\n",
      "}\n",
      "\n",
      "AFTER AUTHOR RESPONSE\n",
      "\n",
      "Thanks for the clarifications. I think your alignments got mixed up in the\n",
      "response somehow (maybe a coding issue), but I think you're aligning 1-0, 0-1,\n",
      "1-1, and later make many-to-many alignments from these. \n",
      "I know that you compare to  Nicolai, Cherry and Kondrak (2015) but my question\n",
      "would have rather been: why not use 1-x (x in 0,1,2) alignments as in  Schnober\n",
      "et al. and then train a neural tagger on these (e.g. BiLSTM). I wonder how much\n",
      "your results would have differed from such a rather simple baseline. (A tagger\n",
      "is a monotone model to start with and given the monotone alignments, everything\n",
      "stays monotone. In contrast, you start out with a more general model and then\n",
      "put hard monotonicity constraints on this ...)\n",
      "\n",
      "NOTES FROM AC\n",
      "\n",
      "Also quite relevant is Cohn et al. (2016),\n",
      "http://www.aclweb.org/anthology/N16-1102 .\n",
      "\n",
      "Isn't your architecture also related to methods like the Stack LSTM, which\n",
      "similarly predicts a sequence of actions that modify or annotate an input?  \n",
      "\n",
      "Do you think you lose anything by using a greedy alignment, in contrast to\n",
      "Rastogi et al. (2016), which also has hard monotonic attention but sums over\n",
      "all alignments?\n",
      "- Strengths:\n",
      "*- Task\n",
      "*- Simple model, yet the best results on SQuAD (single model0\n",
      "*- Evaluation and comparison\n",
      "\n",
      "- Weaknesses:\n",
      "*- Analysis of errors/results (See detailed comments below)\n",
      "\n",
      "- General Discussion:\n",
      "In this paper the authors present a method for directly querying Wikipedia to\n",
      "answer open domain questions. The system consist of two components - a module\n",
      "to query/fetch wikipedia articles and a module to answer the question given the\n",
      "fetched set of wikipedia articles. \n",
      "\n",
      "The document retrieval system is a traditional IR system relying on term\n",
      "frequency models and ngram counts.  The answering system uses a feature\n",
      "representation for paragraphs that consists of word embeddings, indicator\n",
      "features to determine whether a paragraph word occurs in a question,\n",
      "token-level features including POS, NER etc and a soft feature for capturing\n",
      "similarity between question and paragraph tokens in embedding space. A combined\n",
      "feature representation is used as an input to a bi-direction LSTM RNN for\n",
      "encoding. For questions an RNN that works on the word embeddings is used. \n",
      "These are then used to train an overall classifier independently for start and\n",
      "end spans of sentences within a paragraph to answer questions.\n",
      "\n",
      "The system has been trained using different Open Domain QA datasets such as\n",
      "SQuAD and WebQuestions by modifying the training data to include articles\n",
      "fetched by the IR engine instead of just the actual correct document/passage.\n",
      "\n",
      "Overall, an easy to follow interesting paper but I had a few questions:\n",
      "1) The IR system has a Accuracy@5 of over 75 %, and individually the document\n",
      "reader performs well and can beat the best single models on SquAD. What\n",
      "explains the significant drop in Table 6. The authors mention that instead of\n",
      "the fetched results, if they test using the best paragraph the accuracy reaches\n",
      "just 0.49 (from 0.26) but that is still significantly below the 0.78-79 in the\n",
      "SQuAD task.  So, presumably the error is this large because the neural network\n",
      "for matching isnt doing as good a job in learning the answers when using the\n",
      "modified training set (which includes fetched articles) instead of the case\n",
      "when training and testing is done for the document understanding task. Some\n",
      "analysis of whats going on here should be provided. What was the training\n",
      "accuracy in the both cases? What can be done to improve it? To be fair, the\n",
      "authors to allude to this in the conclusion but I think it still needs to be\n",
      "part of the paper to provide some meaningful insights.\n",
      "\n",
      "2) I understand the authors were interested in treating this as a pure machine\n",
      "comprehension task and therefore did not want to rely on external sources such\n",
      "as Freebase which could have helped with entity typing        but that would have\n",
      "been interesting to use. Tying back to my first question -- if the error is due\n",
      "to highly relevant topical sentences as the authors mention, could entity\n",
      "typing have helped?\n",
      "\n",
      "The authors should also refer to QuASE (Sun et. al 2015 at WWW2015) and similar\n",
      "systems in their related work. QuASE is also an Open domain QA system that\n",
      "answers using fetched passages - but it relies on the web instead of just\n",
      "Wikipedia.\n",
      "- Strengths:\n",
      "\n",
      "The authors focus on a very challenging task of answering open-domain question\n",
      "from Wikipedia. Authors have developed 1) a document retriever to retrieve\n",
      "relevant Wikipedia articles for a question, and 2) Document retriever to\n",
      "retrieve the exact answer from the retrieved paragraphs. \n",
      "Authors used Distant Supervision to fine-tune their model. Experiments show\n",
      "that the document reader performs better than WikiSearch API, and Document\n",
      "Reader model does better than some recent models for QA.\n",
      "\n",
      "- Weaknesses:\n",
      "The final results are inferior to some other models, as presented by the\n",
      "authors. Also, no error analysis is provided.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The proposed systems by the authors is end-to-end and interesting. However, I\n",
      "have some concerns below.\n",
      "\n",
      "Document Retriever: Authors have shown a better retrieval performance than Wiki\n",
      "Search. However, it is not described as to how exactly the API is used.\n",
      "WikiSearch may not be a good baseline for querying \"questions\" (API suits\n",
      "structured retrieval more). Why don't the authors use some standard IR\n",
      "baselines for this?\n",
      "\n",
      "Distant Supervision: How effective and reliable was distant supervision?\n",
      "Clearly, the authors had to avoid using many training examples because of this,\n",
      "but whatever examples the authors could use, what fraction was actually \"close\n",
      "to correct\"? Some statistics would be helpful to understand if some more\n",
      "fine-tuning of distant supervision could have helped.\n",
      "\n",
      "Full Wikipedia results: This was the main aim of the authors and as authors\n",
      "themselves said, the full system gives a performance of 26.7 (49.6 when correct\n",
      "doc given, 69.5 when correct paragraph is given). Clearly, that should be a\n",
      "motivation to work more on the retrieval aspect? For WebQuestions, the results\n",
      "are much inferior to YodaQA, and that raises the question -- whether Wikipedia\n",
      "itself is sufficient to answer all the open-domain questions? Should authors\n",
      "think of an integrated model to address this? \n",
      "\n",
      "Overall, the final results shown in Tables 4 and 5 are inferior to some other\n",
      "models. While authors only use Wikipedia, the results are not indicative of\n",
      "this being the best strategy.\n",
      "\n",
      "Other points:\n",
      "The F1 value in Table 5 (78.4) is different from that in Table 4 (Both Dev and\n",
      "Test).\n",
      "Table 5: Why not \"No f_emb\"?\n",
      "Error analysis: Some error analysis is required in various components of the\n",
      "system. \n",
      "Are there some specific type of questions, where the system does not perform\n",
      "well? Is there any way one can choose which question is a good candidate to be\n",
      "answered by Wikipedia, and use this method only for those questions?\n",
      "For WebQuestions, DS degrades the performance further.\n",
      "- Strengths:\n",
      "\n",
      "[+] Well motivated, tackles an interesting problem;\n",
      "\n",
      "[+] Clearly written and structured, accompanied by documented code and dataset;\n",
      "\n",
      "[+] Encouraging results.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "[-] Limited to completely deterministic, hand-engineered minimization rules;\n",
      "\n",
      "[-] Some relevant literature on OIE neglected;\n",
      "\n",
      "[-] Sound but not thorough experimental evaluation.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "This paper tackles a practical issue of most OIE systems, i.e. redundant,\n",
      "uninformative and inaccurate extractions. The proposed approach, dubbed MinOIE,\n",
      "is designed to actually \"minimize\" extractions by removing overly specific\n",
      "portions and turning them into structured annotations of various types\n",
      "(similarly to OLLIE). The authors put MinIE on top of a state-of-the-art OIE\n",
      "system (ClausIE) and test it on two publicly available datasets, showing that\n",
      "it effectively leads to more concise extractions compared to standard OIE\n",
      "approaches, while at the same time retaining accuracy.\n",
      "\n",
      "Overall, this work focuses on an interesting (and perhaps underinvestigated)\n",
      "aspect of OIE in a sound and principled way. The paper is clearly written,\n",
      "sufficiently detailed, and accompanied by supplementary material and a neat\n",
      "Java implementation.\n",
      "My main concern is, however, with the entirely static, deterministic and\n",
      "rule-based structure of MinIE. Even though I understand that a handful of\n",
      "manually engineered rules is technically the best strategy when precision is\n",
      "key, these approaches are typically very hard to scale, e.g. in terms of\n",
      "languages (a recent trend of OIE, see Faruqui and Kumar, 2015; Falke et al.,\n",
      "2016). In other words, I think that this contribution somehow falls short of\n",
      "novelty and substance in proposing a pipeline of engineered rules that are\n",
      "mostly inspired by other OIE systems (such as ClausIE or ReVerb); for instance,\n",
      "I would have really appreciated an attempt to learn these minimization rules\n",
      "instead of hard-coding them.\n",
      "\n",
      "Furthermore, the authors completely ignore a recent research thread on\n",
      "“semantically-informed” OIE (Nakashole et al., 2012; Moro and Navigli,\n",
      "2012; 2013; Delli Bovi et al., 2015) where traditional extractions are\n",
      "augmented with links to underlying knowledge bases and sense inventories\n",
      "(Wikipedia, Wikidata, Yago, BabelNet). These contributions are not only\n",
      "relevant in terms of related literature: in fact, having text fragments (or\n",
      "constituents) explicitly linked to a knowledge base would reduce the need for\n",
      "ad-hoc minimization rules such as those in Sections 6.1 and 6.2. In the example\n",
      "with \"Bill of Rights\" provided by the authors (line 554), an OIE pipeline with\n",
      "a proper Entity Linking module would recognize automatically the phrase as\n",
      "mention of a registered entity, regardless of the shape of its subconstituents.\n",
      "Also, an underlying sense inventory would seamlessly incorporate the external\n",
      "information about collocations and multi-word expressions used in Section 6.2:\n",
      "not by chance, the authors rely on WordNet and Wiktionary to compile their\n",
      "dictionary of collocations.\n",
      "\n",
      "Finally, some remarks on the experimental evaluation:\n",
      "\n",
      "- Despite the claim of generality of MinIE, the authors choose to experiment\n",
      "only with ClausIE as underlying OIE system (most likely the optimal match). It\n",
      "would have been very interesting to see if the improvement brought by MinIE is\n",
      "consistent also with other OIE systems, in order to actually assess its\n",
      "flexibility as a post-processing tool.\n",
      "\n",
      "- Among the test datasets used in Section 7, I would have included the recent\n",
      "OIE benchmark of Stanovsky and Dagan (2016), where results are reported also\n",
      "for comparison systems not included in this paper (TextRunner, WOIE, KrakeN).\n",
      "\n",
      "References:\n",
      "\n",
      "- Manaal Faruqui and Shankar Kumar. Multilingual Open Relation Extraction using\n",
      "Cross-lingual Projection. NAACL-HLT, 2015.\n",
      "\n",
      "- Tobias Falke, Gabriel Stanovsky, Iryna Gurevych and Ido Dagan. Porting an\n",
      "Open Information Extraction System from English to German. EMNLP 2016.\n",
      "\n",
      "- Ndapandula Nakashole, Gerhard Weikum and Fabian Suchanek. PATTY: A Taxonomy\n",
      "of Relational Patterns with Semantic Types. EMNLP 2012.\n",
      "\n",
      "- Andrea Moro, Roberto Navigli. WiSeNet: Building a Wikipedia-based Semantic\n",
      "Network with Ontologized Relations. CIKM 2012.\n",
      "\n",
      "- Andrea Moro, Roberto Navigli. Integrating Syntactic and Semantic Analysis\n",
      "into the Open Information Extraction Paradigm. IJCAI 2013.\n",
      "\n",
      "- Claudio Delli Bovi, Luca Telesca and Roberto Navigli. Large-Scale Information\n",
      "Extraction from Textual Definitions through Deep Syntactic and Semantic\n",
      "Analysis. TACL vol. 3, 2015.\n",
      "\n",
      "- Gabriel Stanovsky and Ido Dagan. Creating a Large Benchmark for Open\n",
      "Information Extraction. EMNLP 2016.\n",
      "- Strengths:\n",
      "   - The paper states clearly the contributions from the beginning \n",
      "   - Authors provide system and dataset\n",
      "   - Figures help in illustrating the approach\n",
      "   - Detailed description of the approach\n",
      "   - The authors test their approach performance on other datasets and compare\n",
      "to other published work\n",
      "\n",
      "- Weaknesses:\n",
      "   -The explanation of methods in some paragraphs is too detailed and there is\n",
      "no mention of other work and it is repeated in the corresponding method\n",
      "sections, the authors committed to address this issue in the final version.\n",
      "   -README file for the dataset [Authors committed to add README file]\n",
      "\n",
      "- General Discussion:\n",
      "   - Section 2.2 mentions examples of DBpedia properties that were used as\n",
      "features. Do the authors mean that all the properties have been used or there\n",
      "is a subset? If the latter please list them. In the authors' response, the\n",
      "authors explain in more details this point and I strongly believe that it is\n",
      "crucial to list all the features in details in the final version for clarity\n",
      "and replicability of the paper. \n",
      "   - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might\n",
      "be beneficial to add that the input is word embeddings (similarly to Lample et\n",
      "al.)\n",
      "   - Figure 3, KNs in source language or in English? (since the mentions have\n",
      "been translated to English). In the authors' response, the authors stated that\n",
      "they will correct the figure.\n",
      "   - Based on section 2.4 it seems that topical relatedness implies that some\n",
      "features are domain dependent. It would be helpful to see how much domain\n",
      "dependent features affect the performance. In the final version, the authors\n",
      "will add the performance results for the above mentioned features, as mentioned\n",
      "in their response. \n",
      "   - In related work, the authors make a strong connection to Sil and Florian\n",
      "work where they emphasize the supervised vs. unsupervised difference. The\n",
      "proposed approach is still supervised in the sense of training, however the\n",
      "generation of training data doesn’t involve human interference\n",
      "- Strengths:\n",
      "\n",
      "- Very impressive resource\n",
      "\n",
      "- fully automatic system - particularly suitable for cross-lingual learning\n",
      "across many languages\n",
      "\n",
      "- Good evaluation both within and outside wikipedia. Good comparison to works\n",
      "that employed manual resources.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "- The clarity of the paper can be improved.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "This paper presents \"a simple yet effective framework that can extract names\n",
      "from 282 languages and link them to an English KB\". Importantly, the system is\n",
      "fully automatic, which is particularly important when aiming to learn across\n",
      "such a large number of languages. Although this is far from trivial, the\n",
      "authors are able to put their results in context and provide evaluation both\n",
      "within and outside of wikipedia - I particularly like the way the put their\n",
      "work in the context of previous work that uses manual resources, it is a good\n",
      "scientific practice and I am glad they do not refrain from doing that in worry\n",
      "that this would not look good.\n",
      "\n",
      "The clarity of the paper can improve. This is not an easy paper to write due to\n",
      "the quite complex process and the very large scale resource it generates.\n",
      "However, the paper is not very well organized and at many points I felt that I\n",
      "am reading a long list of details. I encourage the authors to try and give the\n",
      "paper a better structure. As one example, I would be happy to see a better\n",
      "problem definition and high level motivations from the very beginning. Other\n",
      "examples has to do with better exposition of the motivations, decisions and\n",
      "contributions in each part of the paper (I admire the efforts the authors have\n",
      "already made, but I think this can done even better). This is an important\n",
      "paper and it deserves a clearer presentation.\n",
      "\n",
      "All in all I like the paper and think it provides an important resource. I\n",
      "would like to see this paper presented in ACL 2017.\n",
      "- Strengths:\n",
      "1. The idea of assigning variable-length document segments with dependent\n",
      "topics is novel. This prior knowledge is worth incorporated in the LDA-based\n",
      "framework.\n",
      "2. Whereas we do not have full knowledge on recent LDA literature, we find the\n",
      "part of related work quite convincing.\n",
      "3. The method proposed for segment sampling with O(M) complexity is impressive.\n",
      "It is crucial for efficient computation. \n",
      "\n",
      "- Weaknesses:\n",
      "1. Compared to Balikas COLING16's work, the paper has a weaker visualization\n",
      "(Fig 5), which makes us doubt about the actual segmenting and assigning results\n",
      "of document. It could be more convincing to give a longer exemplar and make\n",
      "color assignment consistent with topics listed in Figure 4.\n",
      "2. Since the model is more flexible than that of Balikas COLING16, it may be\n",
      "underfitting, could you please explain this more?\n",
      "\n",
      "- General Discussion:\n",
      "The paper is well written and structured. The intuition introduced in the\n",
      "Abstract and again exemplified in the Introduction is quite convincing. The\n",
      "experiments are of a full range, solid, and achieves better quantitative\n",
      "results against previous works. If the visualization part is stronger, or\n",
      "explained why less powerful visualization, it will be more confident. Another\n",
      "concern is about computation efficiency, since the seminal LDA work proposed to\n",
      "use Variational Inference which is faster during training compared to MCMC, we\n",
      "wish to see the author’s future development.\n",
      "### Strengths:\n",
      "- Well-written, well-organized\n",
      "- Incorporate topical segmentation to copula LDA to enable the joint learning\n",
      "of segmentation and latent models\n",
      "- Experimental setting is well-designed and show the superiority of the\n",
      "proposed method from several different indicators and datasets\n",
      "\n",
      "### Weaknesses:\n",
      "- No comparison with \"novel\" segmentation methods\n",
      "\n",
      "### General Discussion:\n",
      "This paper presents segLDAcop, a joint latent model for topics and segments.\n",
      "This model is based on the copula LDA and incorporates the topical segmentation\n",
      "to the copula LDA. The authors conduct comprehensive experiments by using\n",
      "several different datasets and evaluation metrics to show the superiority of\n",
      "their model.\n",
      "\n",
      "This paper is well-written and well-organized. The proposed model is a\n",
      "reasonable extension of the copula LDA to enable the joint inference of\n",
      "segmentations and topics. Experimental setting is carefully designed and the\n",
      "superiority of the proposed model is fairly validated.\n",
      "One concern is that the authors only use the simple NP segmentation and single\n",
      "word segmentation as segments of the previous method. As noted in the paper,\n",
      "there are many work to smartly generate segments before running LDA though it\n",
      "is largely affected by the bias of statistical or linguistic tools used. The\n",
      "comparison with more novel (state-of-the-art) segments would be preferable to\n",
      "precisely show the validity of the proposed method.\n",
      "\n",
      "### Minor comment\n",
      "- In line 105, \"latent radom topics\" -> \"latent random topics\"\n",
      "- Strengths:\n",
      "- The paper tackles an important issue, that is building ontologies or thesauri\n",
      "- The methods make sense and seem well chosen\n",
      "- Methods and setups are well detailed\n",
      "- It looks like the authors outperform the state-of-the-art approach (but see\n",
      "below for my concerns)\n",
      "\n",
      "- Weaknesses:\n",
      "The main weaknesses for me are evaluation and overall presentation/writing.\n",
      "- The list of baselines is hard to understand. Some methods are really old and\n",
      "it doesn't seem justified to show them here (e.g., Mpttern).\n",
      "- Memb is apparently the previous state-of-the-art, but there is no mention to\n",
      "any reference.\n",
      "- While it looks like the method outperforms the previous best performing\n",
      "approach, the paper is not convincing enough. Especially, on the first dataset,\n",
      "the difference between the new system and the previous state-of-the-art one is\n",
      "pretty small.\n",
      "- The paper seriously lacks proofreading, and could not be published until this\n",
      "is fixed – for instance, I noted 11 errors in the first column of page 2.\n",
      "- The CilinE hierarchy is very shallow (5 levels only). However apparently, it\n",
      "has been used in the past by other authors. I would expect that the deeper the\n",
      "more difficult it is to branch new hyponym-hypernyms. This can explain the very\n",
      "high results obtained (even by previous studies)...\n",
      "\n",
      "- General Discussion:\n",
      "The approach itself is not really original or novel, but it is applied to a\n",
      "problem that has not been addressed with deep learning yet. For this reason, I\n",
      "think this paper is interesting, but there are two main flaws. The first and\n",
      "easiest to fix is the presentation. There are many errors/typos that need to be\n",
      "corrected. I started listing them to help, but there are just too many of them.\n",
      "The second issue is the evaluation, in my opinion. Technically, the\n",
      "performances are better, but it does not feel convincing as explained above.\n",
      "What is Memb, is it the method from Shwartz et al 2016, maybe? If not, what\n",
      "performance did this recent approach have? I think the authors need to\n",
      "reorganize the evaluation section, in order to properly list the baseline\n",
      "systems, clearly show the benefit of their approach and where the others fail.\n",
      "Significance tests  also seem necessary given the slight improvement on one\n",
      "dataset.\n",
      "- Strengths:\n",
      "\n",
      "  * Knowledge lean, language-independent approach\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "  * Peculiar task/setting\n",
      "  * Marginal improvement over W_Emb (Fu et al, 2014)\n",
      "  * Waste of space\n",
      "  * Language not always that clear\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "It seems to me that this paper is quite similar to (Fu et al, 2014) and only\n",
      "adds marginal improvements. It contains quite a lot of redundancy (e.g. related\n",
      "work in  sec 1 and sec 2), uninformative figures (e.g. Figure 1 vs Figure 2),\n",
      "not so useful descriptions of MLP and RNN, etc. A short paper might have been a\n",
      "better fit.\n",
      "\n",
      "The task looks somewhat idiosyncratic to me. It is only useful if you already\n",
      "have a method that gives you all and only the hypernyms of a given word. This\n",
      "seems to presuppose (Fu et al., 2013). \n",
      "\n",
      "Figure 4: why are the first two stars connected by conjunction and the last two\n",
      "starts by disjunction?              Why is the output \"1\" (dark star) if the the\n",
      "three\n",
      "inputs are \"0\" (white stars)?\n",
      "\n",
      "Sec 4.2, lines 587-589 appears to suggest that thresholds were tuned on the\n",
      "test data (?) \n",
      "\n",
      "W_Emb is poorly explained (lines 650-652).\n",
      "\n",
      "Some parts of the text are puzzling. I can't make sense of the section titled\n",
      "\"Combined with Manually-Built Hierarchies\". Same for sec 4.4. What do the red\n",
      "and dashed lines mean?\n",
      "- Strengths: Useful application for teachers and learners; supports\n",
      "fine-grained comparison of GEC systems.\n",
      "\n",
      "- Weaknesses: Highly superficial description of the system; evaluation not\n",
      "satisfying.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The paper presents an approach of automatically enriching the output of GEC\n",
      "systems with error types. This is a very useful application because both\n",
      "teachers and learners can benefit from this information (and many GEC systems\n",
      "only output a corrected version, without making the type of error explicit). It\n",
      "also allows for finer-grained comparison of GEC systems, in terms of precision\n",
      "in general, and error type-specific figures for recall and precision.\n",
      "\n",
      "Unfortunately, the description of the system remains highly superficial. The\n",
      "core of the system consists of a set of (manually?) created rules but the paper\n",
      "does not provide any details about these rules. The authors should, e.g., show\n",
      "some examples of such rules, specify the number of rules, tell us how complex\n",
      "they are, how they are ordered (could some early rule block the application of\n",
      "a later rule?), etc. -- Instead of presenting relevant details of the system,\n",
      "several pages of the paper are devoted to an evaluation of the systems that\n",
      "participated in CoNLL-2014. Table 6 (which takes one entire page) list results\n",
      "for all systems, and the text repeats many facts and figures that can be read\n",
      "off the table. \n",
      "\n",
      "The evaluation of the proposed system is not satisfying in several aspects. \n",
      "First, the annotators should have independently annotated a gold standard for\n",
      "the 200 test sentences instead of simply rating the output of the system. Given\n",
      "a fixed set of tags, it should be possible to produce a gold standard for the\n",
      "rather small set of test sentences. It is highly probable that the approach\n",
      "taken in the paper yields considerably better ratings for the annotations than\n",
      "comparison with a real gold standard (see, e.g., Marcus et al. (1993) for a\n",
      "comparison of agreement when reviewing pre-annotated data vs. annotating from\n",
      "scratch). \n",
      "Second, it is said that \"all 5 raters individually considered at least 95% of\n",
      "our rule-based error types to be either “Good” or “Acceptable”\".\n",
      "Multiple rates should not be considered individually and their ratings averaged\n",
      "this way, this is not common practice. If each of the \"bad\" scores were\n",
      "assigned to different edits (we don't learn about their distribution from the\n",
      "paper), 18.5% of the edits were considered \"bad\" by some annotator -- this\n",
      "sounds much worse than the average 3.7%, as calculated in the paper.\n",
      "Third, no information about the test data is provided, e.g. how many error\n",
      "categories they contain, or which error categories are covered (according to\n",
      "the cateogories rated as \"good\" by the annotators).\n",
      "Forth, what does it mean that \"edit boundaries might be unusual\"? A more\n",
      "precise description plus examples are at need here. Could this be problematic\n",
      "for the application of the system?\n",
      "\n",
      "The authors state that their system is less domain dependent as compared to\n",
      "systems that need training data. I'm not sure that this is true. E.g., I\n",
      "suppose that Hunspell's vocabulary probably doesn't cover all domains in the\n",
      "same detail, and manually-created rules can be domain-dependent as well -- and\n",
      "are completely language dependent, a clear drawback as compared to machine\n",
      "learning approaches. Moreover, the test data used here (FCE-test, CoNLL-2014)\n",
      "are from one domain only: student essays.\n",
      "\n",
      "It remains unclear why a new set of error categories was designed. One reason\n",
      "for the tags is given: to be able to search easily for underspecified\n",
      "categories (like \"NOUN\" in general). It seems to me that the tagset presented\n",
      "in Nicholls (2003) supports such searches as well. Or why not using the\n",
      "CoNLL-2014 tagset? Then the CoNLL gold standard could have been used for\n",
      "evaluation.\n",
      "\n",
      "To sum up, the main motivation of the paper remains somewhat unclear. Is it\n",
      "about a new system? But the most important details of it are left out. Is it\n",
      "about a new set of error categories? But hardly any motivation or discussion of\n",
      "it is provided. Is it about evaluating the CoNLL-2014 systems? But the\n",
      "presentation of the results remains superficial.\n",
      "\n",
      "Typos:\n",
      "- l129 (and others): c.f. -> cf.\n",
      "- l366 (and others): M2 -> M^2 (= superscribed 2)\n",
      "- l319: 50-70 F1: what does this mean? 50-70%?\n",
      "\n",
      "Check references for incorrect case\n",
      "- e.g. l908: esl -> ESL\n",
      "- e.g. l878/79: fleiss, kappa\n",
      "The paper presents a novel approach for evaluating grammatical error\n",
      "correction (GEC) systems. This approach makes it possible to assess\n",
      "the performance of GEC systems by error type not only in terms of\n",
      "recall but also in terms of precision, which was previously not\n",
      "possible in general since system output is usually not annotated with\n",
      "error categories.\n",
      "\n",
      "Strengths:\n",
      "\n",
      " - The proposed evaluation is an important stepping stone for\n",
      "   analyzing GEC system behavior.\n",
      " - The paper includes evaluation for a variety of systems.\n",
      " - The approach has several advantages over previous work:\n",
      "   - it computes precision by error type\n",
      "   - it is independent of manual error annotation\n",
      "   - it can assess the performance on multi token errors\n",
      " - The automatically selected error tags for pre-computed error spans\n",
      "   are mostly approved of by human experts\n",
      "\n",
      "Weaknesses:\n",
      "\n",
      " - A key part – the rules to derive error types – are not described.\n",
      " - The classifier evaluation lacks a thorough error analysis and based\n",
      "   upon that it lacks directions of future work on how to improve the\n",
      "   classifier.\n",
      " - The evaluation was only performed for English and it is unclear how\n",
      "   difficult it would be to use the approach on another language.\n",
      "\n",
      "Classifier and Classifier Evaluation\n",
      "====================================\n",
      "\n",
      "It is unclear on what basis the error categories were devised. Are\n",
      "they based on previous work?\n",
      "\n",
      "Although the approach in general is independent of the alignment\n",
      "algorithm, the rules are probably not, but the authors don't provide\n",
      "details on that.  The error categories are a major part of the paper\n",
      "and the reader should at least get a glimpse of how a rule to assign\n",
      "an error type looks like.\n",
      "\n",
      "Unfortunately, the paper does not apply the proposed evaluation on\n",
      "languages other than English.  It also does not elaborate on what\n",
      "changes would be necessary to run the classifier on other languages. I\n",
      "assume that the rules used for determining edit boundaries as well as\n",
      "for determining the error tags depend on the language/the\n",
      "pre-processing pipeline to a certain extent and therefore need to be\n",
      "adapted. Also, the error categories might need to be changed.  The\n",
      "authors do not provide any detail on the rules for assigning error\n",
      "categories (how many are there overall/per error type? how complex are\n",
      "they?) to estimate the effort necessary to use the approach on another\n",
      "language.\n",
      "\n",
      "The error spans computed in the pre-processing step seem to be\n",
      "inherently continuous (which is also the case with the M2 scorer), which\n",
      "is problematic since there are errors which can only be tagged\n",
      "accurately when the error span is discontinuous. In German, for\n",
      "example, verbs with separable prefixes are separated from each other\n",
      "in the main clause: [1st constituent] [verb] [other constituents]\n",
      "[verb prefix]. Would the classifier be able to tag discontinuous edit\n",
      "spans?\n",
      "\n",
      "The authors write that all human judges rated at least 95\\% of the\n",
      "automatically assigned error tags as appropriate \"despite the degree\n",
      "of noise introduced by automatic edit extraction\" (295). I would be\n",
      "more cautious with this judgment since the raters might also have been\n",
      "more forgiving when the boundaries were noisy. In addition, they were\n",
      "not asked to select a tag without knowing the system output but could\n",
      "in case of noisy boundaries be more biased towards the system\n",
      "output. Additionally, there was no rating option between \"Bad (Not\n",
      "Appropriate)\" and \"Appropriate\", which might also have led raters to\n",
      "select \"Appropriate\" over \"Bad\". To make the evaluation more sound,\n",
      "the authors should also evaluate how the human judges rate the\n",
      "classifier output if the boundaries were manually created,\n",
      "i.e. without the noise introduced by faulty boundaries.\n",
      "\n",
      "The classifier evaluation lacks a thorough error analysis. It is only\n",
      "mentioned that \"Bad\" is usually traced back to a wrong POS\n",
      "tag. Questions I'd like to see addressed: When did raters select\n",
      "\"Bad\", when \"Appropriate\"? Does the rating by experts point at\n",
      "possibilities to improve the classifier?\n",
      "\n",
      "Gold Reference vs. Auto Reference\n",
      "=================================\n",
      "\n",
      "It is unclear on what data the significance test was performed\n",
      "exactly. Did you test on the F0.5 scores? If so, I don't think this is\n",
      "a good idea since it is a derived measure with weak discriminative\n",
      "power (the performance in terms of recall an precision can be totally\n",
      "different but have the same F0.5 score). Also, at the beginning of\n",
      "Section 4.1 the authors refer to the mismatch between automatic and\n",
      "reference in terms of alignment and classification but as far as I can\n",
      "tell, the comparison between gold and reference is only in terms of\n",
      "boundaries and not in terms of classification.\n",
      "\n",
      "Error Type Evaluation\n",
      "=====================\n",
      "\n",
      "I do not think it is surprising that 5 teams (~line 473) failed to correct\n",
      "any unnecessary token error. For at least two of the systems there is\n",
      "a straightforward explanation why they cannot handle superfluous\n",
      "words. The most obvious is UFC: Their rule-base approach works on POS\n",
      "tags (Ng et al., 2014) and it is just not possible to determine\n",
      "superfluous words based on POS alone. Rozovskaya & Roth (2016) provide\n",
      "an explanation why AMU performs poorly on superfluous words.\n",
      "\n",
      "The authors do not analyze or comment the results in Table 6 with\n",
      "respect to whether the systems were designed to handle the error\n",
      "type. For some error types, there is a straight-forward mapping\n",
      "between error type in the gold standard and in the auto reference, for\n",
      "example for word order error. It remains unclear whether the systems\n",
      "failed completely on specific error types or were just not designed to\n",
      "correct them (CUUI for example is reported with precision+recall=0.0,\n",
      "although it does not target word order errors). In the CUUI case (and\n",
      "there are probably similar cases), this also points at an error in the\n",
      "classification which is neither analyzed nor discussed.\n",
      "\n",
      "Please report also raw values for TP, FP, TN, FN in the appendix for\n",
      "Table 6. This makes it easier to compare the systems using other\n",
      "measures. Also, it seems that for some error types and systems the\n",
      "results in Table 6 are based only on a few instances. This would also\n",
      "be made clear when reporting the raw values.\n",
      "\n",
      "Your write \"All but 2 teams (IITB and IPN) achieved the best score in\n",
      "at least 1 category, which suggests that different approaches to GEC\n",
      "complement different error types.\" (606) It would be nice to mention\n",
      "here that this is in line with previous research.\n",
      "\n",
      "Multi-token error analysis is helpful for future work but the result\n",
      "needs more interpretation: Some systems are probably inherently unable\n",
      "to correct such errors but none of the systems were trained on a\n",
      "parallel corpus of learner data and fluent (in the sense of Sakaguchi\n",
      "et al, 2016) corrections.\n",
      "\n",
      "Other\n",
      "=====\n",
      "\n",
      "- The authors should have mentioned that for some of the GEC\n",
      "  approaches, it was not impossible before to provide error\n",
      "  annotations, e.g. systems with submodules for one error type each.\n",
      "  Admittedly, the system would need to be adapted to include the\n",
      "  submodule responsible for a change in the system output. Still, the\n",
      "  proposed approach enables to compare GEC systems for which producing\n",
      "  an error tagged output is not straightforward to other systems in a\n",
      "  unified way.\n",
      "- References: Some titles lack capitalizations. URL for Sakaguchi et\n",
      "  al. (2016) needs to be wrapped. Page information is missing for\n",
      "  Efron and Tibshirani (1993).\n",
      "\n",
      "Author response\n",
      "===============\n",
      "\n",
      "I agree that your approach is not \"fatally flawed\" and I think this review\n",
      "actually points out quite some positive aspects. The approach is good, but the\n",
      "paper is not ready.\n",
      "\n",
      "The basis for the paper are the rules for classifying errors and the lack of\n",
      "description is a major factor.        This is not just a matter about additional\n",
      "examples. If the rules are not seen as a one-off implementation, they need to\n",
      "be described to be replicable or to adapt them.\n",
      "\n",
      "Generalization to other languages should not be an afterthought.  It would be\n",
      "serious limitation if the approach only worked on one language by design.  Even\n",
      "if you don't perform an adaption for other languages, your approach should be\n",
      "transparent enough for others to estimate how much work such an adaptation\n",
      "would be and how well it could reasonably work.  Just stating that most\n",
      "research is targeted at ESL only reinforces the problem.\n",
      "\n",
      "You write that the error types certain systems tackle would be \"usually obvious\n",
      "from the tables\".  I don't think it is as simple as that -- see the CUUI\n",
      "example mentioned above as well as the unnecessary token errors.  There are\n",
      "five systems that don't correct them (Table 5) and it should therefore be\n",
      "obvious that they did not try to tackle them. However, in the paper you write\n",
      "that \"There\n",
      "is also no obvious explanation as to why these teams had difficulty with this\n",
      "error type\".\n",
      "- Strengths:\n",
      "The paper proposes an end-to-end neural model for semantic graph parsing,\n",
      "based on a well-designed transition system. \n",
      "The work is interesting, learning\n",
      "semantic representations of DMRS, which is capable of resolving semantics\n",
      "such as scope underspecification. This work shows a new scheme for\n",
      "computational semantics, benefiting from an end-to-end transition-based\n",
      "incremental framework, which resolves the parsing with low cost.\n",
      "\n",
      "- Weaknesses:\n",
      "  My major concern is that the paper only gives a very common introduction for\n",
      "the\n",
      "definition of DMRS and EP, and the example even makes me a little confused\n",
      "because I cannot see anything special for DMRS. The description can be a little\n",
      "more detailed, I think. However, upon the space limitation, it is\n",
      "understandable. The same problem exists for the transition system of the\n",
      "parsing model. If I do not have any background of MRS and EP, I can hardly\n",
      "learn something from the paper, just seeing that this paper is very good.\n",
      "\n",
      "- General Discussion:\n",
      "  Overall, this paper is very interesting to me. I like the DMRS for semantic\n",
      "parsing very much and like the paper very much. Hope that the open-source codes\n",
      "and datasets can make this line of research being a hot topic.\n",
      "- Strengths:\n",
      "* Outperforms ALIGN in supervised entity linking task which suggests that the\n",
      "proposed framework improves representations of text and knowledge that are\n",
      "learned jointly.\n",
      "* Direct comparison with closely related approach using very similar input\n",
      "data.\n",
      "* Analysis of the smoothing parameter provides useful analysis since impact of\n",
      "popularity is a persistent issue in entity linking.\n",
      "\n",
      "- Weaknesses:\n",
      "* Comparison with ALIGN could be better. ALIGN used content window size 10 vs\n",
      "this paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear\n",
      "to me whether N(e_j) includes only entities that link to e_j. The graph is\n",
      "directed and consists of wikipedia outlinks, but is adjacency defined as it\n",
      "would be for an undirected graph? For ALIGN, the context of an entity is the\n",
      "set of entities that link to that entity. If N(e_j) is different, we cannot\n",
      "tell how much impact this change has on the learned vectors, and this could\n",
      "contribute to the difference in scores on the entity similarity task. \n",
      "* It is sometimes difficult to follow whether \"mention\" means a string type, or\n",
      "a particular mention in a particular document. The phrase \"mention embedding\"\n",
      "is used, but it appears that embeddings are only learned for mention senses.\n",
      "* It is difficult to determine the impact of sense disambiguation order without\n",
      "comparison to other unsupervised entity linking methods. \n",
      "\n",
      "- General Discussion:\n",
      "This paper addresses the problem of disambiguating/linking textual entity\n",
      "mentions into a given background knowledge base (in this case, English\n",
      "Wikipedia).  (Its title and introduction are a little overblown/misleading,\n",
      "since there is a lot more to bridging text and knowledge than the EDL task, but\n",
      "EDL is a core part of the overall task nonetheless.)  The method is to perform\n",
      "this bridging via an intermediate layer of representation, namely mention\n",
      "senses, thus following two steps: (1) mention to mention sense, and (2) mention\n",
      "sense to entity.  Various embedding representations are learned for the words,\n",
      "the mention senses, and the entities, which are then jointly trained to\n",
      "maximize a single overall objective function that maximizes all three types of\n",
      "embedding equally.  \n",
      "\n",
      "Technically the approach is fairly clear and conforms to the current deep\n",
      "processing fashion and known best practices regarding embeddings; while one can\n",
      "suggest all kinds of alternatives, it’s not clear they would make a material\n",
      "difference.  Rather, my comments focus on the basic approach.  It is not\n",
      "explained, however, exactly why a two-step process, involving the mention\n",
      "senses, is better than a simple direct one-step mapping from word mentions to\n",
      "their entities.  (This is the approach of Yamada et al., in what is called here\n",
      "the ALIGN algorithm.)  Table 2 shows that the two-step MPME (and even its\n",
      "simplification SPME) do better.  By why, exactly?  What is the exact\n",
      "difference, and additional information, that the mention senses have compare4ed\n",
      "to the entities?  To understand, please check if the following is correct (and\n",
      "perhaps update the paper to make it exactly clear what is going on).  \n",
      "\n",
      "For entities: their profiles consist of neighboring entities in a relatedness\n",
      "graph.                    This graph is built (I assume) by looking at word-level\n",
      "relatedness of\n",
      "the entity definitions (pages in Wikipedia).  The profiles are (extended\n",
      "skip-gram-based) embeddings.  \n",
      "\n",
      "For words: their profiles are the standard distributional semantics approach,\n",
      "without sense disambiguation.  \n",
      "\n",
      "For mention senses: their profiles are the standard distributional semantics\n",
      "approach, but WITH sense disambiguation.  Sense disambiguation is performed\n",
      "using a sense-based profile (‘language model’) from local context words and\n",
      "neighboring mentions, as mentioned briefly just before Section 4, but without\n",
      "details.  This is a problem point in the approach.  How exactly are the senses\n",
      "created and differentiated?  Who defines how many senses a mention string can\n",
      "have?  If this is done by looking at the knowledge base, then we get a\n",
      "bijective mapping between mention senses and entities -– that is, there is\n",
      "exactly one entity for each mention sense (even if there may be more entities).\n",
      " In that case, are the sense collection’s definitional profiles built\n",
      "starting with entity text as ‘seed words’?                    If so, what\n",
      "information\n",
      "is used\n",
      "at the mention sense level that is NOT used at the entity level?  Just and\n",
      "exactly the words in the texts that reliably associate with the mention sense,\n",
      "but that do NOT occur in the equivalent entity webpage in Wikipedia?  How many\n",
      "such words are there, on average, for a mention sense?                    That is,\n",
      "how\n",
      "powerful/necessary is it to keep this extra differentiation information in a\n",
      "separate space (the mention sense space) as opposed to just loading these\n",
      "additional words into the Entity space (by adding these words into the\n",
      "Wikipedia entity pages)?  \n",
      "\n",
      "If the above understanding is essentially correct, please update Section 5 of\n",
      "the paper to say so, for (to me) it is the main new information in the paper.  \n",
      "\n",
      "It is not true, as the paper says in Section 6, that “…this is the first\n",
      "work to deal with mention ambiguity in the integration of text and knowledge\n",
      "representations, so there is no exact baselines for comparison”.  The TAC KBP\n",
      "evaluations for the past two years have hosted EDL tasks, involving eight or\n",
      "nine systems, all performing exactly this task, albeit against Freebase, which\n",
      "is considerably larger and more noisy than Wikipedia.  Please see\n",
      "http://nlp.cs.rpi.edu/kbp/2016/ .  \n",
      "\n",
      "On a positive note: I really liked the idea of the smoothing parameter in\n",
      "Section 6.4.2.\n",
      "\n",
      "Post-response: I have read the authors' responses.  I am not really satisfied\n",
      "with their reply about the KBP evaluation not being relevant, but that they are\n",
      "interested in the goodness of the embeddings instead.  In fact, the only way to\n",
      "evaluate such 'goodness' is through an application.  No-one really cares how\n",
      "conceptually elegant an embedding is, the question is: does it perform better?\n",
      "- Strengths:\n",
      "Good ideas, simple neural learning, interesting performance (altough not\n",
      "striking) and finally large set of applications.\n",
      "\n",
      "- Weaknesses: amount of novel content. Clarity in some sections. \n",
      "\n",
      "The paper presents a neural learning method for entity disambiguation and\n",
      "linking. It introduces a good idea to integrate entity, mention and sense\n",
      "modeling within the smame neural language modeling technique. The simple\n",
      "training procedure connected with the modeling allows to support a large set of\n",
      "application.\n",
      "\n",
      "The paper is clear formally, but the discussion is not always at the same level\n",
      "of the technical ideas.\n",
      "\n",
      "The empirical evaluation is good although not striking improvements of the\n",
      "performance are reported. Although it seems an extension of (Yamada et al.,\n",
      "CoNLL 2016), it adds novel ideas and it is of a releant interest.\n",
      "\n",
      "The weaker points of the paper are:\n",
      "\n",
      "- The prose is not always clear. I found Section 3 not as clear. Some details\n",
      "of Figure 2 are not explained and the terminology is somehow redundant: for\n",
      "example, why do you refer to the dictionary of mentions? or the dictionary of\n",
      "entity-mention pairs? are these different from text anchors and types for\n",
      "annotated text anchors?\n",
      "- Tha paper is quite close in nature to Yamada et al., 2016) and the authors\n",
      "should at least outline the differences.\n",
      "\n",
      "One general observation on the current version is:\n",
      "The paper tests the Multiple Embedding model against entity\n",
      "linking/disambiguation tasks. However, word embeddings are not only used to\n",
      "model such tasks, but also some processes not directly depending on entities of\n",
      "the KB, e.g. parsing, coreference or semantic role labeling. \n",
      "The authors should show that the word embeddings provided by the proposed MPME\n",
      "method are not weaker wrt to simpler wordspaces in such other semantic tasks,\n",
      "i.e. those involving directly entity mentions.\n",
      "\n",
      "I did read the author's response.\n",
      "This paper proposes a novel strategy for zero-resource translation where\n",
      "(source, pivot) and (pivot, target) parallel corpora are available. A teacher\n",
      "model for p(target|pivot) is first trained on the (pivot, target) corpus, then\n",
      "a student model for p(target|source) is trained to minimize relative entropy\n",
      "with respect to the teacher on the (source, pivot) corpus. When using\n",
      "word-level relative entropy over samples from the teacher, this approach is\n",
      "shown to outperform previous variants on standard pivoting, as well as other\n",
      "zero-resource strategies.\n",
      "\n",
      "This is a good contribution: a novel idea, clearly explained, and with\n",
      "convincing empirical support. Unlike some previous work, it makes fairly\n",
      "minimal assumptions about the nature of the NMT systems involved, and hence\n",
      "should be widely applicable.\n",
      "\n",
      "I have only a few suggestions for further experiments. First, it would be\n",
      "interesting to see how robust this approach is to more dissimilar source and\n",
      "pivot languages, where intuitively the true p(target|source) and\n",
      "p(target|pivot) will be further apart. Second, given the success of introducing\n",
      "word-based diversity, it was surprising not to see a sentence n-best or\n",
      "sentence-sampling experiment. This would be more costly, but not much more so\n",
      "since you’re already doing beam search with the teacher. Finally, related to\n",
      "the previous, it might be interesting to explore transition from word-based\n",
      "diversity to sentence-based as the student converges and no longer needs the\n",
      "signal from low-probability words.\n",
      "\n",
      "Some further comments:\n",
      "\n",
      "line 241: Despite its simplicity -> Due to its simplicity\n",
      "\n",
      "277: target sentence y -> target word y\n",
      "\n",
      "442: I assume that K=1 and K=5 mean that you compare probabilities of the most\n",
      "probable and 5 most probable words in the current context. If so, how is the\n",
      "current context determined - greedily or with a beam?\n",
      "\n",
      "Section 4.2. The comparison with an essentially uniform distribution doesn’t\n",
      "seem very informative here: it would be extremely surprising if p(y|z) were not\n",
      "significantly closer to p(y|x) than to uniform. It would be more interesting to\n",
      "know to what extent p(y|z) still provides a useful signal as p(y|x) gets\n",
      "better. This would be easy to measure by comparing p(y|z) to models for p(y|x)\n",
      "trained on different amounts of data or for different numbers of iterations.\n",
      "Another useful thing to explore in this section would be the effect of the mode\n",
      "approximation compared to n-best for sentence-level scores.\n",
      "\n",
      "555: It’s odd that word beam does worse than word greedy, since word beam\n",
      "should be closer to word sampling. Do you have an explanation for this?\n",
      "\n",
      "582: The claimed advantage of sent-beam here looks like it may just be noise,\n",
      "given the high variance of these curves.\n",
      "In this paper the authors present a method for training a zero-resource NMT\n",
      "system by using training data from a pivot language. Unlike other approaches\n",
      "(mostly inspired in SMT), the author’s approach doesn’t do two-step\n",
      "decoding. Instead, they use a teacher/student framework, where the teacher\n",
      "network is trained using the pivot-target language pairs, and the student\n",
      "network is trained using the source-pivot data and the teacher network\n",
      "predictions of the target language.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "The results the authors present, show that their idea is promising. Also, the\n",
      "authors present several sets of results that validate their assumptions.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "However, there are many points that need to be address before this paper is\n",
      "ready for publication.\n",
      "\n",
      "1)            Crucial information is missing\n",
      "\n",
      "Can you flesh out more clearly how training and decoding happen in your\n",
      "training framework? I found out that the equations do not completely describe\n",
      "the approach. It might be useful to use a couple of examples to make your\n",
      "approach clearer.\n",
      "\n",
      "Also, how is the montecarlo sampling done? \n",
      "\n",
      "2)            Organization\n",
      "The paper is not very well organized. For example, results are broken into\n",
      "several subsections, while they’d better be presented together.  The\n",
      "organization of the tables is very confusing. Table 7 is referred before table\n",
      "6. This made it difficult to read the results.\n",
      "\n",
      "3)            Inconclusive results:\n",
      "After reading the results section, it’s difficult to draw conclusions when,\n",
      "as the authors point out in their comparisons, this can be explained by the\n",
      "total size of the corpus involved in their methods (621  ). \n",
      "\n",
      "4)            Not so useful information:\n",
      "While I appreciate the fleshing out of the assumptions, I find that dedicating\n",
      "a whole section of the paper plus experimental results is a lot of space. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "Other:\n",
      "578:  We observe that word-level models tend to have lower valid loss compared\n",
      "with sentence- level methods….\n",
      "Is it valid to compare the loss from two different loss functions?\n",
      "\n",
      "Sec 3.2, the notations are not clear. What does script(Y) means?\n",
      "How do we get p(y|x)? this is never explained\n",
      "\n",
      "Eq 7 deserves some explanation, or better removed.\n",
      "320: What approach did you use? You should talk about that here\n",
      "392 : Do you mean 2016?\n",
      "\n",
      "Nitty-gritty:\n",
      "\n",
      "742  : import => important\n",
      "772  : inline citation style\n",
      "778: can significantly outperform \n",
      "275: Assumption 2 needs to be rewritten … a target sentence y from x should\n",
      "be close to that from its counterpart z.\n",
      "- Strengths:\n",
      "This is  a well written paper.\n",
      "The paper is very clear for the most part.\n",
      "The experimental comparisons are very well done.\n",
      "The experiments are well designed and executed.\n",
      "The idea of using KD for zero-resource NMT is impressive.\n",
      "\n",
      "- Weaknesses:\n",
      "There were many sentences in the abstract and in other places in the paper\n",
      "where the authors stuff too much information into a single sentence. This could\n",
      "be avoided. One can always use an extra sentence to be more clear.\n",
      "There could have been a section where the actual method used could be explained\n",
      "in a more detailed. This explanation is glossed over in the paper. It's\n",
      "non-trivial to guess the idea from reading the sections alone.\n",
      "During test time, you need the source-pivot corpus as well. This is a major\n",
      "disadvantage of this approach. This is played down - in fact it's not mentioned\n",
      "at all. I could strongly encourage the authors to mention this and comment on\n",
      "it. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "This paper uses knowledge distillation to improve zero-resource translation.\n",
      "The techniques used in this paper are very similar to the one proposed in Yoon\n",
      "Kim et. al. The innovative part is that they use it for doing zero-resource\n",
      "translation. They compare against other prominent works in the field. Their\n",
      "approach also eliminates the need to do double decoding.\n",
      "\n",
      "Detailed comments:\n",
      "- Line 21-27 - the authors could have avoided this complicated structure for\n",
      "two simple sentences.\n",
      "Line 41 - Johnson et. al has SOTA on English-French and German-English.\n",
      "Line 77-79 there is no evidence provided as to why combination of multiple\n",
      "languages increases complexity. Please retract this statement or provide more\n",
      "evidence. Evidence in literature seems to suggest the opposite.\n",
      "\n",
      "Line 416-420 - The two lines here are repeated again. They were first mentioned\n",
      "in the previous paragraph.\n",
      "Line 577 - Figure 2 not 3!\n",
      "This paper proposes a joint model of salient phrase selection and discourse\n",
      "relation prediction in spoken meeting. Experiments using meeting corpora show\n",
      "that the proposed model has higher performance than the SVM-based classifier.\n",
      "\n",
      "- Strengths:\n",
      "The paper is written to be easy to read. Technical details are described fully,\n",
      "and high performance is also shown in experimental evaluation. It also shows\n",
      "useful comparisons with related research in the field of discourse structure\n",
      "analysis and key phrase identification. It is interesting to note that not only\n",
      "the performance evaluation of phrase selection from discourse, discourse\n",
      "relation labeling, and summary generation as their applications, but also\n",
      "application to the prediction of the consistency of  understanding by team\n",
      "members is also verified .\n",
      "\n",
      "- Weaknesses:\n",
      "Jointly Modeling salient phrase extraction and discourse relationship labeling\n",
      "between speaker turns has been proposed. If intuitive explanation about their\n",
      "interactivity and the usefulness of considering it is fully presented.\n",
      "\n",
      "- General Discussion:\n",
      "SVM-based classifier is set as a comparative method in the experiment. It would\n",
      "be useful to mention the validity of the setting.\n",
      "- Strengths:\n",
      "Zero-shot relation extraction is an interesting problem. The authors have\n",
      "created a large dataset for relation extraction as question answering which\n",
      "would likely be useful to the community.\n",
      "\n",
      "- Weaknesses:\n",
      "Comparison and credit to existing work is severely lacking. Contributions of\n",
      "the paper don't seen particularly novel.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The authors perform relation extraction as reading comprehension. In order to\n",
      "train reading comprehension models to perform relation extraction, they create\n",
      "a large dataset of 30m “querified” (converted to natural language)\n",
      "relations by asking mechanical turk annotators to write natural language\n",
      "queries for relations from a schema. They use the reading comprehension model\n",
      "of Seo et al. 2016, adding the ability to return “no relation,” as the\n",
      "original model must always return an answer. The main motivation/result of the\n",
      "paper appears to be that the authors can perform zero-shot relation extraction,\n",
      "extracting relations only seen at test time.\n",
      "\n",
      "This paper is well-written and the idea is interesting. However, there are\n",
      "insufficient experiments and comparison to previous work to convince me that\n",
      "the paper’s contributions are novel and impactful.\n",
      "\n",
      "First, the authors are missing a great deal of related work: Neelakantan at al.\n",
      "2015 (https://arxiv.org/abs/1504.06662) perform zero-shot relation extraction\n",
      "using RNNs over KB paths. Verga et al. 2017 (https://arxiv.org/abs/1606.05804)\n",
      "perform relation extraction on unseen entities. The authors cite Bordes et al.\n",
      "(https://arxiv.org/pdf/1506.02075.pdf), who collect a similar dataset and\n",
      "perform relation extraction using memory networks (which are commonly used for\n",
      "reading comprehension). However, they merely note that their data was annotated\n",
      "at the “relation” level rather than at the triple (relation, entity pair)\n",
      "level… but couldn’t Bordes et al. have done the same in their annotation?\n",
      "If there is some significant difference here, it is not made clear in the\n",
      "paper. There is also a NAACL 2016 paper\n",
      "(https://www.aclweb.org/anthology/N/N16/N16-2016.pdf) which performs relation\n",
      "extraction using a new model based on memory networks… and I’m sure there\n",
      "are more. Your work is so similar to much of this work that you should really\n",
      "cite and establish novelty wrt at least some of them as early as the\n",
      "introduction -- that's how early I was wondering how your work differed, and it\n",
      "was not made clear.\n",
      "\n",
      "Second, the authors neither 1) evaluate their model on another dataset or 2)\n",
      "evaluate any previously published models on their dataset. This makes their\n",
      "empirical results extremely weak. Given that there is a wealth of existing work\n",
      "that performs the same task and the lack of novelty of this work, the authors\n",
      "need to include experiments that demonstrate that their technique outperforms\n",
      "others on this task, or otherwise show that their dataset is superior to others\n",
      "(e.g. since it is much larger than previous, does it allow for better\n",
      "generalization?)\n",
      "The paper presents a method for relation extraction based on converting the\n",
      "task into a question answering task. The main hypothesis of the paper is that\n",
      "questions are a more generic vehicle for carrying content than particular\n",
      "examples of relations, and are easier to create. The results seem to show good\n",
      "performance, though a direct comparison on a standard relation extraction task\n",
      "is not performed.\n",
      "- Strengths:\n",
      "The technique seems to be adept at identifying relations (a bit under 90\n",
      "F-measure). It works well both on unseen questions (for seen relations) and\n",
      "relatively well on unseen relations. The authors describe a method for\n",
      "obtaining a large training dataset\n",
      "\n",
      "- Weaknesses:\n",
      "I wish performance was also shown on standard relation extraction datasets - it\n",
      "is impossible to determine what types of biases the data itself has here\n",
      "(relations are generated from Wikidata via WikiReading - extracted from\n",
      "Wikipedia, not regular newswire/newsgroups/etc). It seems to me that the NIST\n",
      "TAC-KBP slot filling dataset is good and appropriate to run a comparison.\n",
      "\n",
      "One comparison that the authors did not do here (but should) is to train a\n",
      "relation detection model on the generated data, and see how well it compares\n",
      "with the QA approach.\n",
      "\n",
      "- General Discussion:\n",
      "I found the paper to be well written and argued, and the idea is interesting,\n",
      "and it seems to work decently. I also found it interesting that the zero-shot\n",
      "NL method behaved indistinguishably from the single question baseline, and not\n",
      "very far from the multiple questions system.\n",
      "The paper models the relation extraction problem as reading comprehension and\n",
      "extends a previously proposed reading comprehension (RC) model to extract\n",
      "unseen relations. The approach has two main components:\n",
      "\n",
      "1. Queryfication: Converting a relation into natural question. Authors use\n",
      "crowdsourcing for this part.\n",
      "\n",
      "2. Applying RC model on the generated questions and sentences to get the answer\n",
      "spans. Authors extend a previously proposed approach to accommodate situations\n",
      "where there is no correct answer in the sentence.\n",
      "\n",
      "My comments:\n",
      "\n",
      "1. The paper reads very well and the approach is clearly explained.\n",
      "\n",
      "2. In my opinion, though the idea of using RC for relation extraction is\n",
      "interesting and novel, the approach is not novel. A part of the approach is\n",
      "crowdsourced and the other part is taken directly from a previous work, as I\n",
      "mention above.\n",
      "\n",
      "3. Relation extraction is a well studied problem and there are plenty of\n",
      "recently published works on the problem. However, authors do not compare their\n",
      "methods against any of the previous works. This raises suspicion on the\n",
      "effectiveness of the approach. As seen from Table 2, the performance numbers of\n",
      "the proposed method on the core task are not very convincing. However, this\n",
      "maybe because of the dataset used in the paper. Hence, a comparison with\n",
      "previous methods would actually help assess how the current method stands with\n",
      "the state-of-the-art.\n",
      "\n",
      "4. Slot-filling data preparation: You say \"we took the first sentence s in D to\n",
      "contain both e and a\". How can you get the answer sentence for (all) the\n",
      "relations of an entity from the first sentence of the entity's Wikipedia\n",
      "article? Please clarify this. See the following paper. They have a set of rules\n",
      "to locate (answer) sentences corresponding to an entity property in its\n",
      "Wikipedia page:\n",
      "\n",
      "Wu, Fei, and Daniel S. Weld. \"Open information extraction using Wikipedia.\"\n",
      "Proceedings of the 48th Annual Meeting of the Association for Computational\n",
      "Linguistics. Association for Computational Linguistics, 2010.\n",
      "\n",
      "Overall, I think the paper presents an interesting approach. However, unless\n",
      "the effectiveness of the approach is demonstrated by comparing it against\n",
      "recent works on relation extraction, the paper is not ready for publication.\n",
      "- Strengths:\n",
      "\n",
      "The authors propose a selective encoding model as extension to the\n",
      "sequence-to-sequence framework for abstractive sentence summarization. The\n",
      "paper is very well written and the methods are clearly described. The proposed\n",
      "methods are evaluated on standard benchmarks and comparison to other\n",
      "state-of-the-art tools are presented, including significance scores. \n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "There are some few details on the implementation and on the systems to which\n",
      "the authors compared their work that need to be better explained. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "* Major review:\n",
      "\n",
      "- I wonder if the summaries obtained using the proposed methods are indeed\n",
      "abstractive. I understand that the target vocabulary is build out of the words\n",
      "which appear in the summaries in the training data. But given the example shown\n",
      "in Figure 4, I have the impression that the summaries are rather extractive.\n",
      "The authors should choose a better example for Figure 4 and give some\n",
      "statistics on the number of words in the output sentences which were not\n",
      "present in the input sentences for all test sets.\n",
      "\n",
      "- page 2, lines 266-272: I understand the mathematical difference between the\n",
      "vector hi and s, but I still have the feeling that there is a great overlap\n",
      "between them. Both \"represent the meaning\". Are both indeed necessary? Did you\n",
      "trying using only one of them.\n",
      "\n",
      "- Which neural network library did the authors use for implementing the system?\n",
      "There is no details on the implementation.\n",
      "\n",
      "- page 5, section 44: Which training data was used for each of the systems that\n",
      "the authors compare to? Diy you train any of them yourselves?\n",
      "\n",
      "* Minor review:\n",
      "\n",
      "- page 1, line 44: Although the difference between abstractive and extractive\n",
      "summarization is described in section 2, this could be moved to the\n",
      "introduction section. At this point, some users might no be familiar with this\n",
      "concept.\n",
      "\n",
      "- page 1, lines 93-96: please provide a reference for this passage: \"This\n",
      "approach achieves huge success in tasks like neural machine translation, where\n",
      "alignment between all parts of the input and output are required.\"\n",
      "\n",
      "- page 2, section 1, last paragraph: The contribution of the work is clear but\n",
      "I think the authors should emphasize that such a selective encoding model has\n",
      "never been proposed before (is this true?). Further, the related work section\n",
      "should be moved to before the methods section.\n",
      "\n",
      "- Figure 1 vs. Table 1: the authors show two examples for abstractive\n",
      "summarization but I think that just one of them is enough. Further, one is\n",
      "called a figure while the other a table.\n",
      "\n",
      "- Section 3.2, lines 230-234 and 234-235: please provide references for the\n",
      "following two passages: \"In the sequence-to-sequence machine translation (MT)\n",
      "model, the encoder and decoder are responsible for encoding input sentence\n",
      "information and decoding the sentence representation to generate an output\n",
      "sentence\"; \"Some previous works apply this framework to summarization\n",
      "generation tasks.\"\n",
      "\n",
      "- Figure 2: What is \"MLP\"? It seems not to be described in the paper.\n",
      "\n",
      "- page 3, lines 289-290: the sigmoid function and the element-wise\n",
      "multiplication are not defined for the formulas in section 3.1.\n",
      "\n",
      "- page 4, first column: many elements of the formulas are not defined: b\n",
      "(equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation\n",
      "15).\n",
      "\n",
      "- page 4, line 326: the readout state rt is not depicted in Figure 2\n",
      "(workflow).\n",
      "\n",
      "- Table 2: what does \"#(ref)\" mean?\n",
      "\n",
      "- Section 4.3, model parameters and training. Explain how you achieved the\n",
      "values to the many parameters: word embedding size, GRU hidden states, alpha,\n",
      "beta 1 and 2, epsilon, beam size.\n",
      "\n",
      "- Page 5, line 450: remove \"the\" word in this line? \"SGD as our optimizing\n",
      "algorithms\" instead of \"SGD as our the optimizing algorithms.\"\n",
      "\n",
      "- Page 5, beam search: please include a reference for beam search.\n",
      "\n",
      "- Figure 4: Is there a typo in the true sentence? \"council of europe again\n",
      "slams french prison conditions\" (again or against?)\n",
      "\n",
      "- typo \"supper script\" -> \"superscript\" (4 times)\n",
      "- Strengths:\n",
      "\n",
      "The paper is very clear and well-written. It proposes a novel approach to\n",
      "abstractive sentence summarization; basically sentence compression that is not\n",
      "constrained to having the words in the output be present in the input. \n",
      "\n",
      "- Excellent comparison with many baseline systems. \n",
      "\n",
      "- Very thorough related work. \n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "The criticisms are very minor:\n",
      "\n",
      "- It would be best to report ROUGE F-Score for all three datasets. The reasons\n",
      "for reporting recall on one are understandable (the summaries are all the same\n",
      "length), but in that case you could simply report both recall and F-Score. \n",
      "\n",
      "- The Related Work should come earlier in the paper. \n",
      "\n",
      "- The paper could use some discussion of the context of the work, e.g. how the\n",
      "summaries / compressions are intended to be used, or why they are needed. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "- ROUGE is fine for this paper, but ultimately you would want human evaluations\n",
      "of these compressions, e.g. on readability and coherence metrics, or an\n",
      "extrinsic evaluation.\n",
      "The paper presents a new neural approach for summarization. They build on a\n",
      "standard encoder-decoder with attention framework but add a network that gates\n",
      "every encoded hidden state based on summary vectors from initial encoding\n",
      "stages. Overall, the method seems to outperform standard seq2seq methods by 1-2\n",
      "points on three different evaluation sets.\n",
      "\n",
      "Overall, the technical sections of the paper are reasonably clear. Equation 16\n",
      "needs more explanation, I could not understand the notation. The specific\n",
      "contribution,  the selective mechanism, seems novel and could potentially be\n",
      "used in other contexts. \n",
      "\n",
      "The evaluation is extensive and does demonstrate consistent improvement. One\n",
      "would imagine that adding an additional encoder layer instead of the selective\n",
      "layer is the most reasonable baseline (given the GRU baseline uses only one\n",
      "bi-GRU, this adds expressivity), and this seems to be implemented Luong-NMT. My\n",
      "one concern is LSTM/GRU mismatch. Is the benefit coming from just GRU switch? \n",
      "\n",
      "The quality of the writing, especially in the intro/abstract/related work is\n",
      "quite bad. This paper does not make a large departure from previous work, and\n",
      "therefore a related work nearby the introduction seems more appropriate. In\n",
      "related work, one common good approach is highlighting similarities and\n",
      "differences between your work and previous work, in words before they are\n",
      "presented in equations. Simply listing works without relating them to your work\n",
      "is not that useful. Placement of the related work near the intro will allow you\n",
      "to relieve the intro of significant background detail and instead focus on more\n",
      "high level.\n",
      "The paper proposes an approach to sequence labeling with multitask learning,\n",
      "where language modeling is uses as the auxiliary objective. Thus, a\n",
      "bidirectional neural network architecture learns to predict the output labels\n",
      "as well as to predict the previous or next word in the sentence. The joint\n",
      "objectives lead to improvements over the baselines in grammatical error\n",
      "detection, chunking, NER, and POS tagging.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "The contribution is quite well-written and easy to follow for the most part.\n",
      "The model is exposed in sufficient detail, and the experiments are thorough\n",
      "within the defined framework. The benefits of introducing an auxiliary\n",
      "objective are nicely exposed.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "The paper shows very limited awareness of the related work, which is extensive\n",
      "across the tasks that the experiments highlight. Tables 1-3 only show the three\n",
      "systems proposed by the contribution (Baseline, +dropout, and +LMcost), while\n",
      "some very limited comparisons are sketched textually.\n",
      "\n",
      "A contribution claiming novelty and advancements over the previous state of the\n",
      "art should document these improvements properly: at least by reporting the\n",
      "relevant scores together with the novel ones, and ideally through replication.\n",
      "The datasets used in the experiments are all freely available, the previous\n",
      "results well-documented, and the previous systems are for the most part\n",
      "publicly available.\n",
      "\n",
      "In my view, for a long paper, it is a big flaw not to treat the previous work\n",
      "more carefully.\n",
      "\n",
      "In that sense, I find this sentence particularly troublesome: \"The baseline\n",
      "results are comparable to the previous best results on each of these\n",
      "benchmarks.\" The reader is here led to believe that the baseline system somehow\n",
      "subsumes all the previous contributions, which is shady on first read, and\n",
      "factually incorrect after a quick lookup in related work.\n",
      "\n",
      "The paper states \"new state-of-the-art results for error detection on both FCE\n",
      "and CoNLL-14 datasets\". Looking into the CoNLL 2014 shared task report, it is\n",
      "not straightforward to discern whether the\n",
      "latter part of the claim does holds true, also as per Rei and Yannakoudakis'\n",
      "(2016) paper. The paper should support the claim by inclusion/replication of\n",
      "the related work.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The POS tagging is left as more of an afterthought. The comparison to Plank et\n",
      "al. (2016) is at least partly unfair as they test across multiple languages in\n",
      "the Universal Dependencies realm, showing top-level performance across language\n",
      "families, which I for one believe to be far more relevant than WSJ\n",
      "benchmarking. How does the proposed system scale up/down to multiple languages,\n",
      "low-resource languages with limited training data, etc.? The paper leaves a lot\n",
      "to ask for in that dimension to further substantiate its claims.\n",
      "\n",
      "I like the idea of including language modeling as an auxiliary task. I like the\n",
      "architecture, and sections 1-4 in general. In my view, there is a big gap\n",
      "between those sections and the ones describing the experiments (5-8).\n",
      "\n",
      "I suggest that this nice idea should be further fleshed out before publication.\n",
      "The rework should include at least a more fair treatment of related work, if\n",
      "not replication, and at least a reflection on multilinguality. The data and the\n",
      "systems are all there, as signs of the field's growing maturity. The paper\n",
      "should in my view partake in reflecting this maturity, and not step away from\n",
      "it. In faith that these improvements can be implemented before the publication\n",
      "deadline, I vote borderline.\n",
      "- Strengths: The article is well written; what was done is clear and\n",
      "straightforward. Given how simple the contribution is, the gains are\n",
      "substantial, at least in the error correction task.\n",
      "\n",
      "- Weaknesses: The novelty is fairly limited (essentially, another permutation\n",
      "of tasks in multitask learning), and only one way of combining the tasks is\n",
      "explored. E.g., it would have been interesting to see if pre-training is\n",
      "significantly worse than joint training; one could initialize the weights from\n",
      "an existing RNN LM trained on unlabeled data; etc.\n",
      "\n",
      "- General Discussion: I was hesitating between a 3 and a 4. While the\n",
      "experiments are quite reasonable and the combinations of tasks sometimes new,\n",
      "there's quite a bit of work on multitask learning in RNNs (much of it already\n",
      "cited), so it's hard to get excited about this work. I nevertheless recommend\n",
      "acceptance because the experimental results may be useful to others.\n",
      "\n",
      "- Post-rebuttal: I've read the rebuttal and it didn't change my opinion of the\n",
      "paper.\n",
      "# Summary\n",
      "\n",
      "This paper presents an empirical study to identify a latent dimension of\n",
      "sentiment in word embeddings.\n",
      "\n",
      "# Strengths\n",
      "\n",
      " S1) Tackles a challenging problem of unsupervised sentiment analysis.\n",
      "\n",
      " S2) Figure 2, in particular, is a nice visualisation.\n",
      "\n",
      "# Weaknesses\n",
      "\n",
      " W1) The experiments, in particular, are very thin. I would recommend also\n",
      "measuring F1 performance and expanding the number of techniques compared.\n",
      "\n",
      " W2) The methodology description needs more organisation and elaboration. The\n",
      "ideas tested are itemised, but insufficiently justified. \n",
      "\n",
      " W3) The results are quite weak in terms of the reported accuracy and depth of\n",
      "analysis. Perhaps this work needs more development, particularly with\n",
      "validating the central assumption that the Distributional Hypothesis implies\n",
      "that opposite words, although semantically similar, are separated well in the\n",
      "vector space?\n",
      "- Strengths\n",
      "This paper deals with the issue of finding word polarity orientation in an\n",
      "unsupervised manner, using word embeddings.\n",
      "\n",
      "- Weaknesses\n",
      "The paper presents an interesting and useful idea, however, at this moment, it\n",
      "is not applied to any test case. The ideas on which it is based are explained\n",
      "in an \"intuitive\" manner and not thoroughly justified. \n",
      "\n",
      "- General Discussion\n",
      "This is definitely interesting work. The paper would benefit from more\n",
      "experiments being carried out, comparison with other methods (for example, the\n",
      "use of the Normalized Google Distance by authors such as (Balahur and Montoyo,\n",
      "2008) - http://ieeexplore.ieee.org/abstract/document/4906796/) and the\n",
      "application of the knowledge obtained to a real sentiment analysis scenario. At\n",
      "this point, the work, although promising, is in its initial phase.\n",
      "The paper introduces a general method for improving NLP tasks using embeddings\n",
      "from language models. Context independent word representations have been very\n",
      "useful, and this paper proposes a nice extension by using context-dependent\n",
      "word representations obtained from the hidden states of neural language models.\n",
      "They show significant improvements in tagging and chunking tasks from including\n",
      "embeddings from large language models. There is also interesting analysis which\n",
      "answers several natural questions.\n",
      "\n",
      "Overall this is a very good paper, but I have several suggestions:\n",
      "- Too many experiments are carried out on the test set. Please change Tables 5\n",
      "and 6 to use development data\n",
      "- It would be really nice to see results on some more tasks - NER tagging and\n",
      "chunking don't have many interesting long range dependencies, and the language\n",
      "model might really help in those cases. I'd love to see results on SRL or CCG\n",
      "supertagging.\n",
      "- The paper claims that using a task specific RNN is necessary because a CRF on\n",
      "top of language model embeddings performs poorly. It wasn't clear to me if they\n",
      "were backpropagating into the language model in this experiment - but if not,\n",
      "it certainly seems like there is potential for that to make a task specific RNN\n",
      "unnecessary.\n",
      "The paper proposes an approach where pre-trained word embeddings and\n",
      "pre-trained neural language model embeddings are leveraged (i.e., concatenated)\n",
      "to improve the performance in English chunking and NER on the respective CoNLL\n",
      "benchmarks, and on an out-of-domain English NER test set. The method records\n",
      "state-of-the-art scores for the two tasks.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "For the most part, the paper is well-written and easy to follow. The method is\n",
      "extensively documented. The discussion is broad and thorough.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "Sequence tagging does not equal chunking and NER. I am surprised not to see POS\n",
      "tagging included in the experiment, while more sequence tagging tasks would be\n",
      "welcome: grammatical error detection, supersense tagging, CCG supertagging,\n",
      "etc. This way, the paper is on chunking and NER for English, not for sequence\n",
      "tagging in general, as it lacks both the multilingual component and the breadth\n",
      "of tasks.\n",
      "\n",
      "While I welcomed the extensive description of the method, I do think that\n",
      "figures 1 and 2 overlap and that only one would have sufficed.\n",
      "\n",
      "Related to that, the method itself is rather straightforward and simple. While\n",
      "this is by all means not a bad thing, it seems that this contribution could\n",
      "have been better suited for a short paper. Since I do enjoy the more extensive\n",
      "discussion section, I do not necessarily see it as a flaw, but the core of the\n",
      "method itself does not strike me as particularly exciting. It's more of a\n",
      "\"focused contribution\" (short paper description from the call) than\n",
      "\"substantial\" work (long paper).\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "Bottomline, the paper concatenates two embeddings, and sees improvements in\n",
      "English chunking and NER.\n",
      "\n",
      "As such, does it warrant publication as an ACL long paper? I am ambivalent, so\n",
      "I will let my score reflect that, even if I slightly lean towards a negative\n",
      "answer. Why? Mainly because I would have preferred to see more breadth: a) more\n",
      "sequence tagging tasks and b) more languages.\n",
      "\n",
      "Also, we do not know how well this method scales to low(er)-resource scenarios.\n",
      "What if the pre-trained embeddings are not available? What if they were not as\n",
      "sizeable as they are? The experiments do include a notion of that, but still\n",
      "far above the low-resource range. Could they not have been learned in a\n",
      "multi-task learning setup in your model? That would have been more substantial\n",
      "in my view.\n",
      "\n",
      "For these reasons, I vote borderline, but with a low originality score. The\n",
      "idea of introducing context via the embeddings is nice in itself, but this\n",
      "particular instantiation of it leaves a lot to ask for.\n",
      "This paper describes interesting and ambitious work: the automated conversion\n",
      "of Universal Dependency grammar structures into [what the paper calls] semantic\n",
      "logical form representations.  In essence, each UD construct is assigned a\n",
      "target construction in logical form, and a procedure is defined to effect the\n",
      "conversion, working ‘inside-out’ using an intermediate form to ensure\n",
      "proper nesting of substructures into encapsulating ones.  Two evaluations are\n",
      "carried out: comparing the results to gold-standard lambda structures and\n",
      "measuring the effectiveness of the resulting lambda expressions in actually\n",
      "delivering the answers to questions from two QA sets.  \n",
      "\n",
      "It is impossible to describe all this adequately in the space provided.  The\n",
      "authors have taken some care to cover all principal parts, but there are still\n",
      "many missing details.  I would love to see a longer version of the paper! \n",
      "Particularly the QA results are short-changed; it would have been nice to learn\n",
      "which types of question are not handled, and which are not answered correctly,\n",
      "and why not.  This information would have been useful to gaining better insight\n",
      "into the limitations of the logical form representations.  \n",
      "\n",
      "That leads to my main concern/objection.  This logical form representation is\n",
      "not in fact a ‘real’ semantic one.                          It is, essentially, a\n",
      "rather\n",
      "close\n",
      "rewrite of the dependency structure of the input, with some (good) steps toward\n",
      "‘semanticization’, including the insertion of lambda operators, the\n",
      "explicit inclusion of dropped arguments (via the enhancement operation), and\n",
      "the introduction of appropriate types/units for such constructions as eventive\n",
      "adjectives and nouns like “running horse” and “president in 2009”.  But\n",
      "many (even simple) aspects of semantic are either not present (at least, not in\n",
      "the paper) and/or simply wrong.  Missing: quantification (as in “every” or\n",
      "“all”); numbers (as in “20” or “just over 1000”); various forms of\n",
      "reference (as in “he”, “that man”, “what I said before”); negation\n",
      "and modals, which change the semantics in interesting ways; inter-event\n",
      "relationships (as in the subevent relationship between the events in “the\n",
      "vacation was nice, but traveling was a pain”; etc. etc.  To add them one can\n",
      "easily cheat, by treating these items as if they were just unusual words and\n",
      "defining obvious and simple lambda formulas for them.  But they in fact require\n",
      "specific treatment; for example, a number requires the creation of a separate\n",
      "set object in the representation, with its own canonical variable (allowing\n",
      "later text to refer to “one of them” and bind the variable properly).  For\n",
      "another example, Person A’s model of an event may differ from Person B’s,\n",
      "so one needs two representation symbols for the event, plus a coupling and\n",
      "mapping between them.  For another example, one has to be able to handle time,\n",
      "even if simply by temporally indexing events and states.  None of this is here,\n",
      "and it is not immediately obvious how this would be added.  In some cases, as\n",
      "DRT shows, quantifier and referential scoping is not trivial.  \n",
      "\n",
      "It is easy to point to missing things, and unfair to the paper in some sense;\n",
      "you can’t be expected to do it all.  But you cannot be allowed to make\n",
      "obvious errors.  Very disturbing is the assignment of event relations strictly\n",
      "in parallel with the verb’s (or noun’s) syntactic roles.  No-one can claim\n",
      "seriously that “he broke the window” and “the window broke” has\n",
      "“he” and “the window” filling the same semantic role for “break”. \n",
      "That’s simply not correct, and one cannot dismiss the problem, as the paper\n",
      "does, to some nebulous subsequent semantic processing.                          This\n",
      "really\n",
      "needs\n",
      "adequate treatment, even in this paper.  This is to my mind the principal\n",
      "shortcoming of this work; for me this is the make-or-break point as to whether\n",
      "I would fight to have the paper accepted in the conference.  (I would have been\n",
      "far happier if the authors had simply acknowledged that this aspect is wrong\n",
      "and will be worked on in future, with a sketch saying how: perhaps by reference\n",
      "to FrameNet and semantic filler requirements.)                          \n",
      "\n",
      "Independent of the representation, the notation conversion procedure is\n",
      "reasonably clear.  I like the facts that it is rather cleaner and simpler than\n",
      "its predecessor (based on Stanford dependencies), and also that the authors\n",
      "have the courage of submitting non-neural work to the ACL in these days of\n",
      "unbridled and giddy enthusiasm for anything neural.\n",
      "- Strengths:\n",
      "\n",
      "When introducing the task, the authors use illustrative examples as well as the\n",
      "contributions of this paper. \n",
      "Related Works section covers the state of the art, at the same time pointing\n",
      "similarities and differences between related Works and the proposed method.\n",
      "The presentation of the method is very clear, since the authors separate the\n",
      "tagging scheme and the end-to-end model.\n",
      "Another strong point of this work is the baselines used to compare the proposed\n",
      "methods with several classical triplet extraction methods.\n",
      "At last, the presentation of examples from dataset used to illustrate the\n",
      "advantages and disadvantages of the methods was very important. These outputs\n",
      "complement the explanation of tagging and evaluation of triplets. \n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "One of the main contributions of this paper is a new tagging scheme described\n",
      "in Section 3.1, however there are already other schemes for NER and RE being\n",
      "used, such as IO, BIO and BILOU. \n",
      "Did the authors perform any experiment using other tagging scheme for this\n",
      "method?\n",
      "Regarding the dataset, in line 14, page 5, the authors cite the number of\n",
      "relations (24), but they do not mention the number or the type of named\n",
      "entities.\n",
      "In Section 4.1, the evaluation criteria of triplets are presented. These\n",
      "criteria were based on previous work? As I see it, the stage of entity\n",
      "identification is not complete if you consider only the head of the entity.\n",
      "Regarding example S3, shown in Table 3, the output of the LSTM-LSTM-Bias was\n",
      "considered correct? The text states that the relation role is wrong, although\n",
      "it is not clear if the relation role is considered in the evaluation. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "This paper proposes a novel tagging scheme and investigates the end-to-end\n",
      "models to jointly extract entities and relations. \n",
      "The article is organized in a clear way and it is well written, which makes it\n",
      "easy to understand the proposed method.\n",
      "This paper describes a rule based approach to time expression extraction. Its\n",
      "key insights are time expressions typically are short and contain at least 1\n",
      "time token. It first recognizes the time token through a combination of\n",
      "dictionary lookup, regular expression match with POS tagging information. It\n",
      "then expands the time segment from either direction of the time token until it\n",
      "reaches based on a set of heuristic rules. Finally, it merges the time segments\n",
      "into a single time expression based on another set of rules. Evaluation of this\n",
      "approach with both rule based & ML based systems on 3 data sets show\n",
      "significant improvements.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "It's well written and clearly presented. The rules are motivated by empirical\n",
      "observations of the data, and seems to be well justified as evidenced by the\n",
      "evaluation. \n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      " There are some underspecification in the paper that makes it difficult to\n",
      "reproduce the results. See below for details.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "* Section 4.1: what are there 5 seasons? What about things such as Ramadan\n",
      "month or Holiday Season?\n",
      "* Section 5.1: \"two benchmark datasets\" => \"three datasets\"?\n",
      "* Section 5.2: an example without time token will be helpful.\n",
      "* Section 5.2: given this approach is close to the ceiling of performance since\n",
      "93% expressions contain time token, and the system has achieved 92% recall, how\n",
      "do you plan to improve further?\n",
      "* Is there any plan to release the full set of rules/software used?\n",
      "The paper proposes a method to recognize time expressions from text. It is a\n",
      "simple rule-based method, which is a strong advantage as an analysis tool since\n",
      "time expression recognition should be a basic process in applications.\n",
      "Experiments results show that the proposed method outperforms the\n",
      "state-of-the-art rule-based methods and machine learning based method for time\n",
      "expression recognition. \n",
      "\n",
      "It is great, but my concern is generality of the method. The rules in the\n",
      "method were designed based on observations of corpora that are used for\n",
      "evaluation as well. Hence I’m afraid that the rules over-fit to these\n",
      "corpora. Similarly, domains of these corpora may have affected the rule design.\n",
      "There is no statistic nor discussion to show overlaps in time expressions in\n",
      "the observed corpora. If it was shown that time expressions in these corpora\n",
      "are mostly overlap, the fact should have supported generality of the rules. \n",
      "\n",
      "Anyway, it was better that the experiments have been conducted using a new\n",
      "corpus that was distinct from rule design process in order to show that the\n",
      "proposed method is widely effective.\n",
      "This paper presents several weakly supervised methods for developing NERs. The\n",
      "methods rely on some form of projection from English into another language. The\n",
      "overall approach is not new and the individual methods proposed are\n",
      "improvements of existing methods. For an ACL paper I would have expected more\n",
      "novel approaches.\n",
      "\n",
      "One of the contributions of the paper is the data selection scheme. The formula\n",
      "used to calculate the quality score is quite straightforward and this is not a\n",
      "bad thing. However, it is unclear how the thresholds were calculated for Table\n",
      "2. The paper says only that different thresholds were tried. Was this done on a\n",
      "development set? There is no mention of this in the paper. The evaluation\n",
      "results show clearly that data selection is very important, but one may not\n",
      "know how to tune the parameters for a new data set or a new language pair. \n",
      "\n",
      "Another contribution of the paper is the combination of the outputs of the two\n",
      "systems developed in the paper. I tried hard to understand how it works, but\n",
      "the description provided is not clear. \n",
      "\n",
      "The paper presents a number of variants for each of the methods proposed. Does\n",
      "it make sense to combine more than two weakly supervised systems? Did the\n",
      "authors try anything in this direction.\n",
      "\n",
      "It would be good to know a bit more about the types of texts that are in the\n",
      "\"in-house\" dataset.\n",
      "This paper describes a model for cross-lingual named entity recognition (NER).\n",
      "The authors employ conditional random fields, maximum entropy Markov, and\n",
      "neural network-based NER methods. In addition, authors propose two methods to\n",
      "combine the output of those methods (probability-based and ranking-based), and\n",
      "a method to select the best training instances from cross-lingual comparable\n",
      "corpora. The cross-lingual projection is done using a variant of Mikolov’s\n",
      "proposal. In general, the paper is easy to follow, well-structured, and the\n",
      "English quality is also correct. The results of the combined annotations are\n",
      "interesting.\n",
      "\n",
      "Detailed comments:\n",
      "\n",
      "I was wondering which is the motivation behind proposing a Continuous\n",
      "Bag-of-word (CBOW) model variation. You don’t give much details about this\n",
      "(or the parameters employed). Was the original model (or the Continuous\n",
      "Skip-gram model) offering low results? I suggest to include also the results\n",
      "with the CBOW model, so readers can analyse the improvements of your approach.\n",
      "Since you use a decay factor for the surrounding embeddings, I suggest to take\n",
      "a look to the exponential decay used in [1].\n",
      "\n",
      "Similarly to the previous comment, I would like to look at the differences\n",
      "between the original Mikolov’s cross-lingual projections and your frequency\n",
      "weighted projections. These contributions are more valuable if readers can see\n",
      "that your method is really superior.\n",
      "\n",
      "“the proposed data selection scheme is very effective in selecting\n",
      "good-quality projection-labeled data and the improvement is significant” ←\n",
      "Have you conducted a test of statistical significance? I would like to know if\n",
      "the differences between result in this work are significant. \n",
      "\n",
      "I suggest to integrate the text of Section 4.4 at the beginning of Section 4.2.\n",
      "It would look cleaner. I also recommend to move the evaluation of Table 2 to\n",
      "the evaluation section.\n",
      "\n",
      "I miss a related work section. Your introduction includes part of that\n",
      "information. I suggest to divide the introduction in two sections.\n",
      "\n",
      "The evaluation is quite short (1.5 pages with conclusion section there). You\n",
      "obtain state-of-the-art results, and I would appreciate more discussion and\n",
      "analysis of the results.\n",
      "\n",
      "Suggested references:\n",
      "\n",
      "[1] Iacobacci, I., Pilehvar, M. T., & Navigli, R. (2016). Embeddings for word\n",
      "sense disambiguation: An evaluation study. In Proceedings of the 54th Annual\n",
      "Meeting of the Association for Computational Linguistics (Vol. 1, pp. 897-907).\n",
      "- Strengths:\n",
      " - the model if theoretically solid and motivated by formal semantics. \n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      " - The paper is about is-a relation extraction but the majority of literature\n",
      "about taxonomization is not referenced in the paper, inter alia:\n",
      "\n",
      "Flati Tiziano, Vannella Daniele, Pasini Tommaso, Navigli Roberto.\n",
      "2016. MultiWiBi: The multilingual Wikipedia bitaxonomy project.\n",
      "\n",
      "Soren Auer, Christian Bizer, Georgi Kobilarov, Jens ¨\n",
      "Lehmann, Richard Cyganiak, and Zachary Ive.\n",
      "2007. DBpedia: A nucleus for a web of open data.\n",
      "\n",
      "Gerard de Melo and Gerhard Weikum. 2010. MENTA:\n",
      "Inducing Multilingual Taxonomies from Wikipedia.\n",
      "\n",
      "Zornitsa Kozareva and Eduard H. Hovy. 2010. A\n",
      "Semi-Supervised Method to Learn and Construct\n",
      "Taxonomies Using the Web. \n",
      "\n",
      "Vivi Nastase, Michael Strube, Benjamin Boerschinger,\n",
      "Caecilia Zirn, and Anas Elghafari. 2010. WikiNet:\n",
      "A Very Large Scale Multi-Lingual Concept Network.\n",
      "\n",
      "Simone Paolo Ponzetto and Michael Strube. 2007.\n",
      "Deriving a large scale taxonomy from Wikipedia.\n",
      "\n",
      "Simone Paolo Ponzetto and Michael Strube. 2011.\n",
      "Taxonomy induction based on a collaboratively built\n",
      "knowledge repository. \n",
      "\n",
      "Fabian M. Suchanek, Gjergji Kasneci, and Gerhard\n",
      "Weikum. 2008. YAGO: A large ontology from\n",
      "Wikipedia and WordNet. \n",
      "\n",
      "Paola Velardi, Stefano Faralli, and Roberto Navigli.\n",
      "2013. OntoLearn Reloaded: A graph-based algorithm\n",
      "for taxonomy induction. \n",
      "\n",
      " - Experiments are poor, they only compare against \"Hearst patterns\" without\n",
      "taking into account the works previously cited.\n",
      "\n",
      "- General Discussion:\n",
      " The paper is easy to follow and the supplementary material is also well\n",
      "written and useful, however the paper lack of references of is a relation\n",
      "extraction and taxonomization literature. The same apply for the experiments.\n",
      "In fact no meaningful comparison is performed and the authors not even take\n",
      "into account the existence of other systems (more recent than hearst patterns).\n",
      "\n",
      "I read authors answers but still i'm not convinced that they couldn't perform\n",
      "more evaluations. I understand that they have a solid theoretical motivation\n",
      "but still, i think that comparison are very important to asses if the\n",
      "theoretical intuitions of the authors are confirmed also in practice. While\n",
      "it's true that all the works i suggested as comparison build taxonomies, is\n",
      "also true that a comparison is possible considering the edges of a taxonomy.\n",
      "\n",
      "Anyway, considering the detailed author answer and the discussion with the\n",
      "other reviewer i can rise my score to 3 even if i still think that this paper\n",
      "is poor of experiments and does not frame correctly in the is-a relation\n",
      "extraction / taxonomy building literature.\n",
      "- Strengths:\n",
      "\n",
      "This paper presents an approach for fine-grained IsA extraction by learning\n",
      "modifier interpretations. The motivation of the paper is easy to understand and\n",
      "this is an interesting task. In addition, the approach seems solid in general\n",
      "and the experimental results show that the approach increases in the number of\n",
      "fine-grained classes that can be populated.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "Some parts of the paper are hard to follow. It is unclear to me why D((e, p,\n",
      "o)) is multiplied by w in Eq (7) and why the weight for e in Eq. (8) is\n",
      "explained as the product of how often e has been observed with some property\n",
      "and the weight of that property for the class MH. In addition, it also seems\n",
      "unclear how effective introducing compositional models itself is in increasing\n",
      "the coverage. I think one of the major factors of the increase of the coverage\n",
      "is the modifier expansion, which seems to also be applicable to the baseline\n",
      "'Hearst'. It would be interesting to see the scores 'Hearst' with modifier\n",
      "expansion.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "Overall, the task is interesting and the approach is generally solid. However,\n",
      "since this paper has weaknesses described above, I'm ambivalent about this\n",
      "paper.\n",
      "\n",
      "- Minor comment:\n",
      "\n",
      "I'm confused with some notations. For example, it is unclear for me what 'H'\n",
      "stands for. It seems that 'H' sometimes represents a class such as in (e, H)\n",
      "(- O, but sometimes represents a noun phrase such as in (H, p, N, w) (- D. Is\n",
      "my\n",
      "understanding correct?\n",
      "\n",
      "In Paragraph \"Precision-Recall Analysis\", why the authors use area under the\n",
      "ROC curve instead of area under the Precision-Recall curve, despite the\n",
      "paragraph title \"Precision-Recall Analysis\"?\n",
      "\n",
      "- After reading the response:\n",
      "\n",
      "Thank you for the response. I'm not fully satisfied with the response as to the\n",
      "modifier expansion. I do not think the modifier expansion can be applied to\n",
      "Hearst as to the proposed method. However, I'm wondering whether there is no\n",
      "way to take into account the similar modifiers to improve the coverage of\n",
      "Hearst. I'm actually between 3 and 4, but since it seems still unclear how\n",
      "effective introducing compositional models itself is, I keep my recommendation\n",
      "as it is.\n",
      "- strengths\n",
      "This is a novel approach to modeling the compositional structure of complex\n",
      "categories that maintains a set theoretic interpretation of common nouns and\n",
      "modifiers, while also permitting a distributional interpretation of head\n",
      "modification. The approach is well motivated and clearly defined and the\n",
      "experiments show that show that this decomposed representation can improve upon\n",
      "the Hearst-pattern derived IsA relations upon which it is trained in terms of\n",
      "coverage.\n",
      "\n",
      "- weaknesses\n",
      "The experiments are encouraging. However, it would be nice to see ROC curves\n",
      "for the new approach alone, not in an ensemble with Hearst patterns. Table 5\n",
      "tells us that Mods_I increases coverage at the cost of precision and Figure 2\n",
      "tells us that Mods_I matches Hearst pattern precision for the high precision\n",
      "region of the data. However, neither of these tell us whether the model can\n",
      "distinguish between the high and low precision regions, and the ROC curves\n",
      "(which would tell us this) are only available for ensembled models.\n",
      "\n",
      "I believe that Eqn. 7 has an unnecessary $w$ since it is already the case that\n",
      "$w=D(\\rangle e, p, o \\langle)$.\n",
      "\n",
      "- discussion\n",
      "Overall, this is a nice idea that is well described and evaluated. I think this\n",
      "paper would be a good addition to ACL.\n",
      "- Overview:\n",
      "\n",
      "The paper proposes a new model for training sense embeddings grounded in a\n",
      "lexical-semantic resource (in this case WordNet). There is no direct evaluation\n",
      "that the learned sense vectors are meaningful; instead, the sense vectors are\n",
      "combined back into word embeddings, which are evaluated in a downstream task:\n",
      "PP attachment prediction.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "PP attachment results seem solid.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "Whether the sense embeddings are meaningful remains uninvestigated. \n",
      "\n",
      "The probabilistic model has some details that are hard to understand. Are the\n",
      "\\lambda_w_i hyperparameters or trained? Where does “rank” come from, is\n",
      "this taken from the sense ranks in WordNet?\n",
      "\n",
      "Related work: the idea of expressing embeddings of words as a convex\n",
      "combination of sense embeddings has been proposed a number of times previously.\n",
      "For instance, Johansson and Nieto Piña “Embedding a semantic network in a\n",
      "word space” (NAACL, 2015) decomposed word embeddings into ontology-grounded\n",
      "sense embeddings based on this idea. Also in unsupervised sense vector training\n",
      "this idea has been used, for instance by Arora et al “Linear Algebraic\n",
      "Structure of Word Senses, with Applications to Polysemy”.\n",
      "\n",
      "Minor comments:\n",
      "\n",
      "no need to define types and tokens, this is standard terminology\n",
      "\n",
      "why is the first \\lamba_w_i in equation 4 needed if the probability is\n",
      "unnormalized?\n",
      "\n",
      "- General Discussion:\n",
      "- Strengths:\n",
      "\n",
      "The paper offers a natural and useful extension to recent efforts in\n",
      "interactive topic modeling, namely by allowing human annotators to provide\n",
      "multiple \"anchor words\" to machine-induced topics. The paper is well-organized\n",
      "and the combination of synthetic and user experiments make for a strong paper.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "The paper is fairly limited in scope in terms of the interactive topic model\n",
      "approaches it compares against. I am willing to accept this, since they do make\n",
      "reference to most of them and explain that these other approaches are not\n",
      "necessarily fast enough for interactive experimentation or not conducive to the\n",
      "types of interaction being considered with an \"anchoring\" interface. Some level\n",
      "of empirical support for these claims would have been nice, though.\n",
      "\n",
      "It would also have been nice to see experiments on more than one data set (20\n",
      "newsgroups, which is now sort of beaten-to-death).\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "In general, this is a strong paper that appears to offer an incremental but\n",
      "novel and practical contribution to interactive topic modeling. The authors\n",
      "made the effort to vet several variants of the approach in simulated\n",
      "experiments, and to conduct fairly exhaustive quantitative analyses of both\n",
      "simulated and user experiments using a variety of metrics that measure\n",
      "different facets of topic quality.\n",
      "- Strengths:\n",
      "Clear description of methods and evaluation\n",
      "Successfully employs and interprets a variety of evaluations\n",
      "Solid demonstration of practicality of technique in real-world interactive\n",
      "topic modeling\n",
      "\n",
      "- Weaknesses:\n",
      "Missing related work on anchor words\n",
      "Evaluation on 20 Newsgroups is not ideal\n",
      "Theoretical contribution itself is small \n",
      "\n",
      "- General Discussion:\n",
      "The authors propose a new method of interactive user specification of topics\n",
      "called Tandem Anchors. The approach leverages the anchor words algorithm, a\n",
      "matrix-factorization approach to learning topic models, by replacing the\n",
      "individual anchors inferred from the Gram-Schmidt algorithm with constructed\n",
      "anchor pseudowords created by combining the sparse vector representations of\n",
      "multiple words that for a topic facet. The authors determine that the use of a\n",
      "harmonic mean function to construct pseudowords is optimal by demonstrating\n",
      "that classification accuracy of document-topic distribution vectors using these\n",
      "anchors produces the most improvement over Gram-Schmidt. They also demonstrate\n",
      "that their work is faster than existing interactive methods, allowing\n",
      "interactive iteration, and show in a user study that the multiword anchors are\n",
      "easier and more effective for users.\n",
      "\n",
      "Generally, I like this contribution a lot: it is a straightforward modification\n",
      "of an existing algorithm that actually produces a sizable benefit in an\n",
      "interactive setting. I appreciated the authors’ efforts to evaluate their\n",
      "method on a variety of scales. While I think the technical contribution in\n",
      "itself is relatively small (a strategy to assemble pseudowords based on topic\n",
      "facets) the thoroughness of the evaluation merited having it be a full paper\n",
      "instead of a short paper. It would have been nice to see more ideas as to how\n",
      "to build these facets in the absence of convenient sources like category titles\n",
      "in 20 Newsgroups or when initializing a topic model for interactive learning.\n",
      "\n",
      "One frustration I had with this paper is that I find evaluation on 20\n",
      "Newsgroups to not be great for topic modeling: the documents are widely\n",
      "different lengths, preprocessing matters a lot, users have trouble making sense\n",
      "of many of the messages, and naive bag-of-words models beat topic models by a\n",
      "substantial margin. Classification tasks are useful shorthand for how well a\n",
      "topic model corresponds to meaningful distinctions in the text by topic; a task\n",
      "like classifying news articles by section or reviews by the class of the\n",
      "subject of the review might be more appropriate. It would also have been nice\n",
      "to see a use case that better appealed to a common expressed application of\n",
      "topic models, which is the exploration of a corpus.\n",
      "\n",
      "There were a number of comparisons I think were missing, as the paper contains\n",
      "little reference to work since the original proposal of the anchor word model.\n",
      "In addition to comparing against standard Gram-Schmidt, it would have been good\n",
      "to see the method from Lee et. al. (2014), “Low-dimensional Embeddings for\n",
      "Interpretable Anchor-based Topic Inference”. I also would have liked to have\n",
      "seen references to Nguyen et. al. (2013), “Evaluating Regularized Anchor\n",
      "Words” and Nguyen et. al. (2015) “Is Your Anchor Going Up or Down? Fast and\n",
      "Accurate Supervised Topic Models”, both of which provide useful insights into\n",
      "the anchor selection process.\n",
      "\n",
      "I had some smaller notes:\n",
      "- 164: …entire dataset\n",
      "- 164-166: I’m not quite sure what you mean here. I think you are claiming\n",
      "that it takes too long to do one pass? My assumption would have been you would\n",
      "use only a subset of the data to retrain the model instead of a full sweep, so\n",
      "it would be good to clarify what you mean.\n",
      "- 261&272: any reason you did not consider the and operator or element-wise\n",
      "max? They seem to correspond to the ideas of union and intersection from the or\n",
      "operator and element-wise min, and it wasn’t clear to me why the ones you\n",
      "chose were better options.\n",
      "- 337: Usenet should be capitalized\n",
      "- 338-340: Why fewer than 100 (as that is a pretty aggressive boundary)? Also,\n",
      "did you remove headers, footers, and/or quotes from the messages?\n",
      "- 436-440: I would have liked to see a bit more explanation of what this tells\n",
      "us about confusion.\n",
      "- 692: using tandem anchors\n",
      "\n",
      "Overall, I think this paper is a meaningful contribution to interactive topic\n",
      "modeling that I would like to see available for people outside the machine\n",
      "learning community to investigate, classify, and test hypotheses about their\n",
      "corpora.\n",
      "\n",
      "POST-RESPONSE: I appreciate the thoughtful responses of the authors to my\n",
      "questions. I would maintain that for some of the complimentary related work\n",
      "that it's useful to compare to non-interactive work, even if it does something\n",
      "different.\n",
      "This paper proposes a framework for evaluation of word embeddings based on data\n",
      "efficiency and simple supervised tasks. The main motivation is that word\n",
      "embeddings are generally used in a transfer learning setting, where evaluation\n",
      "is done based on how faster is to train a target model. The approach uses a set\n",
      "of simple tasks evaluated in a supervised fashion, including common benchmarks\n",
      "such as word similarity and word analogy. Experiments on a broad set of\n",
      "embeddings show that ranks tend to be task-specific and change according to the\n",
      "amount of training data used.\n",
      "\n",
      "Strengths\n",
      "\n",
      "- The transfer learning / data efficiency motivation is an interesting one, as\n",
      "it directly relates to the idea of using embeddings as a simple\n",
      "\"semi-supervised\" approach.\n",
      "\n",
      "Weaknesses\n",
      "\n",
      "- A good evaluation approach would be one that propagates to end tasks.\n",
      "Specifically, if the approach gives some rank R for a set of embeddings, I\n",
      "would like it to follow the same rank for an end task like text classification,\n",
      "parsing or machine translation. However, the approach is not assessed in this\n",
      "way so it is difficult to trust the technique is actually more useful than what\n",
      "is traditionally done.\n",
      "- The discussion about injective embeddings seems completely out-of-topic and\n",
      "does not seem to add to the paper's understanding.\n",
      "- The experimental section is very confusing. Section 3.7 points out that the\n",
      "analysis results in answers to questions as \"is it worth fitting syntax\n",
      "specific embeddings even when supervised datset is large?\" but I fail to\n",
      "understand where in the evaluation the conclusion was made.\n",
      "- Still in Section 3.7, the manuscript says \"This hints, that purely\n",
      "unsupervised large scale pretraining might not be suitable for NLP\n",
      "applications\". This is a very bold assumption and I again fail to understand\n",
      "how this can be concluded from the proposed evaluation approach.\n",
      "- All embeddings were obtained as off-the-shelf pretrained ones so there is no\n",
      "control over which corpora they were trained on. This limits the validity of\n",
      "the evaluation shown in the paper.\n",
      "- The manuscript needs proofreading, especially in terms of citing figures in\n",
      "the right places (why Figure 1, which is on page 3, is only cited in page 6?).\n",
      "\n",
      "General Discussion\n",
      "\n",
      "I think the paper starts with a very interesting motivation but it does not\n",
      "properly evaluate if their approach is good or not. As mentioned above, for any\n",
      "intrinsic evaluation approach I expect to see some study if the conclusions\n",
      "propagate to end tasks and this is not done in the paper. The lack of clarity\n",
      "and proofreading in the manuscript also hinders the understanding. In the\n",
      "future, I think the paper would vastly benefit from some extrinsic studies and\n",
      "a more controlled experimental setting (using the same corpora to train all\n",
      "embeddings, for instance). But in the current state I do not think it is a good\n",
      "addition to the conference.\n",
      "- Strengths:\n",
      "\n",
      "This paper proposed an interesting and important metric for evaluating the\n",
      "quality of word embeddings, which is the \"data efficiency\" when it is used in\n",
      "other supervised tasks.\n",
      "\n",
      "Another interesting point in the paper is that the authors separated out three\n",
      "questions: 1) whether supervised task offers more insights to evaluate\n",
      "embedding quality; 2) How stable is the ranking vs labeled data set size; 3)\n",
      "The benefit to linear vs non-linear models.\n",
      "\n",
      "Overall, the authors presented comprehensive experiments to answer those\n",
      "questions, and the results see quite interesting to know for the research\n",
      "community.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "The overall result is not very useful for ML practioners in this field, because\n",
      "it merely confirms what has been known or suspected, i.e. it depends on the\n",
      "task at hand, the labeled data set size, the type of the model, etc. So, the\n",
      "result in this paper is not very actionable. The reviewer noted that this\n",
      "comprehensive analysis deepens the understanding of this topic.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The paper's presentation can be improved. Specifically: \n",
      "\n",
      "1) The order of the figures/tables in the paper should match the order they are\n",
      "mentioned in the papers. Right now their order seems quite random.\n",
      "\n",
      "2) Several typos (L250, 579, etc). Please use a spell checker.\n",
      "\n",
      "3) Equation 1 is not very useful, and its exposition looks strange. It can be\n",
      "removed, and leave just the text explanations.\n",
      "\n",
      "4) L164 mentions the \"Appendix\", but it is not available in the paper.\n",
      "\n",
      "5) Missing citation for the public skip-gram data set in L425.\n",
      "\n",
      "6) The claim in L591-593 is too strong. It must be explained more clearly, i.e.\n",
      "when it is useful and when it is not.\n",
      "\n",
      "7) The observation in L642-645 is very interesting and important. It will be\n",
      "good to follow up on this and provide concrete evidence or example from some\n",
      "embedding. Some visualization may help too.\n",
      "\n",
      "8) In L672 should provide examples of such \"specialized word embeddings\" and\n",
      "how they are different than the general purpose embedding.\n",
      "\n",
      "9) Figuer 3 is too small to read.\n",
      "- Strengths:\n",
      "\n",
      "originality of the CORE evaluation measure, good accuracy of proposed\n",
      "similarity measure and large number and diversity of datasets for evaluation.\n",
      "\n",
      "- Weaknesses: \n",
      "\n",
      " # some typos\n",
      "   - line 116-117, 'to design of a new' -> 'to design a new'\n",
      "   - line 176-177, figure 2 -> figure 1\n",
      "   - line 265, 'among the the top' -> 'among the top'\n",
      "   - line 320, 'figure 4' should be introduced within the article body.\n",
      "   - line 434, 'the dataset was contains' -> 'the dataset contains'\n",
      "   - line 486-487, table 3 -> table 1\n",
      "   - a 'Tensorflow' should be replaced by 'TextFlow'\n",
      "\n",
      " # imprecisions\n",
      "   - features computation accuracy of lemma, pos or wordnet synset should be\n",
      "detailed in the paper and it should be discussed if it impacts the general\n",
      "similarity accuracy evaluation or not\n",
      "  - the neural networks are said to be implemented in Python but the code is\n",
      "not\n",
      "said to be available - to be able to repeat the experiment\n",
      "  - the training and evaluation sets are said to be shared, but it is not said\n",
      "how (on demand?, under license?) - to be able to repeat the experiment\n",
      "\n",
      "- General Discussion:\n",
      "This paper presents a graph-based approach for producing sense-disambiguated\n",
      "synonym sets from a collection of undisambiguated synonym sets.  The authors\n",
      "evaluate their approach by inducing these synonym sets from Wiktionary and from\n",
      "a collection of Russian dictionaries, and then comparing pairwise synonymy\n",
      "relations (using precision, recall, and F1) against WordNet and BabelNet (for\n",
      "the English synonym sets) or RuThes and Yet Another RussNet (for the Russian\n",
      "synonym sets).\n",
      "\n",
      "The paper is very well written and structured.              The experiments and\n",
      "evaluations\n",
      "(or at least the prose parts) are very easy to follow.              The methodology\n",
      "is\n",
      "sensible and the analysis of the results cogent.  I was happy to observe that\n",
      "the objections I had when reading the paper (such as the mismatch in vocabulary\n",
      "between the synonym dictionaries and gold standards) ended up being resolved,\n",
      "or at least addressed, in the final pages.\n",
      "\n",
      "The one thing about the paper that concerns me is that the authors do not seem\n",
      "to have properly understood the previous work, which undercuts the stated\n",
      "motivation for this paper.\n",
      "\n",
      "The first instance of this misunderstanding is in the paragraph beginning on\n",
      "line 064, where OmegaWiki is lumped in with Wiktionary and Wikipedia in a\n",
      "discussion of resources that are \"not formally structured\" and that contain\n",
      "\"undisambiguated synonyms\".  In reality, OmegaWiki is distinguished from the\n",
      "other two resources by using a formal structure (a relational database) based\n",
      "on word senses rather than orthographic forms.              Translations, synonyms,\n",
      "and\n",
      "other semantic annotations in OmegaWiki are therefore unambiguous.\n",
      "\n",
      "The second, and more serious, misunderstanding comes in the three paragraphs\n",
      "beginning on lines 092, 108, and 120.  Here the paper claims that both BabelNet\n",
      "and UBY \"rely on English WordNet as a pivot for mapping of existing resources\"\n",
      "and criticizes this mapping as being \"error-prone\".  Though it is true that\n",
      "BabelNet uses WordNet as a pivot, UBY does not.  UBY is basically a\n",
      "general-purpose specification for the representation of lexical-semantic\n",
      "resources and of links between them.  It exists independently of any given\n",
      "lexical-semantic resource (including WordNet) and of any given alignment\n",
      "between resources (including ones based on \"similarity of dictionary\n",
      "definitions\" or \"cross-lingual links\").  Its maintainers have made available\n",
      "various databases adhering to the UBY spec; these contain a variety of\n",
      "lexical-semantic resources which have been aligned with a variety of different\n",
      "methods.  A given UBY database can be *queried* for synsets, but UBY itself\n",
      "does not *generate* those synsets.  Users are free to produce their own\n",
      "databases by importing whatever lexical-semantic resources and alignments\n",
      "thereof are best suited to their purposes.  The three criticisms of UBY on\n",
      "lines 120 to 125 are therefore entirely misplaced.\n",
      "\n",
      "In fact, I think at least one of the criticisms is not appropriate even with\n",
      "respect to BabelNet.  The authors claim that Watset may be superior to BabelNet\n",
      "because BabelNet's mapping and use of machine translation are error-prone.  The\n",
      "implication here is that Watset's method is error-free, or at least\n",
      "significantly less error-prone.  This is a very grandiose claim that I do not\n",
      "believe is supported by what the authors ought to have known in advance about\n",
      "their similarity-based sense linking algorithms and graph clustering\n",
      "algorithms, let alone by the results of their study.  I think this criticism\n",
      "ought to be moderated.              Also, I think the third criticism (BabelNet's\n",
      "reliance\n",
      "on WordNet as a pivot) somewhat misses the point -- surely the most important\n",
      "issue to highlight isn't the fact that the pivot is English, but rather that\n",
      "its synsets are already manually sense-annotated.\n",
      "\n",
      "I think the last paragraph of §1 and the first two paragraphs of §2 should be\n",
      "extensively revised. They should focus on the *general* problem of generating\n",
      "synsets by sense-level alignment/translation of LSRs (see Gurevych et al., 2016\n",
      "for a survey), rather than particularly on BabelNet (which uses certain\n",
      "particular methods) and UBY (which doesn't use any particular methods, but can\n",
      "aggregate the results of existing ones).  It may be helpful to point out\n",
      "somewhere that although alignment/translation methods *can* be used to produce\n",
      "synsets or to enrich existing ones, that's not always an explicit goal of the\n",
      "process.  Sometimes it's just a serendipitous (if noisy) side-effect of\n",
      "aligning/translating resources with differing granularities.\n",
      "\n",
      "Finally, at several points in the paper (lines 153, 433), the \"synsets\" of TWSI\n",
      "of JoBimText are criticized for including too many words that are hypernyms,\n",
      "co-hypnomyms, etc. instead of synonyms.  But is this problem really unique to\n",
      "TWSI and JoBimText?  That is, how often do hypernyms, co-hypernyms, etc. appear\n",
      "in the output of Watset?  (We can get only a very vague idea of this from\n",
      "comparing Tables 3 and 5, which analyze only synonym relations.)  If Watset\n",
      "really is better at filtering out words with other semantic relations, then it\n",
      "would be nice to see some quantitative evidence of this.\n",
      "\n",
      "Some further relatively minor points that should nonetheless be fixed:\n",
      "\n",
      "* Lines 047 to 049: The sentence about Kiselev et al. (2015) seems rather\n",
      "useless.  Why bother mentioning their analysis if you're not going to tell us\n",
      "what they found?\n",
      "\n",
      "* Line 091: It took me a long time to figure out how \"wat\" has any relation to\n",
      "\"discover the correct word sense\".  I suppose this is supposed to be a pun on\n",
      "\"what\".  Maybe it would have been better to call the approach \"Whatset\"?  Or at\n",
      "least consider rewording the sentence to better explain the pun.\n",
      "\n",
      "* Figure 2 is practically illegible owing to the microscopic font.  Please\n",
      "increase the text size!\n",
      "\n",
      "* Similarly, Tables 3, 4, and 5 are too small to read comfortably.  Please use\n",
      "a larger font.              To save space, consider abbreviating the headers (\"P,\n",
      "\"R\",\n",
      "\"F1\") and maybe reporting scores in the range 0–100 instead of 0–1, which\n",
      "will eliminate a leading 0 from each column.\n",
      "\n",
      "* Lines 517–522: Wiktionary is a moving target.  To help others replicate or\n",
      "compare against your work, please indicate the date of the Wiktionary database\n",
      "dump you used.\n",
      "\n",
      "* Throughout: The constant switching between Times and Computer Modern is\n",
      "distracting.  The root of this problem is a longstanding design flaw in the ACL\n",
      "2017 LaTeX style file, but it's exacerbated by the authors' decision to\n",
      "occasionally set numbers in math mode, even in running text.  Please fix this\n",
      "by removing\n",
      "\n",
      "\\usepackage{times}\n",
      "\n",
      "from the preamble and replacing it with either\n",
      "\n",
      "\\usepackage{newtxtext}\n",
      "\\usepackage{newtxmath}\n",
      "\n",
      "or\n",
      "\n",
      "\\usepackage{mathptmx}\n",
      "\n",
      "References:\n",
      "\n",
      "I Gurevych, J. Eckle-Kohler, and M. Matuschek, 2016. Linked Lexical Knowledge\n",
      "Bases: Foundations and Applications, volume 34 of Synthesis Lectures on Human\n",
      "Language Technologies, chapter 3: Linking Algorithms, pages 29-44. Morgan &\n",
      "Claypool.\n",
      "\n",
      "----\n",
      "\n",
      "I have read the author response.\n",
      "- Strengths:\n",
      "\n",
      "The paper proposes a new method for word sense induction from synonymy\n",
      "dictionaries. The method presents a conceptual improvement over existing ones\n",
      "and demonstrates robust performance in empirical evaluation. The evaluation was\n",
      "done thoroughly, using a number of benchmarks and strong baseline methods. \n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "Just a couple of small points. I would like to see more discussion of the\n",
      "nature of the evaluation. First, one observes that all models' scores are\n",
      "relatively low, under 50% F1. Is there room for much improvement or is there a\n",
      "natural ceiling of performance due to the nature of the task? The authors\n",
      "discuss lexical sparsity of the input data but I wonder how much of the\n",
      "performance gap this sparsity accounts for. \n",
      "Second, I would also like to see some discussion of the evaluation metric\n",
      "chosen. It is known that word senses can be analyzed at different levels of\n",
      "granularity, which can naturally affect the scores of any system.\n",
      "Another point is that it is not clear how the authors obtained vectors for word\n",
      "senses that they used in 3.4, if the senses are only determined after this\n",
      "step, and anyway senses are not marked in the input corpora. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "I recommend the paper for presentation at the ACL Meeting. Solid work.\n",
      "Strengths:\n",
      "\n",
      "- Innovative idea: sentiment through regularization\n",
      "- Experiments appear to be done well from a technical point of view\n",
      "- Useful in-depth analysis of the model\n",
      "\n",
      "Weaknesses:\n",
      "\n",
      "- Very close to distant supervision\n",
      "- Mostly poorly informed baselines\n",
      "\n",
      "General Discussion:\n",
      "\n",
      "This paper presents an extension of the vanilla LSTM model that\n",
      "incorporates sentiment information through regularization.  The\n",
      "introduction presents the key claims of the paper: Previous CNN\n",
      "approaches are bad when no phrase-level supervision is present.\n",
      "Phrase-level annotation is expensive. The contribution of this paper is\n",
      "instead a \"simple model\" using other linguistic resources.\n",
      "\n",
      "The related work section provides a good review of sentiment\n",
      "literature. However, there is no mention of previous attempts at\n",
      "linguistic regularization (e.g., [YOG14]).\n",
      "\n",
      "The explanation of the regularizers in section 4 is rather lengthy and\n",
      "repetitive. The listing on p. 3 could very well be merged with the\n",
      "respective subsection 4.1-4.4. Notation in this section is inconsistent\n",
      "and generally hard to follow. Most notably, p is sometimes used with a\n",
      "subscript and sometimes with a superscript.  The parameter \\beta is\n",
      "never explicitly mentioned in the text. It is not entirely clear to me\n",
      "what constitutes a \"position\" t in the terminology of the paper. t is a\n",
      "parameter to the LSTM output, so it seems to be the index of a\n",
      "sentence. Thus, t-1 is the preceding sentence, and p_t is the prediction\n",
      "for this sentence. However, the description of the regularizers talks\n",
      "about preceding words, not sentences, but still uses. My assumption here\n",
      "is that p_t is actually overloaded and may either mean the sentiment of\n",
      "a sentence or a word. However, this should be made clearer in the text.\n",
      "\n",
      "One dangerous issue in this paper is that the authors tread a fine line\n",
      "between regularization and distant supervision in their work. The\n",
      "problem here is that there are many other ways to integrate lexical\n",
      "information from about polarity, negation information, etc. into a model\n",
      "(e.g., by putting the information into the features). The authors\n",
      "compare against a re-run or re-implementation of Teng et al.'s NSCL\n",
      "model. Here, it would be important to know whether the authors used the\n",
      "same lexicons as in their own work. If this is not the case, the\n",
      "comparison is not fair. Also, I do not understand why the authors cannot\n",
      "run NSCL on the MR dataset when they have access to an implementation of\n",
      "the model. Would this not just be a matter of swapping the datasets? The\n",
      "remaining baselines do not appear to be using lexical information, which\n",
      "makes them rather poor. I would very much like to see a vanilla LSTM run\n",
      "where lexical information is simply appended to the word vectors.\n",
      "\n",
      "The authors end the paper with some helpful analysis of the\n",
      "models. These experiments show that the model indeed learns\n",
      "intensification and negation to some extent. In these experiments, it\n",
      "would be interesting to know how the model behaves with\n",
      "out-of-vocabulary words (with respect to the lexicons). Does the model\n",
      "learn beyond memorization, and does generalization happen for words that\n",
      "the model has not seen in training? Minor remark here: the figures and\n",
      "tables are too small to be read in print.\n",
      "\n",
      "The paper is mostly well-written apart from the points noted above.  It\n",
      "could benefit from some proofreading as there are some grammatical\n",
      "errors and typos left. In particular, the beginning of the abstract is\n",
      "hard to read.\n",
      "\n",
      "Overall, the paper pursues a reasonable line of research. The largest\n",
      "potential issue I see is a somewhat shaky comparison to related\n",
      "work. This could be fixed by including some stronger baselines in the\n",
      "final model. For me, it would be crucial to establish whether\n",
      "comparability is given in the experiments, and I hope that the authors\n",
      "can shed some light on this in their response.\n",
      "\n",
      "[YOG14] http://www.aclweb.org/anthology/P14-1074\n",
      "\n",
      "--------------\n",
      "\n",
      "Update after author response\n",
      "\n",
      "Thank you for clarifying the concerns about the experimental setup. \n",
      "\n",
      "NSCL: I do now believe that the comparison is with Teng et al. is fair.\n",
      "\n",
      "LSTM: Good to know that you did this. However, this is a crucial part of the\n",
      "paper. As it stands, the baselines are weak. Marginal improvement is still too\n",
      "vague, better would be an open comparison including a significance test.\n",
      "\n",
      "OOV: I understand how the model is defined, but what is the effect on OOV\n",
      "words? This would make for a much more interesting additional experiment than\n",
      "the current regularization experiments.\n",
      "- Strengths:\n",
      "This paper proposes a nice way to combine the neural model (LSTM) with\n",
      "linguistic knowledge (sentiment lexicon, negation and intensity). The method is\n",
      "simple yet effective. It achieves the state-of-the-art performance on Movie\n",
      "Review dataset and is competitive against the best models on SST dataset.    \n",
      "\n",
      "- Weaknesses:\n",
      "Similar idea has also been used in (Teng et al., 2016). Though this work is \n",
      "more elegant in the framework design and mathematical representation, the\n",
      "experimental comparison with (Teng et al., 2016) is not as convincing as the\n",
      "comparisons with the rest methods. The authors only reported the\n",
      "re-implementation results on the sentence level experiment of SST and did not\n",
      "report their own phrase-level results.\n",
      "\n",
      "Some details are not well explained, see discussions below.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The reviewer has the following questions/suggestions about this work,\n",
      "\n",
      "1. Since the SST dataset has phrase-level annotations, it is better to show the\n",
      "statistics of the times that negation or intensity words actually take effect.\n",
      "For example, how many times the word \"nothing\" appears and how many times it\n",
      "changes the polarity of the context.\n",
      "\n",
      "2. In section 4.5, the bi-LSTM is used for the regularizers. Is bi-LSTM used to\n",
      "predict the sentiment label?\n",
      "\n",
      "3. The authors claimed that \"we only use the sentence-level annotation since\n",
      "one of\n",
      "our goals is to avoid expensive phrase-level annotation\". However, the reviewer\n",
      "still suggest to add the results. Please report them in the rebuttal phase if\n",
      "possible.\n",
      "\n",
      "4. \"s_c is a parameter to be optimized but could also be set fixed with prior\n",
      "knowledge.\"  The reviewer didn't find the specific definition of s_c in the\n",
      "experiment section, is it learned or set fixed?  What is the learned or fixed\n",
      "value?\n",
      "\n",
      "5. In section 5.4 and 5.5, it is suggested to conduct an additional experiment\n",
      "with part of the SST dataset where only phrases with negation/intensity words\n",
      "are included. Report the results on this sub-dataset with and without the\n",
      "corresponding regularizer can be more convincing.\n",
      "- Strengths:\n",
      "\n",
      "The deviation between \"vocal\" users and \"average users\" is an interesting\n",
      "discovery that could be applied as a way to identify different types of users.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "I see it as an initial work on a new topic that should be expanded in the\n",
      "future. A possible comparison between matrix factorization and similar topics \n",
      "in distributional semantics (e.g. latent semantic analysis) would be useful. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "In this paper, the authors describe an approach for modeling the\n",
      "stance/sentiment of Twitter users about topics. In particular, they address the\n",
      "task of inter-topic preferences modeling. This task consists of measuring the\n",
      "degree to which the stances about different topics are mutually related.This\n",
      "work is claimed to advance state of the art in this task, since previous works\n",
      "were case studies, while the proposed one is about unlimited topics on\n",
      "real-world data.The adopted approach consists of the following steps: A set of\n",
      "linguistic patterns was manually created and, through them, a large number of\n",
      "tweets expressing stance towards various topics was collected. Next, the texts\n",
      "were expressed as triples containing user, topic, and evaluation. The\n",
      "relationships represented by the tuples were arranged as a sparse matrix. After\n",
      "matrix factorization, a low-rank approximation was performed. The optimal rank\n",
      "was identified as 100. The definition of cosine similarity is used to measure\n",
      "the similarity between topics and, thus, detect latent preferences not\n",
      "represented in the original sparse matrix. Finally, cosine similarity is also\n",
      "used to detect inter-topic preferences.A preliminary empirical evaluation shows\n",
      "that the model predicts missing topics preferences. Moreover, predicted\n",
      "inter-topic preferences moderately correlate with the corresponding values from\n",
      "a crowdsourced gold-standard collection of preferences. \n",
      "According to the overview discussed in the related work section, there are no\n",
      "previous systems to be compared in the latter task (i.e. prediction of\n",
      "inter-topic preferences) and, for this reason, it is promising.\n",
      "\n",
      "I listed some specific comments below.\n",
      "\n",
      "- Rows 23 and 744, \"high-quality\": What makes them high-quality? If not\n",
      "properly defined, I would remove all the occurrences of \"high-quality\" in the\n",
      "paper.\n",
      "\n",
      "- Row 181 and caption of Figure 1: I would remove the term \"generic.\"\n",
      "\n",
      "- Row 217, \"This section collect\": -> \"We collected\" or \"This section explains\n",
      "how we collected\"- Row 246: \"ironies\" -> \"irony\"\n",
      "\n",
      "- Row 269, \"I support TPP\": Since the procedure can detect various patterns\n",
      "such as \"to A\" or \"this is A,\" maybe the author should explain that all\n",
      "possible patterns containing the topic are collected, and next manually\n",
      "filtered?\n",
      "\n",
      "- Rows 275 and 280, \"unuseful\": -> useless\n",
      "\n",
      "- Row 306, \"including\": -> are including\n",
      "\n",
      "- Row 309:  \"of\" or \"it\" are not topics but, I guess, terms retrieved by\n",
      "mistakes as topics. \n",
      "\n",
      "- Rows 317-319: I would remove the first sentence and start with \"Twitter\n",
      "user...\"\n",
      "\n",
      "- Rows 419-439: \"I like the procedure used to find the optimal k. In previous\n",
      "works, this number is often assumed, while it is useful to find it\n",
      "empirically.\"\n",
      "\n",
      "- Row 446, \"let\": Is it \"call\"?\n",
      "- Strengths:\n",
      "\n",
      "Detailed guidelines and explicit illustrations.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "The document-independent crowdsourcing annotation is unreliable. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "This work creates a new benchmark corpus for concept-map-based MDS. It is well\n",
      "organized and written clearly. The supplement materials are sufficient. I have\n",
      "two questions here.\n",
      "1)              Is it necessary to treat concept map extraction as a separate\n",
      "task?\n",
      "On\n",
      "the one hand, many generic summarization systems build a similar knowledge\n",
      "graph and then generate summaries accordingly. On the other hand, with the\n",
      "increase of the node number, the concept map becomes growing hard to\n",
      "distinguish. Thus, the general summaries should be more readable.\n",
      "2)              How can you determine the importance of a concept independent of\n",
      "the\n",
      "documents? The definition of summarization is to reserve the main concepts of\n",
      "documents. Therefore, the importance of a concept highly depends on the\n",
      "documents. For example, in the given topic of coal mining accidents, assume\n",
      "there are two concepts: A) an instance of coal mining accidents and B) a cause\n",
      "of coal mining accidents. Then, if the document describes a series of coal\n",
      "mining accidents, A is more important than B. In comparison, if the document\n",
      "explores why coal mining accidents happen, B is more significant than A.\n",
      "Therefore, just given the topic and two concepts A&B, it is impossible to judge\n",
      "their relative importance.\n",
      "\n",
      "I appreciate the great effort spent by authors to build this dataset. However,\n",
      "this dataset is more like a knowledge graph based on common sense rather than\n",
      "summary.\n",
      "Strengths:\n",
      "\n",
      "This paper presents an approach to creating concept maps using crowdsourcing.\n",
      "The general ideas are interesting and the main contribution lies in the\n",
      "collection of the dataset. As such, I imagine that the dataset will be a\n",
      "valuable resource for further research in this field. Clearly a lot of effort\n",
      "has gone into this work.\n",
      "\n",
      "Weaknesses:\n",
      "\n",
      "Overall I felt this paper a bit overstated in placed. As an example, the\n",
      "authors claim a new crowdsourcing scheme as one of their contributions. This\n",
      "claims is quite strong though and it reads more like the authors are applying\n",
      "best practice in crowdsourcing to their work. This isn’t a novel methods\n",
      "then, it’s rather a well thought and sound application of existing knowledge.\n",
      "\n",
      "Similarly, the authors claim that they develop and present a new corpus. This\n",
      "seems true and I can see how a lot of effort was invested in its preparation,\n",
      "but then Section 4.1 reveals that actually this is based on an existing\n",
      "dataset. \n",
      "\n",
      "This is more a criticism of the presentation than the work though.\n",
      "\n",
      "General discussion:\n",
      "\n",
      "Where do the summary sentences come from for the crowdsource task? Aren’t\n",
      "they still quite subjective?\n",
      "\n",
      "Where do the clusters come from? Are they part of the TAC2008b dataset? \n",
      "\n",
      "In 4.6 expert annotators are used to create the gold standard concept maps.\n",
      "More information is needed in this section I would say as it seems to be quite\n",
      "crucial. How were they trained, what made them experts?\n",
      "The paper describes a deep-learning-based model for parsing the creole\n",
      "Singaporean English to Universal Dependencies. They implement a parser based on\n",
      "the model by Dozat and Manning (2016) and add neural stacking (Chen et al.,\n",
      "2016) to it. They train an English model and then use some of the hidden\n",
      "representations of the English model as input to their Singlish parser. This\n",
      "allows them to make use of the much larger English training set along with a\n",
      "small Singlish treebank, which they annotate. They show that their approach\n",
      "(LAS 76.57) works better than just using an English parser (LAS 65.6) or\n",
      "training a parser on their small Singlish data set (LAS 64.01). They also\n",
      "analyze for which\n",
      "common constructions, their approach improves parsing quality. \n",
      "\n",
      "They also describe and evaluate a stacked POS model based on Chen et al.\n",
      "(2016), they discuss how common constructions should be analyzed in the UD\n",
      "framework, and they provide an annotated treebank of 1,200 sentences. 100 of\n",
      "them were annotated by two people and their inter-annotator agreement was 85.3\n",
      "UAS and 75.7 LAS.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      " - They obtain good results and their experimental setup appears to be solid.\n",
      "\n",
      " - They perform many careful analyses and explore the influence on many\n",
      "parameters of their model.\n",
      "\n",
      " - They provide a small Singlish treebank annotated according to the Universal\n",
      "Dependencies v1.4 guidelines.\n",
      "\n",
      " - They propose very sound guidelines on how to analyze common Singlish\n",
      "constructions in UD.\n",
      "\n",
      " - Their method is linguistically informed and they nicely exploit similarity\n",
      "between standard English and the creole Singaporean English.\n",
      "\n",
      " - The paper presents methods for a low-resource language.\n",
      "\n",
      " - They are not just applying an existing English method to another language\n",
      "but instead present a method that can be potentially used for other closely\n",
      "related language pairs.\n",
      "\n",
      " - They use a well-motivated method for selecting the sentences to include in\n",
      "their treebank.\n",
      "\n",
      " - The paper is very well written and easy to read.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      " - The annotation quality seems to be rather poor. They performed double\n",
      "annotation of 100 sentences and their inter-annotator agreement is just 75.72%\n",
      "in terms of LAS. This makes it hard to assess how reliable the estimate of the\n",
      "LAS of their model is, and the LAS of their model is in fact slightly higher\n",
      "than the inter-annotator agreement. \n",
      "\n",
      "UPDATE: Their rebuttal convincingly argued that the second annotator who just\n",
      "annotated the 100 examples to compute the IAA didn't follow the annotation\n",
      "guidelines for several common constructions. Once the second annotator fixed\n",
      "these issues, the IAA was reasonable, so I no longer consider this a real\n",
      "issue.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "I am a bit concerned about the apparently rather poor annotation quality of the\n",
      "data and how this might influence the results, but overall, I liked the paper\n",
      "a lot and I think this would be a good contribution to the conference.\n",
      "\n",
      "- Questions for the authors:\n",
      "\n",
      " - Who annotated the sentences? You just mention that 100 sentences were\n",
      "annotated by one of the authors to compute inter=annotator agreement but you\n",
      "don't mention who annotated all the sentences.\n",
      "\n",
      " - Why was the inter-annotator agreement so low? In which cases was there\n",
      "disagreement? Did you subsequently discuss and fix the sentences for which\n",
      "there was disagreement?\n",
      "\n",
      " - Table A2: There seem to be a lot of discourse relations (almost as many as\n",
      "dobj relations) in your treebank. Is this just an artifact of the colloquial\n",
      "language or did you use \"discourse\" for things that are not considered\n",
      "\"discourse\" in other languages in UD?\n",
      "\n",
      " - Table A3: Are all of these discourse particles or discourse + imported\n",
      "vocab? If the latter, perhaps put them in separate tables, and glosses would be\n",
      "helpful.\n",
      "\n",
      "- Low-level comments:\n",
      "\n",
      " - It would have been interesting if you had compared your approach to the one\n",
      "by Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you\n",
      "should mention this paper in the reference section.\n",
      "\n",
      " - You use the word \"grammar\" in a slightly strange way. I think replacing\n",
      "\"grammar\" with syntactic constructions would make it clearer what you try to\n",
      "convey. (e.g., line 90)\n",
      "\n",
      " - Line 291: I don't think this can be regarded as a variant of\n",
      "it-extraposition. But I agree with the analysis in Figure 2, so perhaps just\n",
      "get rid of this sentence.\n",
      "\n",
      " - Line 152: I think the model by Dozat and Manning (2016) is no longer\n",
      "state-of-the art, so perhaps just replace it with \"very high performing model\"\n",
      "or something like that.\n",
      "\n",
      " - It would be helpful if you provided glosses in Figure 2.\n",
      "- Strengths:\n",
      "Nice results, nice data set. Not so much work on Creole-like languages,\n",
      "especially English.  \n",
      "\n",
      "- Weaknesses:\n",
      "A global feeling of \"Deja-vu\", a lot of similar techniques have been applied to\n",
      "other domains, other ressource-low languages. Replace word embeddings by\n",
      "clusters and neural models by whatever was in fashion 5 years ago and we can\n",
      "find more or less the same applied to Urdu or out-of-domain parsing. I liked\n",
      "this paper though, but I would have appreciated the authors to highlight more\n",
      "their contributions and position their work better within the literature.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "This paper presents a set of experiments designed a) to show the effectiveness\n",
      "of a neural parser  in a scarce resource scenario and b) to introduce a new\n",
      "data set of Creole English (from Singapour, called Singlish). While this data\n",
      "set is relatively small (1200 annotated sentences, used with 80k unlabeled\n",
      "sentences for word embeddings induction), the authors manage to present\n",
      "respectable results via interesting approach even though using features from\n",
      "relatively close languages are not unknown from the parsing community (see all\n",
      "the line of work on parsing Urdu/Hindi, on Arabic dialect using MSA based\n",
      "parsers, and so on).\n",
      "Assuming we can see Singlish as an extreme of Out-of-domain English and given\n",
      "all the set of experiments, I wonder why the authors didn’t try the classical\n",
      "technique on domain-adaptation, namely training with UD_EN+90% of the Singlish\n",
      "within a 10 cross fold experiment ? just so we can have another interesting\n",
      "baseline (with and without word embeddings, with bi-lingual embeddings if\n",
      "enough parallel data is available).\n",
      "I think that paper is interesting but I really would have appreciated more\n",
      "positioning regarding all previous work in parsing low-ressources languages and\n",
      "extreme domain adaptation. A table presenting some results for Irish and other\n",
      "very small treebanks would be nice.\n",
      "Also how come the IAA is so low regarding the labeled relations?\n",
      "\n",
      "*****************************************\n",
      "Note after reading the authors' answer\n",
      "*****************************************\n",
      "\n",
      "Thanks for your clarifications (especially for redoing the IAA evaluation). I\n",
      "raised my recommendation to 4, I hope it'll get accepted.\n",
      "The authors construct a new dataset of 1200 Singaporean English (Singlish)\n",
      "sentences annotated with Universal Dependencies. They show that they can\n",
      "improve the performance of a POS tagger and a dependency parser on the Singlish\n",
      "corpus by integrating English syntactic knowledge via a neural stacking model.\n",
      "\n",
      "- Strengths:\n",
      "Singlish is a low-resource language. The NLP community needs more data for low\n",
      "resource languages, and the dataset accompanying this paper is a useful\n",
      "contribution. There is also relatively little NLP research on creoles, and the\n",
      "potential of using transfer-learning to analyze creoles, and this paper makes a\n",
      "nice contribution in that area.\n",
      "\n",
      "The experimental setup used by the authors is clear. They provide convincing\n",
      "evidence that incorporating knowledge from an English-trained parser into a\n",
      "Singlish parser outperforms both an English-only parser and a Singlish-only\n",
      "parser on the Singlish data. They also provide a good overview of the relevant\n",
      "differences between English and Singlish for the purposes of syntactic parser\n",
      "and a useful analysis of how different parsing models handle these\n",
      "Singlish-specific constructions.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "There are three main issues I see with this paper:\n",
      "*  There is insufficient comparison to the UD annotation of non-English\n",
      "languages. Many of the constructions they bring up as specific to Singlish are\n",
      "also present in other UD languages, and the annotations should ideally be\n",
      "consistent between Singlish and these languages.\n",
      "*  I'd like to see an analysis on the impact of training data size. A central\n",
      "claim of this paper is that using English data can improve performance on a\n",
      "low-resource language like Singlish. How much more Singlish data would be\n",
      "needed before the English data became unnecessary?\n",
      "*  What happens if you train a single POS/dep parsing model on the concatenated\n",
      "UD Web and Singlish datasets? This is much simpler than incorporating neural\n",
      "stacking. The case for neural stacking is stronger if it can outperform this\n",
      "baseline.\n",
      "\n",
      "- General Discussion:\n",
      "Line 073: “POS taggers and dependency parsers perform poorly on such Singlish\n",
      "texts based on our observations” - be more clear that you will quantify this\n",
      "later. As such, it seems a bit hand-wavy.\n",
      "\n",
      "Line 169: Comparison to neural network models for multi-lingual parsing. As far\n",
      "as I can tell, you don't directly try the approach of mapping Singlish and\n",
      "English word embeddings into the same embedding space.\n",
      "\n",
      "Line 212: Introduction of UD Eng. At this point, it is appropriate to point out\n",
      "that the Singlish data is also web data, so the domain matches UD Eng.\n",
      "\n",
      "Line 245: “All borrowed words are annotated according to their original\n",
      "meanings”. Does this mean they have the same POS as in  the language from\n",
      "which they were borrowed? Or the POS of their usage in Singlish?\n",
      "\n",
      "Figure 2: Standard English glosses would be very useful in understanding the\n",
      "constructions and checking the correctness of the UD relations used.\n",
      "\n",
      "Line 280: Topic prominence: You should compare with the “dislocated” label\n",
      "in UD. From the UD paper: “The dislocated relation captures preposed (topics)\n",
      "and postposed elements”. The syntax you are describing sounds similar to a\n",
      "topic-comment-style syntax; if it is different, then you should make it clear\n",
      "how.\n",
      "\n",
      "Line 294: “Second, noun phrases used to modify the predicate with the\n",
      "presence of a preposition is regarded as a “nsubj” (nominal subject).”\n",
      "Here, I need a gloss to determine if this analysis makes sense. If the phrase\n",
      "is really being used to modify the predicate, then this should not be nsubj. UD\n",
      "makes a distinction between core arguments (nsubj, dobj, etc) and modifiers. If\n",
      "this is a case of modification, then you should use one of the modification\n",
      "relations, not a core argument relation. Should clarify the language here.\n",
      "\n",
      "Line 308: “In UD-Eng standards, predicative “be” is the only verb used as\n",
      "a copula, which often depends on its complement to avoid copular head.” This\n",
      "is an explicit decision made in UD, to increase parallelism with non-copular\n",
      "languages (e.g., Singlish). You should call this out. I think the rest of the\n",
      "discussion of copula handling is not necessary.\n",
      "\n",
      "Line 322: “NP deletion: Noun-phrase (NP) deletion often results in null\n",
      "subjects or objects.” This is common in other languages (zero-anaphora in\n",
      "e.g. Spanish, Italian, Russian, Japanese… )Would be good to point this out,\n",
      "and also point to how this is dealt with in UD in those languages (I believe\n",
      "the same way you handle it).\n",
      "\n",
      "Ling 330: Subj/verb inversion is common in interrogatives in other languages\n",
      "(“Fue Marta al supermercado/Did Marta go to the supermarket?”). Tag\n",
      "questions are present in English (though perhaps are not as frequent). You\n",
      "should make sure that your analysis is consistent with these languages.\n",
      "\n",
      "Sec 3.3 Data Selection and Annotation:\n",
      "The way you chose the Singlish sentences, of course an English parser will do\n",
      "poorly (they are chosen to be disimilar to sentences an English parser has seen\n",
      "before). But do you have a sense of how a standard English parser does overall\n",
      "on Singlish, if it is not filtered this way? How common are sentences with\n",
      "out-of-vocabulary terms or the constructions you discussed in 3.2?\n",
      "\n",
      "A language will not necessarily capture unusual sentence structure,\n",
      "particularly around long-distance dependencies. Did you investigate whether\n",
      "this method did a good job of capturing sentences with the grammatical\n",
      "differences to English you discussed in Section 3.2?\n",
      "\n",
      "Line 415: “the inter-annotator agreement has an unlabeled attachment score\n",
      "(UAS) of 85.30% and a labeled attachment score (LAS) of 75.72%.”\n",
      "*  What’s the agreement on POS tags? Is this integrated with LAS?\n",
      "*  Note that in Silveira et al 2014, which produced UD-Eng, they measured 94%\n",
      "inter-annotator agreement on a per-token basis. Why the discrepancy?\n",
      "\n",
      "POS tagging and dep parsing sections:\n",
      "For both POS-tagging and dep parsing, I’d like to see some analysis on the\n",
      "effect of training set size. E.g., how much more Singlish data would be needed\n",
      "to train a POS tagger/dep parser entirely on Singlish and get the same accuracy\n",
      "as the stacked model?\n",
      "\n",
      "What happens if you just concatenate the datasets? E.g., train a model on a\n",
      "hybrid dataset of EN and Singlish, and see what the result is?\n",
      "\n",
      "Line 681: typo: “pre-rained” should be “pre-trained”\n",
      "\n",
      "742 “The neural stacking model leads to the biggest improvement over nearly\n",
      "all categories except for a slightly lower yet competitive performance on “NP\n",
      "Deletion” cases” --- seems that the English data strongly biases the parser\n",
      "to expect an explicit subj/obj. you could try deleting subj/obj from some\n",
      "English sentences to improve performance on this construction.\n",
      "- Strengths:\n",
      "\n",
      "This paper proposes to apply NLP to speech transcripts (narratives and\n",
      "descriptions) in order to identify patients with MCI (mild cognitive\n",
      "impairment, ICD-10 code F06.7). The authors claim that they were able to\n",
      "distinguish between healthy control participants and patients with MCI (lines\n",
      "141-144). However in the conclusion, lines 781-785, they say that “…\n",
      "accuracy ranging from 60% to 85% …. means that it is not easy to distinguish\n",
      "between healthy subjects and those with cognitive impairments”. So the paper\n",
      "beginning is more optimistic than the conclusion but anyway the message is\n",
      "encouraging and the reader becomes curious to see more details about what has\n",
      "been actually done.\n",
      "\n",
      "The corpus submitted in the dataset is constructed for 20 healthy patients and\n",
      "20 control participants only (20+20), and it is non-understandable for people\n",
      "who do not speak Portuguese. It would be good to incorporate more technological\n",
      "details in the article and probably to include at least one example of a short\n",
      "transcript that is translated to English, and eventually a (part of a) sample\n",
      "network with embeddings for this transcript.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "The paper starts with a detailed introduction and review of relevant work. Some\n",
      "of the cited references are more or less NLP background so they can be omitted\n",
      "e.g. (Salton 1989) in section 4.2.3. Other references are not directly related\n",
      "to the topic e.g. “sentiment classification” and “pedestrian detection in\n",
      "images”, lines 652-654, and they can be omitted too. In general lines\n",
      "608-621, section 4.2.3 can be shortened as well etc. etc. The suggestion is to\n",
      "compress the first 5 pages, focusing the review strictly on the paper topic,\n",
      "and consider the technological innovation in more detail, incl. samples of\n",
      "English translations of the ABCD and/or Cindarela narratives.\n",
      "\n",
      "The relatively short narratives in Portuguese esp. in ABCD dataset open the\n",
      "question how the similarities between words have been found, in order to\n",
      "construct word embeddings. In lines 272-289 the authors explain that they\n",
      "generate word-level networks from continuous word representations. What is the\n",
      "source for learning the continuous word representations; are these the datasets\n",
      "ABCD+Cinderella only, or external corpora were used? In lines 513-525 it is\n",
      "written that sub-word level (n-grams) networks were used to generate word\n",
      "embeddings. Again, what is the source for the training? Are we sure that the\n",
      "two kinds of networks together provide better accuracy? And what are the\n",
      "“out-of-vocabulary words” (line 516), from where they come?\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "It is important to study how NLP can help to discover cognitive impairments;\n",
      "from this perspective the paper is interesting. Another interesting aspect is\n",
      "that it deals with NLP for Portuguese, and it is important to explain how one\n",
      "computes embeddings for a language with relatively fewer resources (compared to\n",
      "English). \n",
      "\n",
      "The text needs revision: shortening sections 1-3, compressing 4.1 and adding\n",
      "more explanations about the experiments. Some clarification about the NURC/SP\n",
      "N. 338 EF and 331 D2 transcription norms can be given.\n",
      "\n",
      "Technical comments:\n",
      "\n",
      "Line 029: ‘… as it a lightweight …’ -> shouldn’t this be ‘… as in\n",
      "a lightweight …’\n",
      "\n",
      "Line 188: PLN -> NLP\n",
      "\n",
      "Line 264: ‘out of cookie out of the cookie’ – some words are repeated\n",
      "twice \n",
      "\n",
      "Table 3, row 2, column 3: 72,0 -> 72.0\n",
      "\n",
      "Lines 995-996: the DOI number is the same as the one at lines 1001-1002; the\n",
      "link behind the title at lines 992-993 points to the next paper in the list\n",
      "- Strengths:\n",
      "This paper explores is problem of identifying patients with Mild Cognitive\n",
      "Impairment (MCI) by analyzing speech transcripts available from three different\n",
      "datasets. A graph based method leveraging co-occurrence information between\n",
      "words found in transcripts is described. Features are encoded using different\n",
      "characteristics of the graph lexical, syntactic properties, and many others. \n",
      "Results are reported using 5 fold cross validation using a number of\n",
      "classifiers. Different models exhibit different performance across the three\n",
      "datasets. This work targets a well defined problem and uses appropriate\n",
      "datasets. \n",
      "\n",
      "- Weaknesses:\n",
      "The paper suffers from several drawbacks\n",
      "1. The paper is hard to read due to incorrect usage of English. The current\n",
      "manuscript would benefit a  lot from a review grammar and spellings. \n",
      "2. The main machine learning problem being addressed is poorly described. What\n",
      "was a single instance of classification? It seems every transcripts was\n",
      "classified as MCI or No MCI. If this is the case, the dataset descriptions\n",
      "should describe the numbers at a transcript level. Tables 1,2, and 3 should\n",
      "describe the data not the study that produced the transcripts. The age of the\n",
      "patients is irrelevant for the classification task. A lot of text (2 pages) is\n",
      "consumed in simply describing the datasets with details that do not affect the\n",
      "end classification task. Also, I was unsure why numbers did not add up. For\n",
      "e.g.: in section 4.1.1 the text says 326 people were involved. But the total\n",
      "number of males and females in Table 1 are less than 100?\n",
      "3. What is the motivation behind enriching the graph? Why not represent each\n",
      "word by a node in the graph and connect them by the similarity between their\n",
      "vectors, irrespective of co-occurrence?\n",
      "4. The datsets are from a biomedical domain. No domain specific tools have been\n",
      "leveraged.\n",
      "5. Since dataset class distribution is unclear, it is unclear to determine if\n",
      "accuracy is a good measure for evaluation. In either case, since it is a binary\n",
      "classification task, F1 would have been a desirable metric. \n",
      "6. Results are reported unto 4 decimal places on very small datasets (43\n",
      "transcripts) without statistical tests over increments. Therefore, it is\n",
      "unclear if the gains are significant.\n",
      "The paper describes a novel application of mostly existing representations,\n",
      "features sets, and methods: namely, detecting Mild Cognitive Impairment (MCI) \n",
      "in speech narratives. The nature of the problem, datasets, and domain are\n",
      "thoroughly described. While missing some detail, the proposed solution and\n",
      "experiments sound reasonable. Overall, I found the study interesting and\n",
      "informative.\n",
      "\n",
      "In terms of drawbacks, the paper needs some considerable editing to improve\n",
      "readability. Details on some key concepts appear to be missing. For example, \n",
      "details on the multi-view learning used are omitted; the set of “linguistic\n",
      "features” needs to be clarified; it is not entirely clear what datasets were\n",
      "used to generate the word embeddings (presumably the 3 datasets described in\n",
      "the paper, which appear to be too small for that purpose…). It is also not\n",
      "clear why disfluencies (filled pauses, false starts, repetitions, etc.) were\n",
      "removed from the dataset. One might suggest that they are important features in\n",
      "the context of MCI. It is also not clear why the most popular tf-idf weighting\n",
      "scheme was not used for the BoW classifications. In addition, tests for\n",
      "significance are not provided to substantiate the conclusions from the\n",
      "experiments. Lastly, the related work is described a bit superficially. \n",
      "\n",
      "Detailed comments are provided below:\n",
      "\n",
      "Abstract: The abstract needs to be shortened. See detailed notes below.\n",
      "\n",
      "Lines 22,23 need rephrasing.            “However, MCI disfluencies produce\n",
      "agrammatical speech impacting in parsing results” → impacting the parsing\n",
      "results?\n",
      "\n",
      "Lines 24,25: You mean correct grammatical errors in transcripts manually? It is\n",
      "not clear why this should be performed, doesn’t the fact that grammatical\n",
      "errors are present indicate MCI? … Only after reading the Introduction and\n",
      "Related Work sections it becomes clear what you mean. Perhaps include some\n",
      "examples of disfluencies.\n",
      "\n",
      "Lines 29,30 need rephrasing: “as it a lightweight and language  independent\n",
      "representation”\n",
      "\n",
      "Lines 34-38 need rephrasing: it is not immediately clear which exactly are the\n",
      "3 datasets. Maybe: “the other two: Cinderella and … “            \n",
      "\n",
      "Line 70: “15% a year” → Not sure what exactly “per year” means…\n",
      "\n",
      "Line 73 needs rephrasing.\n",
      "\n",
      "Lines 115 - 117: It is not obvious why BoW will also have problems with\n",
      "disfluencies, some explanation will be helpful.\n",
      "\n",
      "Lines 147 - 149: What do you mean by “the best scenario”?\n",
      "\n",
      "Line 157: “in public corpora of Dementia Bank” → a link or citation to\n",
      "Dementia Bank will be helpful. \n",
      "\n",
      "Line 162: A link or citation describing the “Picnic picture of the Western\n",
      "Aphasia Battery” will be helpful.\n",
      "\n",
      "Line 170: An explanation as to what the WML subtest is will be helpful.\n",
      "\n",
      "Line 172 is missing citations.\n",
      "\n",
      "Lines 166 - 182: This appears to be the core of the related work and it is\n",
      "described a bit superficially. For example, it will be helpful to know\n",
      "precisely what methods were used to achieve these tasks and how they compare to\n",
      "this study.\n",
      "\n",
      "Line 185: Please refer to the conference citation guidelines. I believe they\n",
      "are something along these lines: “Aluisio et al. (2016)  used…”\n",
      "\n",
      "Line 188: The definition of “PLN” appears to be missing.\n",
      "\n",
      "Lines 233 - 235 could you some rephrasing. Lemmatization is not necessarily a\n",
      "last step in text pre-processing and normalization, in fact there are also\n",
      "additional common normalization/preprocessing steps omitted. \n",
      "\n",
      "Lines 290-299: Did you create the word embeddings using the MCI datasets or\n",
      "external datasets?\n",
      "\n",
      "Line 322: consisted of → consist of\n",
      "\n",
      "Lines 323: 332 need to be rewritten. ... “manually segmented of the\n",
      "DementiaBank and Cinderella” →  What do you mean by segmented, segmented\n",
      "into sentences? Why weren’t all datasets automatically segmented?; “ABCD”\n",
      "is not defined; You itemized the datasets in i) and ii), but subsequently  you\n",
      "refer to 3 dataset, which is a bit confusing. Maybe one could explicitly name\n",
      "the datasets, as opposed to referring to them as “first”, “second”,\n",
      "“third”.\n",
      "\n",
      "Table 1 Caption: The demographic information is present, but there are no any\n",
      "additional statistics of the dataset, as described.\n",
      "\n",
      "Lines 375 - 388:  It is not clear why filled pauses, false starts, repetitions,\n",
      "etc. were removed. One might suggest that they are important features in the\n",
      "context of MCI ….\n",
      "\n",
      "Line 399: … multidisciplinary team with psychiatrists ... → consisting of\n",
      "psychiatrists…\n",
      "\n",
      "Lines 340-440: A link or citation describing the transcription norms will be\n",
      "helpful.\n",
      "\n",
      "Section 4.2.1. It is not clear what dataset was used to generate the word\n",
      "embeddings. \n",
      "\n",
      "Line 560. The shortest path as defined in feature 6?\n",
      "\n",
      "Section “4.2.2 Linguistic Features” needs to be significantly expanded for\n",
      "clarity. Also, please check the conference guidelines regarding additional\n",
      "pages (“Supplementary Material”).\n",
      "\n",
      "Line 620: “In this work term frequency was …” → “In this work, term\n",
      "frequency was …” Also, why not tf-idf, as it seems to be the most common\n",
      "weighting scheme? \n",
      "\n",
      "The sentence on lines 641-645 needs to be rewritten.\n",
      "\n",
      "Line 662: What do you mean by “the threshold parameter”? The threshold for\n",
      "the word embedding cosine distance?\n",
      "\n",
      "Line 735 is missing a period.\n",
      "\n",
      "Section 4.3 Classification Algorithms: Details on exactly what scheme of\n",
      "multi-view learning was used are entirely omitted. Statistical significance of\n",
      "result differences is not provided.\n",
      "- Strengths:\n",
      "Nicely written and understandable.\n",
      "Clearly organized. Targeted answering of research questions, based on \n",
      "different experiments.\n",
      "\n",
      "- Weaknesses:\n",
      "Minimal novelty. The \"first sentence\" heuristic has been in the summarization\n",
      "literature for many years. This work essentially applies this heuristic\n",
      "(evolved) in the keyword extraction setting. This is NOT to say that the work\n",
      "is trivial: it is just not really novel.\n",
      "\n",
      "Lack of state-of-the-art/very recent methods. The experiment on the system\n",
      "evaluation vs state-of-the-art systems simply uses strong baselines. Even\n",
      "though the experiment answers the question \"does it perform better than\n",
      "baselines?\", I am not confident it illustrates that the system performs better\n",
      "than the current state-of-the-art. This somewhat reduces the value of the\n",
      "paper.\n",
      "\n",
      "- General Discussion:\n",
      "Overall the paper is good and I propose that it be published and presented. \n",
      "\n",
      "On the other hand, I would propose that the authors position themselves (and\n",
      "the system performance) with respect to:\n",
      "Martinez‐Romo, Juan, Lourdes Araujo, and Andres Duque Fernandez. \"SemGraph:\n",
      "Extracting keyphrases following a novel semantic graph‐based approach.\"\n",
      "Journal of the Association for Information Science and Technology 67.1 (2016):\n",
      "71-82.\n",
      "(with which the work holds remarkable resemblance in some points)\n",
      "\n",
      "Le, Tho Thi Ngoc, Minh Le Nguyen, and Akira Shimazu. \"Unsupervised Keyphrase\n",
      "Extraction: Introducing New Kinds of Words to Keyphrases.\" Australasian Joint\n",
      "Conference on Artificial Intelligence. Springer International Publishing, 2016.\n",
      "- Strengths:\n",
      "This paper proposes an evaluation metric for automatically evaluating the\n",
      "quality of dialogue responses in non-task-oriented dialogue. The metric\n",
      "operates on continuous vector space representations obtained by using RNNs and\n",
      "it comprises two components: one that compares the context and the given\n",
      "response and the other that compares a reference response and the given\n",
      "response. The comparisons are conducted by means of dot product after\n",
      "projecting the response into corresponding context and reference response\n",
      "spaces. These projection matrices are learned by minimizing the squared error\n",
      "between the model predictions and human annotations.\n",
      "\n",
      "I think this work gives a remarkable step forward towards the evaluation of\n",
      "non-task-oriented dialogue systems. Different from previous works in this area,\n",
      "where pure semantic similarity was pursued, the authors are going beyond pure\n",
      "semantic similarity in a very elegant manner by learning projection matrices\n",
      "that transform the response vector into both context and reference space\n",
      "representations. I am very curious on how your projection matrices M and N\n",
      "differ from the original identity initialization after training the models. I\n",
      "think the paper will be more valuable if further discussion on this is\n",
      "introduced, rather than focusing so much on resulting correlations. \n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "The paper also leaves lots questions related to the implementation. For\n",
      "instance, it is not clear whether the human scores used to train and evaluate\n",
      "the system were single AMT annotations or the resulting average of few\n",
      "annotations. Also, it is not clear how the dataset was split into\n",
      "train/dev/test and whether n-fold cross validation was conducted or not. Also,\n",
      "it would be nice to better explain why in table 2 correlation for ADEM related\n",
      "scores are presented for the validation and test sets, while for the other\n",
      "scores they are presented for the full dataset and test set. The section on\n",
      "pre-training with VHRED is also very clumsy and confusing, probably it is\n",
      "better to give less technical details but a better high level explanation of\n",
      "the pre-training strategy and its advantages.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "“There are many obvious cases where these metrics fail, as they are often\n",
      "incapable of considering the semantic similarity between responses (see Figure\n",
      "1).” Be careful with statements like this one. This is not a problem of\n",
      "semantic similarity! Opposite to it, the problem is that completely different\n",
      "semantic cues might constitute pragmatically valid responses. Then, semantic\n",
      "similarity itself is not enough to evaluate a dialogue system response.\n",
      "Dialogue system response evaluation must go beyond semantics (This is actually\n",
      "what your M and N matrices are helping to do!!!) \n",
      "\n",
      "“an accurate model that can evaluate dialogue response quality automatically\n",
      "— what could be considered an automatic Turing test —“ The original\n",
      "intention of Turing test was to be a proxy to identify/define intelligent\n",
      "behaviour. It actually proposes a test on intelligence based on an\n",
      "“intelligent” machine capability to imitate human behaviour in such a way\n",
      "that it would be difficult for a common human to distinguish between such a\n",
      "machine responses and actual human responses. It is of course related to\n",
      "dialogue system performance, but I think it is not correct to say that\n",
      "automatically evaluating dialogue response quality is an automatic Turing test.\n",
      "Actually, the title itself “Towards an Automatic Turing Test” is somehow\n",
      "misleading!\n",
      "\n",
      "“the simplifying assumption that a ‘good’ chatbot is one whose responses\n",
      "are scored highly on appropriateness by human evaluators.” This is certainly\n",
      "the correct angle to introduce the problem of non-task-oriented dialogue\n",
      "systems, rather than “Turing Test”. Regarding this, there has been related\n",
      "work you might like to take a look at, as well as to make reference to, in the\n",
      "WOCHAT workshop series (see the shared task description and corresponding\n",
      "annotation guidelines).\n",
      "\n",
      "In the discussion session: “and has has been used” -> “and it has been\n",
      "used”\n",
      "- General Discussion:\n",
      "\n",
      "This paper extends Zhou and Xu's ACL 2015 approach to semantic role labeling\n",
      "based on deep BiLSTMs. In addition to applying recent best practice techniques,\n",
      "leading to further quantitative improvements, the authors provide an insightful\n",
      "qualitative analysis of their results. The paper is well written and has a\n",
      "clear structure. The authors provide a comprehensive overview of related work\n",
      "and compare results to a representative set of other SRL models that hace been\n",
      "applied on the same data sets.\n",
      "\n",
      "I found the paper to be interesting and convincing. It is a welcome research\n",
      "contribution that not only shows that NNs work well, but also analyzes merits\n",
      "and shortcomings of an end-to-end learning approach.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "Strong model, insightful discussion/error analysis.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "Little to no insights regarding the SRL task itself.\n",
      "This paper presents a new state-of-the-art deep learning model for semantic\n",
      "role labeling (SRL) that is a natural extension of the previous\n",
      "state-of-the-art system (Zhou and Xu, 2015) with recent best practices for\n",
      "initialization and regularization in the deep learning literature.\n",
      "The model gives a 10% relative error reduction which is a big gain on this\n",
      "task. The paper also gives in-depth empirical analyses to reveal the strengths\n",
      "and the remaining issues, that give a quite valuable information to the\n",
      "researchers in this field. \n",
      "\n",
      "Even though I understand that the improvement of 3 point in F1 measure is a\n",
      "quite meaningful result from the engineering point of view, I think the main\n",
      "contribution of the paper is on the extensive analysis in the experiment\n",
      "section and a further in-depth investigation on analysis section. The detailed\n",
      "analyses shown in Section 4 are performed in a quite reasonable way and give\n",
      "both comparable results in SRL literature and novel information such as\n",
      "relation between accuracies in syntactic parsing and SRL. This type of analysis\n",
      "had often been omitted in recent papers. However, it is definitely important\n",
      "for further improvement.\n",
      "\n",
      "The paper is well-written and well-structured. \n",
      "I really enjoyed the paper and would like to see it accepted.\n",
      "- Strengths:\n",
      "\n",
      "This paper presents a step in the direction of developing more challenging\n",
      "corpora for training sentence planners in data-to-text NLG, which is an\n",
      "important and timely direction. \n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "It is unclear whether the work reported in this paper represents a substantial\n",
      "advance over Perez-Beltrachini et al.'s (2016) method for selecting content. \n",
      "The authors do not directly compare the present paper to that one. It appears\n",
      "that the main novelty of this paper is the additional analysis, which is\n",
      "however rather superficial.\n",
      "\n",
      "It is good that the authors report a comparison of how an NNLG baseline fares\n",
      "on this corpus in comparison to that of Wen et al. (2016).  However, the\n",
      "BLEU scores in Wen et al.'s paper appear to be much much higher, suggesting\n",
      "that this NNLG baseline is not sufficient for an informative comparison.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The authors need to more clearly articulate why this paper should count as a\n",
      "substantial advance over what has been published already by Perez-Beltrachini\n",
      "et al, and why the NNLG baseline should be taken seriously.  In contrast to\n",
      "LREC, it is not so common for ACL to publish a main session paper on a corpus\n",
      "development methodology in the absence of some new results of a system making\n",
      "use of the corpus.\n",
      "\n",
      "The paper would also be stronger if it included an analysis of the syntactic\n",
      "constructions in the two corpora, thereby more directly bolstering the case\n",
      "that the new corpus is more complex.  The exact details of how the number of\n",
      "different path shapes are determined should also be included, and ideally\n",
      "associated with the syntactic constructions.\n",
      "\n",
      "Finally, the authors should note the limitation that their method does nothing\n",
      "to include richer discourse relations such as Contrast, Consequence,\n",
      "Background, etc., which have long been central to NLG. In this respect, the\n",
      "corpora described by Walker et al. JAIR-2007 and Isard LREC-2016 are more\n",
      "interesting and should be discussed in comparison to the method here.\n",
      "\n",
      "References\n",
      "\n",
      "Marilyn Walker, Amanda Stent, François Mairesse, and\n",
      "Rashmi Prasad. 2007. Individual and domain adaptation\n",
      "in sentence planning for dialogue. Journal of\n",
      "Artificial Intelligence Research (JAIR), 30:413–456.\n",
      "\n",
      "Amy Isard, 2016. “The Methodius Corpus of Rhetorical Discourse\n",
      "Structures and Generated Texts” , Proceedings of the Tenth Conference\n",
      "on Language Resources and Evaluation (LREC 2016), Portorož, Slovenia,\n",
      "May 2016.\n",
      "\n",
      "---\n",
      "Addendum following author response:\n",
      "\n",
      "Thank you for the informative response.  As the response offers crucial\n",
      "clarifications, I have raised my overall rating.  Re the comparison to\n",
      "Perez-Beltrachini et al.: While this is perhaps more important to the PC than\n",
      "to the eventual readers of the paper, it still seems to this reviewer that the\n",
      "advance over this paper could've been made much clearer.  While it is true that\n",
      "Perez-Beltrachini et al. \"just\" cover content selection, this is the key to how\n",
      "this dataset differs from that of Wen et al.  There doesn't really seem to be\n",
      "much to the \"complete methodology\" of constructing the data-to-text dataset\n",
      "beyond obvious crowd-sourcing steps; to the extent these steps are innovative\n",
      "or especially crucial, this should be highlighted.  Here it is interesting that\n",
      "8.7% of the crowd-sourced texts were rejected during the verification step;\n",
      "related to Reviewer 1's concerns, it would be interesting to see some examples\n",
      "of what was rejected, and to what extent this indicates higher-quality texts\n",
      "than those in Wen et al.'s dataset.  Beyond that, the main point is really that\n",
      "collecting the crowd-sourced texts makes it possible to make the comparisons\n",
      "with the Wen et al. corpus at both the data and text levels (which this\n",
      "reviewer can see is crucial to the whole picture).\n",
      "\n",
      "Re the NNLG baseline, the issue is that the relative difference between the\n",
      "performance of this baseline on the two corpora could disappear if Wen et al.'s\n",
      "substantially higher-scoring method were employed.  The assumption that this\n",
      "relative difference would remain even with fancier methods should be made\n",
      "explicit, e.g. by acknowledging the issue in a footnote.  Even with this\n",
      "limitation, the comparison does still strike this reviewer as a useful\n",
      "component of the overall comparison between the datasets.\n",
      "\n",
      "Re whether a paper about dataset creation should be able to get into ACL\n",
      "without system results:  though this indeed not unprecedented, the key issue is\n",
      "perhaps how novel and important the dataset is likely to be, and here this\n",
      "reviewer acknowledges the importance of the dataset in comparison to existing\n",
      "ones (even if the key advance is in the already published content selection\n",
      "work).\n",
      "\n",
      "Finally, this reviewer concurs with Reviewer 1 about the need to clarify the\n",
      "role of domain dependence and what it means to be \"wide coverage\" in the final\n",
      "version of the paper, if accepted.\n",
      "- Strengths:\n",
      "* Potentially valuable resource\n",
      "* Paper makes some good points\n",
      "\n",
      "- Weaknesses:\n",
      "* Awareness of related work (see below)\n",
      "* Is what the authors are trying to do (domain-independent microplanning) even\n",
      "possible (see below)\n",
      "* Are the crowdsourced texts appropriate (see below)\n",
      "\n",
      "- General Discussion:\n",
      "This is an interesting paper which presents a potentially valuable resource,\n",
      "and I in many ways I am sympathetic to it.  However, I have some high-level\n",
      "concerns, which are not addressed in the paper.  Perhaps the authors can\n",
      "address these in their response.\n",
      "\n",
      "(1) I was a bit surprised by the constant reference and comparison to Wen 2016,\n",
      "which is a fairly obscure paper I have not previously heard of.  It would be\n",
      "better if the authors justified their work by comparison to well-known corpora,\n",
      "such as the ones they list in Section 2. Also, there are many other NLG\n",
      "projects that looked at microplanning issue when verbalising DBPedia, indeed\n",
      "there was a workshop in 2016 with many papers on NLG and DBPedia\n",
      "(https://webnlg2016.sciencesconf.org/  and\n",
      "http://aclweb.org/anthology/W/W16/#3500); see also previous work by Duboue and\n",
      "Kutlak.  I would like to see less of a fixation on Wen (2016), and more\n",
      "awareness of other work on NLG and DBPedia.\n",
      "\n",
      "(2) Microplanning tends to be very domain/genre dependent.  For example,\n",
      "pronouns are used much more often in novels than in aircraft maintenance\n",
      "manuals.   This is why so much work has focused on domain-dependent resources. \n",
      "  So there are some real questions about whether it is possible even in theory\n",
      "to train a \"wide-coverage microplanner\".  The authors do not discuss this at\n",
      "all; they need to show they are aware of this concern.\n",
      "\n",
      "(3) I would be concerned about the quality of the texts obtained from\n",
      "crowdsourcing.              A lot of people dont write very well, so it is not at all\n",
      "clear\n",
      "to me that gathering example texts from random crowdsourcers is going to\n",
      "produce a good corpus for training microplanners.  Remember that the ultimate\n",
      "goal of microplanning is to produce texts that are easy to *read*.  Imitating\n",
      "human writers (which is what this paper does, along with most learning\n",
      "approaches to microplanning) makes sense if we are confident that the human\n",
      "writers have produced well-written easy-to-read texts.              Which is a\n",
      "reasonable\n",
      "assumption if the writers are professional journalists (for example), but a\n",
      "very dubious one if the writers are random crowdsourcers.\n",
      "\n",
      "From a presentational perspective, the authors should ensure that all text in\n",
      "their paper meets the ACL font size criteria.  Some of the text in Fig 1 and\n",
      "(especially) Fig 2 is tiny and very difficult to read; this text should be the\n",
      "same font size as the text in the body of the paper.\n",
      "\n",
      "I will initially rate this paper as borderline.  I look forward to seeing the\n",
      "author's response, and will adjust my rating accordingly.\n",
      "- Strengths:\n",
      "\n",
      "-- A well-motivated approach, with a clear description and solid results.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "-- Nothing substantial other than the comments below. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The paper describes a new method called attention-over-attention for reading\n",
      "comprehension. First layers of the network compute a vector for each query word\n",
      "and document word, resulting in a |Q|xK matrix for the query and a |D|xK for\n",
      "the document. Since the answer is a document word, an attention mechanism is\n",
      "used for assigning weights to each word, depending on their interaction with\n",
      "query words. In this work, the authors deepen a traditional attention mechanism\n",
      "by computing a weight for each query word through a separate attention and then\n",
      "using that to weight the main attention over document words. Evaluation is\n",
      "properly conducted on benchmark datasets, and various insights are presented\n",
      "through an analysis of the results as well as a comparison to prior work. I\n",
      "think this is a solid piece of work on an important problem, and the method is\n",
      "well-motivated and clearly described, so that researchers can easily reproduce\n",
      "results and apply the same techniques to other similar tasks.\n",
      "\n",
      "- Other remarks:\n",
      "\n",
      "-- p4, Equation 12: I am assuming i is iterating over training set and p(w) is\n",
      "referring to P(w|D,Q) in the previous equation? Please clarify to avoid\n",
      "confusion.\n",
      "\n",
      "-- I am wondering whether you explored/discussed initializing word embeddings\n",
      "with existing vectors such as Google News or Glove? Is there a reason to\n",
      "believe the general-purpose word semantics would not be useful in this task?\n",
      "\n",
      "-- p6 L589-592: It is not clear what the authors are referring to when they say\n",
      "'letting the model explicitly learn weights between individual attentions'? Is\n",
      "this referring to their own architecture, more specifically the GRU output\n",
      "indirectly affecting how much attention will be applied to each query and\n",
      "document word? Clarifying that would be useful. Also, I think the improvement\n",
      "on validation is not 4.1, rather 4.0 (72.2-68.2).\n",
      "\n",
      "-- p7 Table 5: why do you think the weight for local LM is relatively higher\n",
      "for the CN task while the benefit of adding it is less? Since you included the\n",
      "table, I think it'll be nice to provide some insights to the reader.\n",
      "\n",
      "-- I would have liked to see the software released as part of this submission.\n",
      "\n",
      "-- Typo p2 L162 right column: \"is not that effective than expected\" --> \"is not\n",
      "as effective as expected\"?\n",
      "\n",
      "-- Typo p7 L689 right column: \"appear much frequent\" --> \"appears more\n",
      "frequently\"?\n",
      "\n",
      "-- Typo p8 L719-721 left column: \"the model is hard to\" --> \"it is hard for the\n",
      "model to\"? & \"hard to made\" --> \"hard to make\"?\n",
      "- Strengths: The paper addresses a relevant topic: learning the mapping between\n",
      "natural language and KB relations, in the context of QA (where we have only\n",
      "partial information for one of the arguments), and in the case of having a very\n",
      "large number of possible target relations.\n",
      "\n",
      "The proposal consists in a new method to combine two different representations\n",
      "of the input text: a word level representation (i.e. with segmentation of the\n",
      "target relation names and also the input text), and relations as a single token\n",
      "(i.e without segmentation of relation names nor input text). \n",
      "\n",
      "It seems, that the main contribution in QA is the ability to re-rank entities\n",
      "after the Entity Linking step.\n",
      "\n",
      "Results show an improvement compared with the state of the art. \n",
      "\n",
      "- Weaknesses:\n",
      "The approach has been evaluated in a limited dataset. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "I think, section 3.1 fits better inside related work, so the 3.2 can become\n",
      "section 3 with the proposal. Thus, new section 3 can be splitted more properly.\n",
      "This paper considers the problem of KB completion and proposes ITransF for this\n",
      "purpose. Unlike STransE that assigns each relation an independent matrix, this\n",
      "paper proposes to share the parameters between different relations. A model is\n",
      "proposed where a tensor D is constructed that contains various relational\n",
      "matrices as its slices and a selectional vector \\alpha is used to select a\n",
      "subset of relevant relational matrix for composing a particular semantic\n",
      "relation. The paper then discuss a technique to make \\alpha sparse.\n",
      "Experimental results on two standard benchmark datasets shows the superiority\n",
      "of ITransF over prior proposals.\n",
      "\n",
      "The paper is overall well written and the experimental results are good.\n",
      "However, I have several concerns regarding this work that I hope the authors\n",
      "will answer in their response.\n",
      "\n",
      "1. Just by arranging relational matrices in a tensor and selecting (or more\n",
      "appropriately considering a linearly weighted sum of the relational matrices)\n",
      "does not ensure any information sharing between different relational matrices.\n",
      "This would have been the case if you had performed some of a tensor\n",
      "decomposition and projected the different slices (relational matrices) into\n",
      "some common lower-dimensional core tensor. It is not clear why this approach\n",
      "was not taken despite the motivation to share information between different\n",
      "relational matrices.\n",
      "2. The two requirements (a) to share information across different relational\n",
      "matrices and (b) make the attention vectors sparse are some what contradictory.\n",
      "If the attention vector is truly sparse and has many zeros then information\n",
      "will not flow into those slices during optimisation. \n",
      "3. The authors spend a lot of space discussing techniques for computing sparse\n",
      "attention vectors. The authors mention in page 3 that \\ell_1 regularisation did\n",
      "not work in their preliminary experiments. However, no experimental results are\n",
      "shown for \\ell_1 regularisation nor they explain why \\ell_1 is not suitable for\n",
      "this task. To this reviewer, it appears as an obvious baseline to try,\n",
      "especially given the ease of optimisation. You use \\ell_0 instead and get into\n",
      "NP hard optimisations because of it. Then you propose a technique and a rather\n",
      "crude approximation in the end to solve it. All that trouble could be spared if\n",
      "\\ell_1 was used.\n",
      "4. The vector \\alpha is performing a selection or a weighing over the slices of\n",
      "D. It is slightly misleading to call this as “attention” as it is a term\n",
      "used in NLP for a more different type of models (see attention model used in\n",
      "machine translation).\n",
      "5. It is not clear why you need to initialise the optimisation by pre-trained\n",
      "embeddings from TransE. Why cannot you simply randomly initialise the\n",
      "embeddings as done in TransE and then update them? It is not fair to compare\n",
      "against TransE if you use TransE as your initial point.\n",
      "\n",
      "Learning the association between semantic relations is an idea that has been\n",
      "used in related problems in NLP such as relational similarity measurement\n",
      "[Turney JAIR 2012] and relation adaptation [Bollegala et al. IJCAI 2011]. It\n",
      "would be good to put the current work with respect to such prior proposals in\n",
      "NLP for modelling inter-relational correlation/similarity.\n",
      "\n",
      "Thanks for providing feedback. I have read it.\n",
      "- Strengths: 1) an interesting task, 2) the paper is very clearly written, easy\n",
      "to follow, 3) the created data set may be\n",
      "useful for other researchers, 4) a detailed analysis of the performance of the\n",
      "model.\n",
      "\n",
      "- Weaknesses: 1) no method adapted from related work for a result comparison 2)\n",
      "some explanations about the uniqueness of the task and discussion on\n",
      "limitations of previous research for solving this problem can be added to\n",
      "emphasize the research contributions further. \n",
      "\n",
      "- General Discussion: The paper presents supervised and weakly supervised\n",
      "models for frame classification in tweets. Predicate rules are generated\n",
      "exploiting language-based and Twitter behavior-based signals, which are then\n",
      "supplied to the probabilistic soft logic framework to build classification\n",
      "models. 17 political frames are classified in tweets in a multi-label\n",
      "classification task. The experimental results demonstrate the benefit of the\n",
      "predicates created using the behavior-based signals. Please find my more\n",
      "specific comments below:\n",
      "\n",
      "The paper should have a discussion on how frame classification differs from\n",
      "stance classification. Are they both under the same umbrella but with different\n",
      "levels of granularity?\n",
      "\n",
      "The paper will benefit from adding a brief discussion on how exactly the\n",
      "transition from long congressional speech to short tweets adds to the\n",
      "challenges of the task. For example, does past research rely on any specific\n",
      "cross-sentential features that do not apply to tweets? Consider adapting the\n",
      "method of a frame classification work on\n",
      "congressional speech (or a stance classification work on any text) to the\n",
      "extent possible due to its limitations on Twitter data, to compare with the\n",
      "results of this work.\n",
      "\n",
      "It seems “weakly supervised” and “unsupervised” – these two terms\n",
      "have been interchangeably used in the paper (if this is not the case, please\n",
      "clarify in author response). I believe \"weakly supervised\" is\n",
      "the\n",
      "more technically correct terminology under the setup of this work that should\n",
      "be used consistently throughout. The initial unlabeled data may not have been\n",
      "labeled by human annotators, but the classification does use weak or noisy\n",
      "labels of some sort, and the keywords do come from experts. The presented\n",
      "method does not use completely unsupervised data as traditional unsupervised\n",
      "methods such as clustering, topic models or word embeddings would.  \n",
      "\n",
      "The calculated Kappa may not be a straightforward reflection of the difficulty\n",
      "of\n",
      "frame classification for tweets (lines: 252-253), viewing it as a proof is a\n",
      "rather strong claim. The Kappa here merely represents the\n",
      "annotation difficulty/disagreement. Many factors can contribute to a low value \n",
      "such as poorly written annotation\n",
      "guidelines, selection of a biased annotator, lack of annotator training etc.\n",
      "(on\n",
      "top of any difficulty of frame classification for tweets by human annotators,\n",
      "which the authors actually intend to relate to).\n",
      "73.4% Cohen’s Kappa is strong enough for this task, in my opinion, to rely on\n",
      "the annotated labels. \n",
      "\n",
      "Eq (1) (lines: 375-377) will ignore any contextual information (such as\n",
      "negation\n",
      "or conditional/hypothetical statements impacting the contributing word) when\n",
      "calculating similarity of a frame and a tweet. Will this have any effect on the\n",
      "frame prediction model? Did the authors consider using models that can\n",
      "determine similarity with larger text units such as perhaps using skip thought\n",
      "vectors or vector compositionality methods?  \n",
      "\n",
      "An ideal set up would exclude the annotated data from calculating statistics\n",
      "used to select the top N bi/tri-grams (line: 397 mentions entire tweets data\n",
      "set has been used), otherwise statistics from any test fold (or labeled data in\n",
      "the weakly supervised setup) still leaks into\n",
      "the selection process. I do not think this would have made any difference in\n",
      "the current selection of the bi/tri-grams or results as the size of the\n",
      "unlabeled data is much larger, but would have constituted a cleaner\n",
      "experimental setup.  \n",
      "\n",
      "Please add precision and recall results in Table 4. \n",
      "\n",
      "Minor:\n",
      "please double check any rules for footnote placements concerning placement\n",
      "before or after the punctuation.\n",
      "- Strengths: The authors address a very challenging, nuanced problem in\n",
      "political discourse reporting a relatively high degree of success.\n",
      "\n",
      "The task of political framing detection may be of interest to the ACL\n",
      "community.\n",
      "\n",
      "The paper is very well written.\n",
      "\n",
      "- Weaknesses: Quantitative results are given only for the author's PSL model\n",
      "and not compared against any traditional baseline classification algorithms,\n",
      "making it unclear to what degree their model is necessary. Poor comparison with\n",
      "alternative approaches makes it difficult to know what to take away from the\n",
      "paper.\n",
      "\n",
      "The qualitative investigation is interesting, but the chosen visualizations are\n",
      "difficult to make sense of and add little to the discussion. Perhaps it would\n",
      "make sense to collapse across individual politicians to create a clearer\n",
      "visual.\n",
      "\n",
      "- General Discussion: The submission is well written and covers a topic which\n",
      "may be of interest to the ACL community. At the same time, it lacks proper\n",
      "quantitative baselines for comparison. \n",
      "\n",
      "Minor comments:\n",
      "\n",
      "- line 82: A year should be provided for the Boydstun et al. citation\n",
      "\n",
      "- It’s unclear to me why similar behavior (time of tweeting) should\n",
      "necessarily be indicative of similar framing and no citation was given to\n",
      "support this assumption in the model.\n",
      "\n",
      "- The related work goes over quite a number of areas, but glosses over the work\n",
      "most clearly related (e.g. PSL models and political discourse work) while\n",
      "spending too much time mentioning work that is only tangential (e.g.\n",
      "unsupervised models using Twitter data).\n",
      "\n",
      "- Section 4.2 it is unclear whether Word2Vec was trained on their dataset or if\n",
      "they used pre-trained embeddings.\n",
      "\n",
      "- The authors give no intuition behind why unigrams are used to predict frames,\n",
      "while bigrams/trigrams are used to predict party.\n",
      "\n",
      "- The authors note that temporal similarity worked best with one hour chunks,\n",
      "but make no mention of how important this assumption is to their results. If\n",
      "the authors are unable to provide full results for this work, it would still be\n",
      "worthwhile to give the reader a sense of what performance would look like if\n",
      "the time window were widened.\n",
      "\n",
      "- Table 4: Caption should make it clear these are F1 scores as well as\n",
      "clarifying how the F1 score is weighted (e.g. micro/macro). This should also be\n",
      "made clear in the “evaluation metrics” section on page 6.\n",
      "The paper presents a neural model for predicting SQL queries directly from\n",
      "natural language utterances, without going through an intermediate formalism.\n",
      "In addition, an interactive online feedback loop is proposed and tested on a\n",
      "small scale.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "1\\ The paper is very clearly written, properly positioned, and I enjoyed\n",
      "reading it.\n",
      "\n",
      "2\\ The proposed model is tested and shown to perform well on 3 different\n",
      "domains (academic, geographic queries, and flight booking)\n",
      "\n",
      "3\\ The online feedback loop is interesting and seems promising, despite of the\n",
      "small scale of the experiment.\n",
      "\n",
      "4\\ A new semantic corpus is published as part of this work, and additionally\n",
      "two\n",
      "existing corpora are converted to SQL format, which I believe would be\n",
      "beneficial for future work in this area.\n",
      "\n",
      "- Weaknesses / clarifications:\n",
      "\n",
      "1\\ Section 4.2 (Entity anonymization) - I am not sure I understand the choice\n",
      "of the length of span for querying the search engine. Why and how is it\n",
      "progressively reduced? (line 333).\n",
      "\n",
      "2\\ Section 5 (Benchmark experiments) - If I understand correctly, the feedback\n",
      "loop (algorithm 1) is *not* used for these experiments. If this is indeed the\n",
      "case, I'm not sure when does data augmentation occur. Is all the annotated\n",
      "training data augmented with paraphrases? When is the \"initial data\" from\n",
      "templates added? Is it also added to the gold training set? If so, I think it's\n",
      "not surprising that it doesn't help much, as the gold queries may be more\n",
      "diverse.  In any case, I think this should be stated more clearly. In addition,\n",
      "I think it's interesting to see what's the performance of the \"vanilla\" model,\n",
      "without any augmentation, I think that this is not reported in the paper.\n",
      "\n",
      "3\\ Tables 2 and 3: I find the evaluation metric used here somewhat unclear. \n",
      "Does the accuracy measure the correctness of the execution of the query (i.e.,\n",
      "the retrieved answer) as the text seem to indicate? (Line 471 mentions\n",
      "*executing* the query). Alternatively, are the queries themselves compared? (as\n",
      "seems to be the case for Dong and Lapata in Table 2). If this is done\n",
      "differently for different systems (I.e., Dong and Lapata), how are these\n",
      "numbers comparable? In addition, the text mentions the SQL model has \"slightly\n",
      "lower accuracy than the best non-SQL results\" (Line 515), yet in table 2 the\n",
      "difference is almost 9 points in accuracy.  What is the observation based upon?\n",
      "Was some significance test performed? If not, I think the results are still\n",
      "impressive for direct to SQL parsing, but that the wording should be changed,\n",
      "as the difference in performance does seem significant.\n",
      "\n",
      "4\\ Line 519 - Regarding the data recombination technique used in Jia and Liang\n",
      "(2016): Since this technique is applicable in this scenario, why not try it as\n",
      "well?  Currently it's an open question whether this will actually improve\n",
      "performance. Is this left as future work, or is there something prohibiting the\n",
      "use of this technique?\n",
      "\n",
      "5\\ Section 6.2 (Three-stage online experiment) - several details are missing /\n",
      "unclear:\n",
      "\n",
      "* What was the technical background of the recruited users?\n",
      "\n",
      "* Who were the crowd workers, how were they recruited and trained?\n",
      "\n",
      "* The text says \"we recruited 10 new users and asked them to issue at least 10\n",
      "utterances\". Does this mean 10 queries *each* (e.g., 100 overall), or 10 in\n",
      "total (1 for each).\n",
      "\n",
      "* What was the size of the initial (synthesized) training  set? \n",
      "\n",
      "* Report statistics of the queries - some measure of their lexical variability\n",
      "/ length / complexity of the generated SQL? This seems especially important for\n",
      "the first phase, which is doing surprisingly well. Furthermore, since SCHOLAR\n",
      "uses SQL and NL, it would have been nice if it were attached to this\n",
      "submission, to allow its review during this period.\n",
      "\n",
      "6\\ Section 6.3 (SCHOLAR dataset)\n",
      "\n",
      "* The dataset seems pretty small in modern standards (816 utterances in total),\n",
      "while one of the main advantages of this process is its scalability. What\n",
      "hindered the creation of a much larger dataset?\n",
      "\n",
      "* Comparing performance - is it possible to run another baseline on this newly\n",
      "created dataset to compare against the reported 67% accuracy obtained in this\n",
      "paper (line 730).\n",
      "\n",
      "7\\ Evaluation of interactive learning experiments (Section 6): I find the\n",
      "experiments to be somewhat hard to replicate as they involve manual queries of\n",
      "specific annotators. For example, who's to say if the annotators in the last\n",
      "phase just asked simpler questions? I realise that this is always problematic\n",
      "for online learning scenarios, but I think that an effort should be made\n",
      "towards an objective comparison. For starters, the statistics of the queries\n",
      "(as I mentioned earlier) is a readily available means to assess whether this\n",
      "happens. Second, maybe there can be some objective held out test set? This is\n",
      "problematic as the model relies on the seen queries, but scaling up the\n",
      "experiment (as I suggested above) might mitigate this risk. Third, is it\n",
      "possible to assess a different baseline using this online technique? I'm not\n",
      "sure whether this is applicable given that previous methods were not devised as\n",
      "online learning methods.\n",
      "\n",
      "- Minor comments:\n",
      "\n",
      "1\\ Line 48 - \"requires\" -> \"require\"\n",
      "\n",
      "2\\ Footnote 1 seems too long to me. Consider moving some of its content to the\n",
      "body of the text.\n",
      "\n",
      "3\\ Algorithm 1: I'm not sure what \"new utterances\" refers to (I guess it's new\n",
      "queries from users?). I think that an accompanying caption to the algorithm\n",
      "would make the reading easier.\n",
      "\n",
      "4\\ Line 218 - \"Is is\" -> \"It is\"\n",
      "\n",
      "5\\ Line 278 mentions an \"anonymized\" utterance. This confused me at the first\n",
      "reading, and if I understand correctly it refers to the anonymization described\n",
      "later in 4.2. I think it would be better to forward reference this. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "Overall, I like the paper, and given answers to the questions I raised above,\n",
      "would like to see it appear in the conference.\n",
      "\n",
      "- Author Response:\n",
      "\n",
      "I appreciate the detailed response made by the authors, please include these\n",
      "details in a final version of the paper.\n",
      "This paper proposes a simple attention-based RNN model for generating SQL\n",
      "queries from natural language without any intermediate representation. Towards\n",
      "this end they employ a data augmentation approach where more data is\n",
      "iteratively collected from crowd annotation, based on user feedback on how well\n",
      "the SQL queries produced by the model do. Results on both the benchmark and\n",
      "interactive datasets show that data augmentation is a promising approach.\n",
      "\n",
      "Strengths:\n",
      "\n",
      "- No intermediate representations were used. \n",
      "\n",
      "- Release of a potentially valuable dataset on Google SCHOLAR.\n",
      "\n",
      "Weaknesses:\n",
      "\n",
      "- Claims of being comparable to state of the art when the results on GeoQuery\n",
      "and\n",
      "ATIS do not support it. \n",
      "\n",
      "General Discussion:\n",
      "\n",
      "This is a sound work of research and could have future potential in the way\n",
      "semantic parsing for downstream applications is done. I was a little\n",
      "disappointed with the claims of “near-state-of-the-art accuracies” on ATIS\n",
      "and GeoQuery, which doesn’t seem to be the case (8 points difference from\n",
      "Liang et. al., 2011)). And I do not necessarily think that getting SOTA numbers\n",
      "should be the focus of the paper, it has its own significant contribution. I\n",
      "would like to see this paper at ACL provided the authors tone down their\n",
      "claims, in addition I have some questions for the authors.\n",
      "\n",
      "- What do the authors mean by minimal intervention? Does it mean minimal human\n",
      "intervention, because that does not seem to be the case. Does it mean no\n",
      "intermediate representation? If so, the latter term should be used, being less\n",
      "ambiguous.\n",
      "\n",
      "- Table 6: what is the breakdown of the score by correctness and\n",
      "incompleteness?\n",
      "What % of incompleteness do these queries exhibit?\n",
      "\n",
      "- What is expertise required from crowd-workers who produce the correct SQL\n",
      "queries? \n",
      "\n",
      "- It would be helpful to see some analysis of the 48% of user questions which\n",
      "could not be generated.\n",
      "\n",
      "- Figure 3 is a little confusing, I could not follow the sharp dips in\n",
      "performance without paraphrasing around the 8th/9th stages. \n",
      "\n",
      "- Table 4 needs a little more clarification, what splits are used for obtaining\n",
      "the ATIS numbers?\n",
      "\n",
      "I thank the authors for their response.\n",
      "This paper proposes an approach to learning a semantic parser using an\n",
      "encoder-decoder neural architecture, with the distinguishing feature that the\n",
      "semantic output is full SQL queries. The method is evaluated over two standard\n",
      "datasets (Geo880 and ATIS), as well as a novel dataset relating to document\n",
      "search.\n",
      "\n",
      "This is a solid, well executed paper, which takes a relatively well\n",
      "established technique in the form of an encoder-decoder with some trimmings\n",
      "(e.g. data augmentation through paraphrasing), and uses it to generate SQL\n",
      "queries, with the purported advantage that SQL queries are more expressive\n",
      "than other semantic formalisms commonly used in the literature, and can be\n",
      "edited by untrained crowd workers (familiar with SQL but not semantic\n",
      "parsing). I buy that SQL is more expressive than the standard semantic\n",
      "formalisms, but then again, were there really any queries in any of your three\n",
      "datasets where the standard formalisms are unable to capture the full\n",
      "semantics of the query? I.e. are they really the best datasets to showcase the\n",
      "expressivity of SQL? Also, in terms of what your model learns, what fraction\n",
      "of SQL does it actually use? I.e. how much of the extra expressivity in SQL is\n",
      "your model able to capture? Also, does it have biases in terms of the style of\n",
      "queries that it tends to generate? That is, I wanted to get a better sense of\n",
      "not just the *potential* of SQL, but the actuality of what your model is able\n",
      "to capture, and the need for extra expressivity relative to the datasets you\n",
      "experiment over. Somewhat related to this, at the start of Section 5, you\n",
      "assert that it's harder to directly produce SQL. You never actually show this,\n",
      "and this seems to be more a statement of the expressivity of SQL than anything\n",
      "else (which returns me to the question of how much of SQL is the model\n",
      "actually generating).\n",
      "\n",
      "Next, I would really have liked to have seen more discussion of the types of\n",
      "SQL queries your model generates, esp. for the second part of the evaluation,\n",
      "over the SCHOLAR dataset. Specifically, when the query is ill-formed, in what\n",
      "ways is it ill-formed? When a crowd worker is required to post-edit the query,\n",
      "how much effort does that take them? Equally, how correct are the crowd\n",
      "workers at constructing SQL queries? Are they always able to construct perfect\n",
      "queries (experience would suggest that this is a big ask)? In a similar vein\n",
      "to having more error analysis in the paper, I would have liked to have seen\n",
      "agreement numbers between annotators, esp. for Incomplete Result queries,\n",
      "which seems to rely heavily on pre-existing knowledge on the part of the\n",
      "annotator and therefore be highly subjective.\n",
      "\n",
      "Overall, what the paper achieves is impressive, and the paper is well\n",
      "executed; I just wanted to get more insights into the true ability of the\n",
      "model to generate SQL, and a better sense of what subset of the language it\n",
      "generates.\n",
      "\n",
      "A couple of other minor things:\n",
      "\n",
      "l107: \"non-linguists can write SQL\" -- why refer to \"non-linguists\" here? Most\n",
      "linguists wouldn't be able to write SQL queries either way; I think the point\n",
      "you are trying to make is simply that \"annotators without specific training in\n",
      "the semantic translation of queries\" are able to perform the task\n",
      "\n",
      "l218: \"Is is\" -> \"It is\"\n",
      "\n",
      "l278: it's not clear what an \"anonymized utterance\" is at this point of the\n",
      "paper\n",
      "\n",
      "l403: am I right in saying that you paraphrase only single words at a time?\n",
      "Presumably you exclude \"entities\" from paraphrasing?\n",
      "\n",
      "l700: introduce a visual variable in terms of line type to differentiate the\n",
      "three lines, for those viewing in grayscale\n",
      "\n",
      "There are various inconsistencies in the references, casing issues\n",
      "(e.g. \"freebase\", \"ccg\"), Wang et al. (2016) is missing critical publication\n",
      "details, and there is an \"In In\" for Wong and Mooney (2007)\n",
      "The paper introduces a simple and effective method for morphological paradigm\n",
      "completion in low-resource settings. The method uses a character-based seq2seq\n",
      "model trained on a mix of examples in two languages: a resource-poor language\n",
      "and a closely-related resource-rich language; each training example is\n",
      "annotated with a paradigm properties and a language ID. Thus, the model enables\n",
      "transfer learning across languages when the two languages share common\n",
      "characters and common paradigms. While the proposed multi-lingual solution is\n",
      "not novel (similar architectures have been explored in syntax, language\n",
      "modeling, and MT), the novelty of this paper is to apply the approach to\n",
      "morphology. Experimental results show substantial improvements over monolingual\n",
      "baselines, and include a very thorough analysis of the impact of language\n",
      "similarities on the quality of results. The paper is interesting, very clearly\n",
      "written, I think it’ll be a nice contribution to the conference program. \n",
      "\n",
      "Detailed comments: \n",
      "\n",
      "— My main question is why the proposed general multilingual methodology was\n",
      "limited to pairs of languages, rather than to sets of similar languages? For\n",
      "example, all Romance languages could be included in the training to improve\n",
      "Spanish paradigm completion, and all Slavic languages with Cyrillic script\n",
      "could be mixed to improve Ukrainian. It would be interesting to see the\n",
      "extension of the models from bi-lingual to multilingual settings. \n",
      "\n",
      "— I think Arabic is not a fair (and fairly meaningless) baseline, given how\n",
      "different is its script and morphology from the target languages. A more\n",
      "interesting baseline would be, e.g., a language with a partially shared\n",
      "alphabet but a different typology. For example, a Slavic language with Latin\n",
      "script could be used as a baseline language for Romance languages. If Arabic is\n",
      "excluded, and if we consider a most distant language in the same the same\n",
      "family as a baseline, experimental results are still strong. \n",
      "\n",
      "— A half-page discussion of contribution of Arabic as a regularizer also adds\n",
      "little to the paper; I’d just remove Arabic from all the experiments and\n",
      "would add a regularizer (which, according to footnote 5, works even better than\n",
      "adding Arabic as a transfer language).              \n",
      "\n",
      "— Related work is missing a line of work on “language-universal” RNN\n",
      "models that use basically the same approach: they learn shared parameters for\n",
      "inputs in multiple languages, and add a language tag to the input to mediate\n",
      "between languages. Related studies include a multilingual parser (Ammar et al.,\n",
      "2016), language models (Tsvetkov et al., 2016), and machine translation\n",
      "(Johnson et al., 2016 )\n",
      "\n",
      "Minor: \n",
      "— I don’t think that the claim is correct in line 144 that POS tags are\n",
      "easy to transfer across languages. Transfer of POS annotations is also a\n",
      "challenging task.  \n",
      "\n",
      "References: \n",
      "\n",
      "Waleed              Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah\n",
      "A.\n",
      "Smith. \"Many languages, one parser.” TACL 2016. \n",
      "\n",
      "Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick\n",
      "Littell, David Mortensen, Alan W. Black, Lori Levin, and Chris Dyer. \"Polyglot\n",
      "neural language models: A case study in cross-lingual phonetic representation\n",
      "learning.” NAACL 2016.\n",
      "\n",
      "Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng\n",
      "Chen, Nikhil Thorat et al. \"Google's Multilingual Neural Machine Translation\n",
      "System: Enabling Zero-Shot Translation.\" arXiv preprint arXiv:1611.04558 2016.\n",
      "\n",
      "-- Response to author response: \n",
      "\n",
      "Thanks for your response & I'm looking forward to reading the final version!\n",
      "- Strengths:\n",
      "The approach described in the manuscript outperformed the previous approaches\n",
      "and achieved the state-of-the-art result.\n",
      "\n",
      "Regarding data, the method used the combination of market and text data.\n",
      "\n",
      "The approach used word embeddings to define the weight of each lexicon term by\n",
      "extending it to the similar terms in the document.\n",
      "\n",
      "- Weaknesses:\n",
      "Deep-learning based methods were known to be able to achieve relatively good\n",
      "performances without much feature engineering in sentimental analysis. More\n",
      "literature search is needed to compare with the related works would be better.\n",
      "\n",
      "The approach generally improved performance by feature-based methods without\n",
      "much novelty in model or proposal of new features.\n",
      "\n",
      "- General Discussion:\n",
      "The manuscript described an approach in sentimental analysis. The method used a\n",
      "relatively new method of using word embeddings to define the weight of each\n",
      "lexicon term. However, the novelty is not significant enough.\n",
      "- Strengths:\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "This paper investigates sentiment signals in  companies’ annual 10-K filing\n",
      "reports to forecast volatility. \n",
      "\n",
      "The authors evaluate information retrieval term weighting models which are\n",
      "seeded with a finance-oriented sentiment lexicon and expanded with word\n",
      "embeddings. PCA is used to reduce dimensionality before Support Vector\n",
      "Regression is applied for similarity estimation.\n",
      "\n",
      "In addition to text-based features, the authors also use non-text-based market\n",
      "features (e.g. sector information and volatility estimates).\n",
      "\n",
      "Multiple fusion methods to combine text features with market features are\n",
      "evaluated.\n",
      "\n",
      "COMMENTS\n",
      "\n",
      "It would be interesting to include two more experimental conditions, namely 1)\n",
      "a simple trigram SVM which does not use any prior sentiment lexica, and 2)\n",
      "features that reflect delta-IDFs scores for individual features.\n",
      "As an additional baseline, it would be good to see binary features.\n",
      "\n",
      "This paper could corroborate your references:\n",
      "\n",
      "https://pdfs.semanticscholar.org/57d6/29615c19caa7ae6e0ef2163eebe3b272e65a.pdf\n",
      "- Strengths:\n",
      "This paper introduced a novel method to improve zero pronoun resolution\n",
      "performance.. The main contributions of this papers are: 1) proposed a simple\n",
      "method to automatically generate a large training set for zero pronoun\n",
      "resolution task; 2) adapted a two step learning process to transfer knowledge\n",
      "from large data set to the specific domain data; 3) differentiate unknown words\n",
      "using different tags. In general, the paper is well written. Experiments are\n",
      "thoroughly designed. \n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "But I have a few questions regarding finding the antecedent of a zero pronoun:\n",
      "1. How will an antecedent be identified, when the prediction is a pronoun? The\n",
      "authors proposed a method by matching the head of noun phrases. It’s not\n",
      "clear how to handle the situation when the head word is not a pronoun.\n",
      "2. What if the prediction is a noun that could not be found in the previous\n",
      "contents?\n",
      "3. The system achieves great results on standard data set. I’m curious is it\n",
      "possible to evaluate the system in two steps? The first step is to evaluate the\n",
      "performance of the model prediction, i.e. to recover the dropped zero pronoun\n",
      "into a word; the second step is to evaluate how well the systems works on\n",
      "finding an antecedent.\n",
      "\n",
      "I’m also curious why the authors decided to use attention-based neural\n",
      "network. A few sentences to provide the reasons would be helpful for other\n",
      "researchers.\n",
      "\n",
      "A minor comment:\n",
      "In figure 2, should it be s1, s2 … instead of d1, d2 ….? \n",
      "\n",
      "- General Discussion:\n",
      "Overall it is a great paper with innovative ideas and solid experiment setup.\n",
      "- Strengths:\n",
      "\n",
      "The approach is novel and the results are very promising, beating\n",
      "state-of-the-art.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      " The linguistic motivation behind the paper is troublesome (see below). I feel\n",
      "that the paper would benefit a lot from a more thoughtful interpretation of the\n",
      "results.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "This paper presents an approach for Zero Pronoun Resolution in Chinese. The\n",
      "authors advocate a novel procedure for generating large amount of relevant data\n",
      "from unlabeled documents. These data are then integrated smartly in an NN-based\n",
      "architecture at a pre-training step. The results improve on state-of-the-art.\n",
      "\n",
      "I have mixed feelings about this study. On the one hand, the approach seems\n",
      "sound and shows promising results, beating very recent systems (e.g., Chen&Ng\n",
      "2016). On the other hand, the way the main contribution is framed is very\n",
      "disturbing from the linguistic point of view. In particular, (zero) pronoun\n",
      "resolution is, linguistically speaking, a context modeling task, requiring\n",
      "accurate interpretation of discourse/salience, semantic and syntactic clues. It\n",
      "starts from the assumption that (zero) pronouns are used in specific contexts,\n",
      "where full NPs shouldn't normally be possible. From this perspective,\n",
      "generating ZP data via replacing nominal with zeroes (\"blank\") doesn't sound\n",
      "very convincing. And indeed, as the authors themselves show, the pre-training\n",
      "module alone doesn't achieve a reasonable performance. To sum it up, i don't\n",
      "think that these generated pseudo-data can be called AZP data. It seems more\n",
      "likely that they encode some form of selectional preferences (?). It would be\n",
      "nice if the authors could invest some effort in better understanding what\n",
      "exactly the pre-training module learns -- and then reformulate the\n",
      "corresponding sections. \n",
      "\n",
      "The paper can benefit from a proofreading by a native speaker of English -- for\n",
      "example, the sentence on lines 064-068 is not grammatical.\n",
      "\n",
      "-- other points --\n",
      "\n",
      "lines 78-79: are there any restrictions on the nouns and especially pronouns?\n",
      "for example, do you use this strategy for very common pronouns (as English\n",
      "\"it\")? if so, how do you guarantee that the two occurrences of the same token \n",
      "are indeed coreferent?\n",
      "\n",
      "line 91: the term antecedent is typically used to denote a preceding mention\n",
      "coreferent with the anaphor, which is not what you mean here\n",
      "\n",
      "line 144: OntoNotes (typo)\n",
      "\n",
      "lines 487-489: it has been shown that evaluation on gold-annotated data does\n",
      "not provide reliable estimation of performance. and, indeed, all the recent\n",
      "studies of coreference evaluate on system mentions. for example, the studies of\n",
      "Chen&Ng you are citing, provide different types of evaluation, including those\n",
      "on system mentions. please consider rerunning your experiments to get a more\n",
      "realistic evaluation setup\n",
      "\n",
      "line 506: i don't understand what the dagger over the system's name means. is\n",
      "your improvement statistically significant on all the domains? including bn and\n",
      "tc??\n",
      "\n",
      "line 565: learn (typo)\n",
      "\n",
      "section 3.3: in this section you use the abbreviation AZP instead of ZP without\n",
      "introducing it, please unify the terminology\n",
      "\n",
      "references -- please double-check for capitalization\n",
      "Thanks for the response. I look forward to reading about the effect of\n",
      "incentives and the ambiguity of the language in the domain.\n",
      "\n",
      "Review before author response:\n",
      "The paper proposes a way to build natural language interfaces by allowing a set\n",
      "of users to define new concepts and syntax. It's an (non-trivial) extension of\n",
      "S. I. Wang, P. Liang, and C. Manning. 2016. Learning language games through\n",
      "interaction\n",
      "\n",
      "Questions:\n",
      "- What is the size of the vocabulary used \n",
      "- Is it possible to position this paper with respect to previous work on\n",
      "inverse reinforcement learning and imitation learning ?\n",
      "\n",
      "Strengths:\n",
      "- The paper is well written\n",
      "- It provides a compelling direction/solution to the problem of dealing with a\n",
      "large set of possible programs while learning natural language interfaces. \n",
      "\n",
      "Weaknesses:\n",
      "- The authors should discuss the effect of the incentives on the final\n",
      "performance ? Were other alternatives considered ? \n",
      "- While the paper claims that the method can be extended to more practical\n",
      "domains, it is not clear to me how straightforward it is going to be. How\n",
      "sensitive is the method to the size of the vocabulary required in a domain ?\n",
      "Would increased ambiguity in natural language create new problems ? These\n",
      "questions are not discussed in the current experiments.\n",
      "- A real-world application would definitely strengthen the paper even more.\n",
      "- Strengths: This paper reports on an interesting project to enable people to\n",
      "design their own language for interacting with a computer program, in place of\n",
      "using a programming language. The specific construction that the authors focus\n",
      "on is the ability for people to make definitions. Very nicely, they can make\n",
      "recursive definitions to arrive at a very general way of giving a command. The\n",
      "example showing how the user could generate definitions to create a palm tree\n",
      "was motivating. The approach using learning of grammars to capture new cases\n",
      "seems like a good one. \n",
      "\n",
      "- Weaknesses: This seems to be an extension of the ACL 2016 paper on a similar\n",
      "topic. It would be helpful to be more explicit about what is new in this paper\n",
      "over the old one. \n",
      "\n",
      "There was not much comparison with previous work: no related work section. \n",
      "\n",
      "The features for learning are interesting but it's not always clear how they\n",
      "would come into play. For example, it would be good to see an example of how\n",
      "the social features influenced the outcome. I did not otherwise see how people\n",
      "work together to create a language. \n",
      "\n",
      "- General Discussion:\n",
      "- Strengths:\n",
      "\n",
      "The ideas and the task addressed in this paper are beautiful and original.\n",
      "Combining indirect supervision (accepting the resulting parse) with direct\n",
      "supervision (giving a definition) makes it a particularly powerful way of\n",
      "interactively building a natural language interface to a programming language.\n",
      "The proposed has a wide range of potential applications. \n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "The paper has several typos and language errors and some text seems to be\n",
      "missing from the end of section 6. It could benefit from careful proofreading\n",
      "by a native English speaker. \n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The paper presents a method for collaborative naturalization of a 'core'\n",
      "programming language by a community of users through incremental expansion of\n",
      "the syntax of the language. This expansion is performed interactively, whereby\n",
      "a user just types a command in the naturalized language, and then either\n",
      "selects through a list of candidate parses or provides a definition also in the\n",
      "natural language. The users give intuitive definitions using literals instead\n",
      "of variables (e.g. \"select orange\"), which makes this method applicable to\n",
      "non-programmers. \n",
      "A grammar is induced incrementally which is used to provide the candidate\n",
      "parses.\n",
      "\n",
      "I have read the authors' response.\n",
      "Dear Authors\n",
      "\n",
      "thanks for replying to our review comments, which clarifies some detail\n",
      "questions. I appreciate your promise to publish the code, which will be very\n",
      "helpful to other researchers. \n",
      "\n",
      "Based on this, i increased my overall score to 4. \n",
      "\n",
      "Strengths:\n",
      "- well-written\n",
      "- extensive experiments\n",
      "- good results\n",
      "\n",
      "- Weaknesses:\n",
      "- nothing ground-breaking, application of existing technologies\n",
      "- code not available\n",
      "- results are as could be expected\n",
      "\n",
      "- General Discussion:\n",
      "- why didn't you use established audio features such as MFCCs?\n",
      "\n",
      "- Minor Details:\n",
      "- L155 and other places: a LSTM -> an LSTM\n",
      "- L160, L216 and other Places: why are there hyphens (-) after the text?\n",
      "- L205: explanation of convolution is not clear\n",
      "- Table1 should appear earlier, on page 2 already cited\n",
      "- L263: is 3D-CNN a standard approach in video processing? alternatives?\n",
      "- L375, 378: the ^ should probably positioned above the y\n",
      "- L380: \"to check overfitting\" -> did you mean \"to avoid\"?\n",
      "- L403, 408..: put names in \" \" or write them italic, to make it easier to\n",
      "recognize them\n",
      "- L420: a SVM -> an SVM\n",
      "- L448: Output ... are -> wrong numerus, either \"Outputs\", or use \"is\" \n",
      "- L489: superflous whitespace after \"layer\"\n",
      "- L516, 519: \"concatenation\" should not be in a new line\n",
      "- L567: why don't you know the exact number of persons?\n",
      "- L626: remove comma after Since\n",
      "- L651: doesnt -> does not \n",
      "- L777: insert \"hand, the\" after other\n",
      "- References: need some cleanup: L823 superflous whitespace, L831 Munich, L860\n",
      "what is ACL(1)?, L888 superflous ), L894 Volume, L951 superflous new lines,\n",
      "L956 indent Linguistics properly\n",
      "The paper presents two approaches for generating English poetry. The first\n",
      "approach combine a neural phonetic encoder predicting the next phoneme with a\n",
      "phonetic-orthographic HMM decoder computing the most likely word corresponding\n",
      "to a sequence of phonemes. The second approach combines a character language\n",
      "model with a weigthed FST to impose rythm constraints on the output of the\n",
      "language model. For the second approach, the authors also present a heuristic\n",
      "approach which permit constraining the generated poem according to theme (e.g;,\n",
      "love) or poetic devices (e.g., alliteration). The generated poems are evaluated\n",
      "both instrinsically by comparing the rythm of the generated lines with a gold\n",
      "standard and extrinsically by asking 70 human evaluators to (i) determine\n",
      "whether the poem was written by a human or a machine and (ii) rate poems wrt to\n",
      "readability, form and evocation.  The results indicate that the second model\n",
      "performs best and that human evaluators find it difficult to distinguish\n",
      "between human written and machine generated poems.\n",
      "\n",
      "This is an interesting, clearly written article with novel ideas (two different\n",
      "models for poetry generation, one based on a phonetic language model the other\n",
      "on a character LM) and convincing results.\n",
      "\n",
      " For the evaluation, more precision about the evaluators and the protocol would\n",
      "be good. Did all evaluators evaluate all poems and if not how many judgments\n",
      "were collected for each poem for each task ? You mention 9 non English native\n",
      "speakers. Poems are notoriously hard to read. How fluent were these ? \n",
      "\n",
      "In the second model (character based), perhaps I missed it, but do you have a\n",
      "mechanism to avoid generating non words ? If not, how frequent are non words in\n",
      "the generated poems ?\n",
      "\n",
      "In the first model, why use an HMM to transliterate from phonetic to an\n",
      "orhographic representation rather than a CRF? \n",
      "\n",
      "Since overall, you rule out the first model as a good generic model for\n",
      "generating poetry, it might have been more interesting to spend less space on\n",
      "that model and more on the evaluation of the second model. In particular, I\n",
      "would have been interested in a more detailed discussion of the impact of the\n",
      "heuristic you use to constrain theme or poetic devices. How do these impact\n",
      "evaluation results ? Could they be combined to jointly constrain theme and\n",
      "poetic devices ? \n",
      "\n",
      "The combination of a neural mode with a WFST is reminiscent of the following\n",
      "paper which combine character based neural model to generate from dialog acts\n",
      "with an WFST to avoid generating non words. YOu should relate your work to\n",
      "theirs and cite them. \n",
      "\n",
      "Natural Language Generation through Character-Based RNNs with Finite-State\n",
      "Prior Knowledge\n",
      "Goyal, Raghav and Dymetman, Marc and Gaussier, Eric and LIG, Uni\n",
      "COLING 2016\n",
      "The paper describes two methodologies for the automatic generation of rhythmic\n",
      "poetry. Both rely on neural networks, but the second one allows for better\n",
      "control of form.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "Good procedure for generating rhythmic poetry.\n",
      "\n",
      "Proposals for adding control of theme and poetic devices (alliteration,\n",
      "consonance, asonance).\n",
      "\n",
      "Strong results in evaluation of rhythm.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "Poor coverage of existing literature on poetry generation.\n",
      "\n",
      "No comparison with existing approaches to poetry generation.\n",
      "\n",
      "No evaluation of results on theme and poetic devices.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The introduction describes the problem of poetry generation as divided into two\n",
      "subtasks: the problem of content (the poem's semantics) and the problem of form\n",
      "(the \n",
      "\n",
      "aesthetic rules the poem follows). The solutions proposed in the paper address\n",
      "both of these subtasks in a limited fashion. They rely on neural networks\n",
      "trained over corpora \n",
      "\n",
      "of poetry (represented at the phonetic or character level, depending on the\n",
      "solution) to encode the linguistic continuity of the outputs. This does indeed\n",
      "ensure that the \n",
      "\n",
      "outputs resemble meaningful text. To say that this is equivalent to having\n",
      "found a way of providing the poem with appropriate semantics would be an\n",
      "overstatement. The \n",
      "\n",
      "problem of form can be said to be addressed for the case of rhythm, and partial\n",
      "solutions are proposed for some poetic devices. Aspects of form concerned with\n",
      "structure at a \n",
      "\n",
      "larger scale (stanzas and rhyme schemes) remain beyond the proposed solutions.\n",
      "Nevertheless, the paper constitutes a valuable effort in the advancement of\n",
      "poetry generation.\n",
      "\n",
      "The review of related work provided in section 2 is very poor. It does not even\n",
      "cover the set of previous efforts that the authors themselves consider worth\n",
      "mentioning in their paper (the work of Manurung et al 2000 and Misztal and\n",
      "Indurkhya 2014 is cited later in the paper - page 4 - but it is not placed in\n",
      "section 2 with respect to the other authors mentioned there).\n",
      "\n",
      "A related research effort of particular relevance that the authors should\n",
      "consider is:\n",
      "\n",
      "- Gabriele Barbieri, François Pachet, Pierre Roy, and Mirko Degli Esposti.\n",
      "2012. Markov constraints for generating lyrics with style. In Proceedings of\n",
      "the 20th European Conference on Artificial Intelligence (ECAI'12), Luc De\n",
      "Raedt, Christian Bessiere, Didier Dubois, Patrick Doherty, and Paolo Frasconi\n",
      "(Eds.). IOS Press, Amsterdam, The Netherlands, The Netherlands, 115-120. DOI:\n",
      "https://doi.org/10.3233/978-1-61499-098-7-115\n",
      "\n",
      "This work addresses very similar problems to those discussed in the present\n",
      "paper (n-gram based generation and the problem of driving generation process\n",
      "with additional constraints). The authors should include a review of this work\n",
      "and discuss the similarities and differences with their own.\n",
      "\n",
      "Another research effort that is related to what the authors are attempting (and\n",
      "has bearing on their evaluation process) is:\n",
      "\n",
      "- Stephen McGregor, Matthew Purver and Geraint Wiggins, Process Based\n",
      "Evaluation of Computer Generated Poetry,  in: Proceedings of the INLG 2016\n",
      "Workshop on Computational Creativity and Natural Language Generation, pages\n",
      "51–60,Edinburgh, September 2016.c2016 Association for Computational\n",
      "Linguistics\n",
      "\n",
      "This work is also similar to the current effort in that it models language\n",
      "initially at a phonological level, but considers a word n-gram level\n",
      "superimposed on that, and also features a layer representint sentiment. Some of\n",
      "the considerations McGregor et al make on evaluation of computer generated\n",
      "poetry are also relevant for the extrinsic evaluation described in the present\n",
      "paper.\n",
      "\n",
      "Another work that I believe should be considered is:\n",
      "\n",
      "- \"Generating Topical Poetry\" (M. Ghazvininejad, X. Shi, Y. Choi, and K.\n",
      "Knight), Proc. EMNLP, 2016.\n",
      "\n",
      "This work generates iambic pentameter by combining finite-state machinery with\n",
      "deep learning. It would be interesting to see how the proposal in the current\n",
      "paper constrasts with this particular approach.\n",
      "\n",
      "Although less relevant to the present paper, the authors should consider\n",
      "extending their classification of poetry generation systems (they mention\n",
      "rule-based expert systems and statistical approaches) to include evolutionary\n",
      "solutions. They already mention in their paper the work of Manurung, which is\n",
      "evolutionary in nature, operating over TAG grammars.\n",
      "\n",
      "In any case, the paper as it stands holds little to no effort of comparison to\n",
      "prior approaches to poetry generation. The authors should make an effort to\n",
      "contextualise their work with respect to previous efforts, specially in the\n",
      "case were similar problems are being addressed (Barbieri et al, 2012) or\n",
      "similar methods are being applied (Ghazvininejad,  et al, 2016).\n",
      "- Strengths:\n",
      "\n",
      "The paper makes several novel contributions to (transition-based) dependency\n",
      "parsing by extending the notion of non-monotonic transition systems and dynamic\n",
      "oracles to unrestricted non-projective dependency parsing. The theoretical and\n",
      "algorithmic analysis is clear and insightful, and the paper is admirably clear.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "Given that the main motivation for using Covington's algorithm is to be able to\n",
      "recover non-projective arcs, an empirical error analysis focusing on\n",
      "non-projective structures would have further strengthened the paper. And even\n",
      "though the main contributions of the paper are on the theoretical side, it\n",
      "would have been relevant to include a comparison to the state of the art on the\n",
      "CoNLL data sets and not only to the monotonic baseline version of the same\n",
      "parser.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The paper extends the transition-based formulation of Covington's dependency\n",
      "parsing algorithm (for unrestricted non-projective structures) by allowing\n",
      "non-monotonicity in the sense that later transitions can change structure built\n",
      "by earlier transitions. In addition, it shows how approximate dynamic oracles\n",
      "can be formulated for the new system. Finally, it shows experimentally that the\n",
      "oracles provide a tight approximation and that the non-monotonic system leads\n",
      "to improved parsing accuracy over its monotonic counterpart for the majority of\n",
      "the languages included in the study.\n",
      "\n",
      "The theoretical contributions are in my view significant enough to merit\n",
      "publication, but I also think the paper could be strengthened on the empirical\n",
      "side. In particular, it would be relevant to investigate, in an error analysis,\n",
      "whether the non-monotonic system improves accuracy specifically on\n",
      "non-projective structures. Such an analysis can be motivated on two grounds:\n",
      "(i) the ability to recover non-projective structures is the main motivation for\n",
      "using Covington's algorithm in the first place; (ii) non-projective structures\n",
      "often involved long-distance dependencies that are hard to predict for a greedy\n",
      "transition-based parser, so it is plausible that the new system would improve\n",
      "the situation. \n",
      "\n",
      "Another point worth discussion is how the empirical results relate to the state\n",
      "of the art in light of recent improvements thanks to word embeddings and neural\n",
      "network techniques. For example, the non-monotonicity is claimed to mitigate\n",
      "the error propagation typical of classical greedy transition-based parsers. But\n",
      "another way of mitigating this problem is to use recurrent neural networks as\n",
      "preprocessors to the parser in order to capture more of the global sentence\n",
      "context in word representations. Are these two techniques competing or\n",
      "complementary? A full investigation of these issues is clearly outside the\n",
      "scope of the paper, but some discussion would be highly relevant.\n",
      "\n",
      "Specific questions:\n",
      "\n",
      "Why were only 9 out of the 13 data sets from the CoNLL-X shared task used? I am\n",
      "sure there is a legitimate reason and stating it explicitly may prevent readers\n",
      "from becoming suspicious. \n",
      "\n",
      "Do you have any hypothesis about why accuracy decreases for Basque with the\n",
      "non-monotonic system? Similar (but weaker) trends can be seen also for Turkish,\n",
      "Catalan, Hungarian and (perhaps) German.\n",
      "\n",
      "How do your results compare to the state of the art on these data sets? This is\n",
      "relevant for contextualising your results and allowing readers to estimate the\n",
      "significance of your improvements.\n",
      "\n",
      "Author response:\n",
      "\n",
      "I am satisfied with the author's response and see no reason to change my\n",
      "previous review.\n",
      "- Strengths:\n",
      "\n",
      "Relatively clear description of context and structure of proposed approach.\n",
      "Relatively complete description of the math. Comparison to an extensive set of\n",
      "alternative systems.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "Weak results/summary of \"side-by-side human\" comparison in Section 5. Some\n",
      "disfluency/agrammaticality.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "The article proposes a principled means of modeling utterance context,\n",
      "consisting of a sequence of previous utterances. Some minor issues:\n",
      "\n",
      "1. Past turns in Table 1 could be numbered, making the text associated with\n",
      "this table (lines 095-103) less difficult to ingest. Currently, readers need to\n",
      "count turns from the top when identifying references in the authors'\n",
      "description, and may wonder whether \"second\", \"third\", and \"last\" imply a\n",
      "side-specific or global enumeration.\n",
      "\n",
      "2. Some reader confusion may be eliminated by explicitly defining what\n",
      "\"segment\" means in \"segment level\", as occurring on line 269. Previously, on\n",
      "line 129, this seemingly same thing was referred to as \"a sequence-sequence\n",
      "[similarity matrix]\". The two terms appear to be used interchangeably, but it\n",
      "is not clear what they actually mean, despite the text in section 3.3. It seems\n",
      "the authors may mean \"word subsequence\" and \"word subsequence to word\n",
      "subsequence\", where \"sub-\" implies \"not the whole utterance\", but not sure.\n",
      "\n",
      "3. Currently, the variable symbol \"n\" appears to be used to enumerate words in\n",
      "an utterance (line 306), as well as utterances in a dialogue (line 389). The\n",
      "authors may choose two different letters for these two different purposes, to\n",
      "avoid confusing readers going through their equations.\n",
      "\n",
      "4. The statement \"This indicates that a retrieval based chatbot with SMN can\n",
      "provide a better experience than the state-of-the-art generation model in\n",
      "practice.\" at the end of section 5 appears to be unsupported. The two\n",
      "approaches referred to are deemed comparable in 555 out of 1000 cases, with the\n",
      "baseline better than the proposed method in 238 our of the remaining 445 cases.\n",
      "The authors are encouraged to assess and present the statistical significance\n",
      "of this comparison. If it is weak, their comparison permits to at best claim\n",
      "that their proposed method is no worse (rather than \"better\") than the VHRED\n",
      "baseline.\n",
      "\n",
      "5. The authors may choose to insert into Figure 1 the explicit \"first layer\",\n",
      "\"second layer\" and \"third layer\" labels they use in the accompanying text.\n",
      "\n",
      "6.  Their is a pervasive use of \"to meet\" as in \"a response candidate can meet\n",
      "each utterace\" on line 280 which is difficult to understand.\n",
      "\n",
      "7. Spelling: \"gated recurrent unites\"; \"respectively\" on line 133 should be\n",
      "removed; punctuation on line 186 and 188 is exchanged; \"baseline model over\" ->\n",
      "\"baseline model by\"; \"one cannot neglects\".\n",
      "This paper introduces new configurations and training objectives for neural\n",
      "sequence models in a multi-task setting. As the authors describe well, the\n",
      "multi-task setting is important because some tasks have shared information\n",
      "and in some scenarios learning many tasks can improve overall performance.\n",
      "\n",
      "The methods section is relatively clear and logical, and I like where it ended\n",
      "up, though it could be slightly better organized. The organization that I\n",
      "realized after reading is that there are two problems: 1) shared features end\n",
      "up in the private feature space, and 2) private features end up in the \n",
      "shared space. There is one novel method for each problem. That organization up\n",
      "front would make the methods more cohesive. In any case, they introduce one \n",
      "method that keeps task-specific features out of shared representation\n",
      "(adversarial\n",
      "loss) and another to keep shared features out of task-specific representations\n",
      "(orthogonality constraints). My only point of confusion is the adversarial\n",
      "system.\n",
      "After LSTM output there is another layer, D(s^k_T, \\theta_D), relying on\n",
      "parameters\n",
      "U and b. This output is considered a probability distribution which is compared\n",
      "against the actual. This means it is possible it will just learn U and b that\n",
      "effectively mask task-specific information from  the LSTM outputs, and doesn't \n",
      "seem like it can guarantee task-specific information is removed.\n",
      "\n",
      "Before I read the evaluation section I wrote down what I hoped the experiments\n",
      "would look like and it did most of it. This is an interesting idea and there\n",
      "are \n",
      "a lot more experiments one can imagine but I think here they have the basics\n",
      "to show the validity of their methods. It would be helpful to have best known\n",
      "results on these tasks.\n",
      "\n",
      "My primary concern with this paper is the lack of deeper motivation for the \n",
      "approach. I think it is easy to understand that in a totally shared model\n",
      "there will be problems due to conflicts in feature space. The extension to \n",
      "partially shared features seems like a reaction to that issue -- one would \n",
      "expect that the useful shared information is in the shared latent space and \n",
      "each task-specific space would learn features for that space. Maybe this works\n",
      "and maybe it doesn't, but the logic is clear to me. In contrast, the authors\n",
      "seem to start from the assumption that this \"shared-private\" model has this\n",
      "issue. I expected the argument flow to be 1) Fully-shared obviously has this\n",
      "problem; 2) shared-private seems to address this; 3) in practice shared-private\n",
      "does not fully address this issue for reasons a,b,c.; 4) we introduce a method\n",
      "that more effectively constrains the spaces.\n",
      "Table 4 helped me to partially understand what's going wrong with\n",
      "shared-private\n",
      "and what your methods do; some terms are _usually_ one connotation\n",
      "or another, and that general trend can probably get them into the shared\n",
      "feature\n",
      "space. This simple explanation, an example, and a more logical argument flow\n",
      "would help the introduction and make this a really nice reading paper.\n",
      "\n",
      "Finally, I think this research ties into some other uncited MTL work [1],\n",
      "which does deep hierarchical MTL - supervised POS tagging at a lower level,\n",
      "chunking\n",
      "at the next level up, ccg tagging higher, etc. They then discuss at the end\n",
      "some of the qualities that make MTL possible and conclude that MTL only works\n",
      "\"when tasks are sufficiently similar.\" The ASP-MTL paper made me think of this\n",
      "previous work because potentially this model could learn what sufficiently\n",
      "similar is -- i.e., if two tasks are not sufficiently similar the shared model\n",
      "would learn nothing and it would fall back to learning two independent systems,\n",
      "as compared to a shared-private model baseline that might overfit and perform\n",
      "poorly.\n",
      "\n",
      "[1]\n",
      "@inproceedings{sogaard2016deep,\n",
      "  title={Deep multi-task learning with low level tasks supervised at lower\n",
      "layers},\n",
      "  author={S{\\o}gaard, Anders and Goldberg, Yoav},\n",
      "  booktitle={Proceedings of the 54th Annual Meeting of the Association for\n",
      "Computational Linguistics},\n",
      "  volume={2},\n",
      "  pages={231--235},\n",
      "  year={2016},\n",
      "  organization={Association for Computational Linguistics}\n",
      "}\n",
      "# Paper summary\n",
      "\n",
      "This paper presents a method for learning well-partitioned shared and\n",
      "task-specific feature spaces for LSTM text classifiers. Multiclass adversarial\n",
      "training encourages shared space representations from which a discriminative\n",
      "classifier cannot identify the task source (and are thus generic). The models\n",
      "evaluates are a fully-shared, shared-private and adversarial shared-private --\n",
      "the lattermost ASP model is one of the main contributions. They also use\n",
      "orthogonality constraints to help reward shared and private spaces that are\n",
      "distinct. The ASP model has lower error rate than single-task and other\n",
      "multi-task neural models. They also experiment with a task-level cross\n",
      "validation to explore whether the shared representation can transfer across\n",
      "tasks, and it seems to favourably. Finally, there is some analysis of shared\n",
      "layer activations suggesting that the ASP model is not being misled by strong\n",
      "weights learned on a specific (inappropriate) task.\n",
      "\n",
      "# Review summary\n",
      "\n",
      "Good ideas, well expressed and tested. Some minor comments.\n",
      "\n",
      "# Strengths\n",
      "\n",
      "* This is a nice set of ideas working well together. I particularly like the\n",
      "focus on explicitly trying to create useful shared representations. These have\n",
      "been quite successful in the CV community, but it appears that one needs to\n",
      "work quite hard to create them for NLP.\n",
      "* Sections 2, 3 and 4 are very clearly expressed.\n",
      "* The task-level cross-validation in Section 5.5 is a good way to evaluate the\n",
      "transfer.\n",
      "* There is an implementation and data.\n",
      "\n",
      "# Weaknesses\n",
      "\n",
      "* There are a few minor typographic and phrasing errors. Individually, these\n",
      "are fine, but there are enough of them to warrant fixing:\n",
      "** l:84 the “infantile cart” is slightly odd -- was this a real example\n",
      "from the data?\n",
      "** l:233 “are different in” -> “differ in”\n",
      "** l:341 “working adversarially towards” -> “working against” or\n",
      "“competing with”?\n",
      "** l:434 “two matrics” -> “two matrices”\n",
      "** l:445 “are hyperparameter” -> “are hyperparameters”\n",
      "** Section 6 has a number of number agreement errors\n",
      "(l:745/746/765/766/767/770/784) and should be closely re-edited.\n",
      "** The shading on the final row of Tables 2 and 3 prints strangely…\n",
      "* There is mention of unlabelled data in Table 1 and semi-supervised learning\n",
      "in Section 4.2, but I didn’t see any results on these experiments. Were they\n",
      "omitted, or have I misunderstood?\n",
      "* The error rate differences are promising in Tables 2 and 3, but statistical\n",
      "significance testing would help make them really convincing. Especially between\n",
      "SP-MLT and ASP-MTL results to highlight the benefit of adversarial training. It\n",
      "should be pretty straightforward to adapt the non-parametric approximate\n",
      "randomisation test (see\n",
      "http://www.lr.pi.titech.ac.jp/~takamura/pubs/randtest.pdf for promising notes a\n",
      "reference to the Chinchor paper) to produce these.\n",
      "* The colours are inconsistent in the caption of Figure 5 (b). In 5 (a), blue\n",
      "is used for “Ours”, but this seems to have swapped for 5 (b). This is worth\n",
      "checking, or I may have misunderstood the caption.\n",
      "\n",
      "# General Discussion\n",
      "\n",
      "* I wonder if there’s some connection with regularisation here, as the effect\n",
      "of the adversarial training with orthogonal training is to help limit the\n",
      "shared feature space. It might be worth drawing that connection to other\n",
      "regularisation literature.\n",
      "COMMENTS AFTER AUTHOR RESPONSE:\n",
      "\n",
      "Thanks for your response, particularly for the clarification wrt the\n",
      "hypothesis. I agree with the comment wrt cross-modal mapping. What I don't\n",
      "share is the kind of equation \"visual = referential\" that you seem to assume. A\n",
      "referent can be visually presented, but visual information can be usefully\n",
      "added to a word's representation in aggregate form to encode perceptual aspects\n",
      "of the words' meaning, the same way that it is done for textual information;\n",
      "for instance, the fact that bananas are yellow\n",
      "will not frequently be mentioned in text, and adding visual information\n",
      "extracted from images will account for this aspect of the semantic\n",
      "representation of the word. This is kind of technical and specific to how we\n",
      "build distributional models, but it's also relevant if you think of human\n",
      "cognition (probably our representation for \"banana\" has some aggregate\n",
      "information about all the bananas we've seen --and touched, tasted, etc.). \n",
      "It would be useful if you could discuss this issue explicitly, differentiating\n",
      "between multi-modal distributional semantics in general and the use of\n",
      "cross-modal mapping in particular.\n",
      "\n",
      "Also, wrt the \"all models perform similarly\" comment: I really\n",
      "urge you, if the paper is accepted, to state it in this form, even if it\n",
      "doesn't completely align with your hypotheses/goals (you have enough results\n",
      "that do). It is a better description of the results, and more useful for the\n",
      "community, than clinging to the\n",
      "n-th digit difference (and this is to a large extent independent of whether the\n",
      "difference\n",
      "is actually statistical significant or not: If one bridge has 49% chances of\n",
      "collapsing and another one 50%, the difference may be statistically\n",
      "significant, but that doesn't really make the first bridge a better bridge to\n",
      "walk on).\n",
      "\n",
      "Btw, small quibble, could you find a kind of more compact and to the point\n",
      "title? (More geared towards either generally what you explore or to what you\n",
      "find?)\n",
      "\n",
      "----------\n",
      "\n",
      "The paper tackles an extremely interesting issue, that the authors label\n",
      "\"referential word meaning\", namely, the connection between a word's meaning and\n",
      "the referents (objects in the external world) it is applied to. If I understood\n",
      "it correctly, they argue that\n",
      "this is different from a typical word meaning representation as obtained e.g.\n",
      "with distributional\n",
      "methods, because one thing is the abstract \"lexical meaning\" of a word and the\n",
      "other which label is appropriate for a given referent with specific properties\n",
      "(in a specific context, although context is something they explicitly leave\n",
      "aside in this paper). This hypothesis has been previously explored in work by\n",
      "Schlangen and colleagues (cited in the paper). The paper explores referential\n",
      "word meaning empirically on a specific version of the task of Referential\n",
      "Expression Generation (REG), namely, generating the appropriate noun for a\n",
      "given visually represented object.\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "1) The problem they tackle I find extremely interesting; as they argue, REG is\n",
      "a problem that had previously been addressed mainly using symbolic methods,\n",
      "that did not easily allow for an exploration of how speakers choose the names\n",
      "of the objects. The scope of the research goes beyond REG as such, as it\n",
      "addresses the link between semantic representations and reference more broadly.\n",
      "\n",
      "2) I also like how they use current techniques and datasets (cross-modal\n",
      "mapping and word classifiers, the ReferIt dataset containing large amounts of\n",
      "images with human-generated referring expressions) to address the problem at\n",
      "hand. \n",
      "\n",
      "3) There are a substantial number of experiments as well as analysis into the\n",
      "results. \n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "1) The main weakness for me is the statement of the specific hypothesis, within\n",
      "the general research line, that the paper is probing: I found it very\n",
      "confusing.  As a result, it is also hard to make sense of the kind of feedback\n",
      "that the results give to the initial hypothesis, especially because there are a\n",
      "lot of them and they don't all point in the same direction.\n",
      "\n",
      "The paper says:\n",
      "\n",
      "\"This paper pursues the hypothesis that an accurate\n",
      "model of referential word meaning does not\n",
      "need to fully integrate visual and lexical knowledge\n",
      "(e.g. as expressed in a distributional vector\n",
      "space), but at the same time, has to go beyond\n",
      "treating words as independent labels.\"\n",
      "\n",
      "The first part of the hypothesis I don't understand: What is it to fully\n",
      "integrate (or not to fully integrate) visual and lexical knowledge? Is the goal\n",
      "simply to show that using generic distributional representation yields worse\n",
      "results than using specific, word-adapted classifiers trained on the dataset?\n",
      "If so, then the authors should explicitly discuss the bounds of what they are\n",
      "showing: Specifically, word classifiers must be trained on the dataset itself\n",
      "and only word classifiers with a sufficient amount of items in the dataset can\n",
      "be obtained, whereas word vectors are available for many other words and are\n",
      "obtained from an independent source (even if the cross-modal mapping itself is\n",
      "trained on the dataset); moreover, they use the simplest Ridge Regression,\n",
      "instead of the best method from Lazaridou et al. 2014, so any conclusion as to\n",
      "which method is better should be taken with a grain of salt. However, I'm\n",
      "hoping that the research goal is both more constructive and broader. Please\n",
      "clarify. \n",
      "\n",
      "2) The paper uses three previously developed methods on a previously available\n",
      "dataset. The problem itself has been defined before (in Schlangen et al.). In\n",
      "this sense, the originality of the paper is not high. \n",
      "\n",
      "3) As the paper itself also points out, the authors select a very limited\n",
      "subset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm\n",
      "not even sure why they limited it this way (see detailed comments below).\n",
      "\n",
      "4) Some aspects could have been clearer (see detailed comments).\n",
      "\n",
      "5) The paper contains many empirical results and analyses, and it makes a\n",
      "concerted effort to put them together; but I still found it difficult to get\n",
      "the whole picture: What is it exactly that the experiments in the paper tell us\n",
      "about the underlying research question in general, and the specific hypothesis\n",
      "tested in particular? How do the different pieces of the puzzle that they\n",
      "present fit together?\n",
      "\n",
      "- General Discussion: [Added after author response]\n",
      "\n",
      "Despite the weaknesses, I find the topic of the paper very relevant and also\n",
      "novel enough, with an interesting use of current techniques to address an \"old\"\n",
      "problem, REG and reference more generally, in a way that allows aspects to be\n",
      "explored that have not received enough attention. The experiments and analyses\n",
      "are a substantial contribution, even though, as mentioned above, I'd like the\n",
      "paper to present a more coherent overall picture of how the many experiments\n",
      "and analyses fit together and address the question pursued.\n",
      "\n",
      "- Detailed comments:\n",
      "\n",
      "Section 2 is missing the following work in computational semantic approaches to\n",
      "reference:\n",
      "\n",
      "Abhijeet  Gupta,  Gemma  Boleda,  Marco  Baroni,  and Sebastian  Pado. 2015.  \n",
      "Distributional                                            vectors  encode \n",
      "referential        \n",
      "\n",
      "attributes.\n",
      "Proceedings of\n",
      "EMNLP,\n",
      "12-21\n",
      "\n",
      "Aurelie Herbelot and Eva Maria Vecchi.                                           \n",
      "2015. \n",
      "Building\n",
      "a\n",
      "shared\n",
      "world:\n",
      "mapping\n",
      "distributional to model-theoretic semantic spaces. Proceedings of EMNLP,\n",
      "22–32.\n",
      "\n",
      "142 how does Roy's work go beyond early REG work?\n",
      "\n",
      "155 focusses links\n",
      "\n",
      "184 flat \"hit @k metric\": \"flat\"?\n",
      "\n",
      "Section 3: please put the numbers related to the dataset in a table, specifying\n",
      "the image regions, number of REs, overall number of words, and number of object\n",
      "names in the original ReferIt dataset and in the version you use. By the way,\n",
      "will you release your data? I put a \"3\" for data because in the reviewing form\n",
      "you marked \"Yes\" for data, but I can't find the information in the paper.\n",
      "\n",
      "229 \"cannot be considered to be names\" ==> \"image object names\"\n",
      "\n",
      "230 what is \"the semantically annotated portion\" of ReferIt?\n",
      "\n",
      "247 why don't you just keep \"girl\" in this example, and more generally the head\n",
      "nouns of non-relational REs? More generally, could you motivate your choices a\n",
      "bit more so we understand why you ended up with such a restricted subset of\n",
      "ReferIt?\n",
      "\n",
      "258 which 7 features? (list) How did you extract them?\n",
      "\n",
      "383 \"suggest that lexical or at least distributional knowledge is detrimental\n",
      "when learning what a word refers to in the world\": How does this follow from\n",
      "the results of Frome et al. 2013 and Norouzi et al. 2013? Why should\n",
      "cross-modal projection give better results? It's a very different type of\n",
      "task/setup than object labeling.\n",
      "\n",
      "394-395 these numbers belong in the data section\n",
      "\n",
      "Table 1: Are the differences between the methods statistically significant?\n",
      "They are really numerically so small that any other conclusion to \"the methods\n",
      "perform similarly\" seems unwarranted to me. Especially the \"This suggests...\"\n",
      "part (407). \n",
      "\n",
      "Table 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost\n",
      "identical to wac); this is counter-intuitive given the @1 and @2 results. Any\n",
      "idea of what's going on?\n",
      "\n",
      "Section 5.2: Why did you define your ensemble classifier by hand instead of\n",
      "learning it? Also, your method amounts to majority voting, right? \n",
      "\n",
      "Table 2: the order of the models is not the same as in the other tables + text.\n",
      "\n",
      "Table 3: you report cosine distances but discuss the results in terms of\n",
      "similarity. It would be clearer (and more in accordance with standard practice\n",
      "in CL imo) if you reported cosine similarities.\n",
      "\n",
      "Table 3: you don't comment on the results reported in the right columns. I\n",
      "found it very curious that the gold-top k data similarities are higher for\n",
      "transfer+sim-wap, whereas the results on the task are the same. I think that\n",
      "you could squeeze more information wrt the phenomenon and the models out of\n",
      "these results.\n",
      "\n",
      "496 format of \"wac\"\n",
      "\n",
      "Section 6 I like the idea of the task a lot, but I was very confused as to how\n",
      "you did and why: I don't understand lines 550-553. What is the task exactly? An\n",
      "example would help. \n",
      "\n",
      "558 \"Testsets\"\n",
      "\n",
      "574ff Why not mix in the train set examples with hypernyms and non-hypernyms?\n",
      "\n",
      "697 \"more even\": more wrt what?\n",
      "\n",
      "774ff \"Previous cross-modal mapping models ... force...\": I don't understand\n",
      "this claim.\n",
      "\n",
      "792 \"larger test sets\": I think that you could even exploit ReferIt more (using\n",
      "more of its data) before moving on to other datasets.\n",
      "AFTER AUTHOR RESPONSE\n",
      "\n",
      "I accept the response about emphasizing novelty of the task and comparison with\n",
      "previous work. Also increase ratings for the dataset and software that are\n",
      "promised to become public before the article publishing.\n",
      "\n",
      "======================\n",
      "\n",
      "GENERAL \n",
      "The paper presents an interesting empirical comparison of 3 referring\n",
      "expression generation models. The main novelty lies in the comparison of a yet\n",
      "unpublished model called SIM-WAP (in press by Anonymous). The model is\n",
      "described in SECTION 4.3 but it is not clear whether it is extended or modified\n",
      "anyhow in the current paper.  \n",
      "\n",
      "The novelty of the paper may be considered as the comparison of the unpublished\n",
      "SIM-WAP model to existing 2 models. This complicates evaluation of the novelty\n",
      "because similar experiments were already performed for the other two models and\n",
      "it is unclear why this comparison was not performed in the paper where SIM-WAP\n",
      "model was presented. A significant novelty might be the combined model yet this\n",
      "is not stated clearly and the combination is not described with enough details.\n",
      "\n",
      "The contribution of the paper may be considered the following: the side-by-side\n",
      "comparison of the 3 methods for REG; analysis of zero-shot experiment results\n",
      "which mostly confirms similar observations in previous works; analysis of the\n",
      "complementarity of the combined model.                     \n",
      "\n",
      "WEAKNESSES\n",
      "Unclear novelty and significance of contributions. The work seems like an\n",
      "experimental extension of the cited Anonymous paper where the main method was\n",
      "introduced.    \n",
      "\n",
      "Another weakness is the limited size of the vocabulary in the zero-shot\n",
      "experiments that seem to be the most contributive part. \n",
      "\n",
      "Additionally, the authors never presented significance scores for their\n",
      "accuracy results. This would have solidified the empirical contribution of the\n",
      "work which its main value.   \n",
      "\n",
      "My general feeling is that the paper is more appropriate for a conference on\n",
      "empirical methods such as EMNLP. \n",
      "\n",
      "Lastly, I have not found any link to any usable software. Existing datasets\n",
      "have been used for the work.  \n",
      "\n",
      "Observations by Sections: \n",
      "\n",
      "ABSTRACT\n",
      "\"We compare three recent models\" -- Further in the abstract you write that you\n",
      "also experiment with the combination of approaches. In Section 2 you write that\n",
      "\"we present a model that exploits distributional knowledge for learning\n",
      "referential word meaning as well, but explore and compare different ways of\n",
      "combining visual and lexical aspects of referential word meaning\" which\n",
      "eventually might be a better summarization of the novelty introduced in the\n",
      "paper and give more credit to the value of your work. \n",
      "\n",
      "My suggestion is to re-write the abstract (and eventually even some sections in\n",
      "the paper) focusing on the novel model and results and not just stating that\n",
      "you compare models of others.                  \n",
      "\n",
      "INTRODUCTION \n",
      "\"Determining such a name is is\" - typo \n",
      "\"concerning e.g.\" -> \"concerning, e.g.,\" \n",
      "\"having disjunct extensions.\" - specify or exemplify, please \n",
      "\"building in Figure 1\" -> \"building in Figure 1 (c)\"\n",
      "\n",
      "SECTION 4\n",
      "\"Following e.g. Lazaridou et al. (2014),\" - \"e.g.\" should be omitted  \n",
      "\n",
      "SECTION 4.2\n",
      "\"associate the top n words with their corresponding distributional vector\" -\n",
      "What are the values of N that you used? If there were any experiments for\n",
      "finding the optimal values, please, describe because this is original work. The\n",
      "use top N = K is not obvious and not obvious why it should be optimal (how\n",
      "about finding similar vectors to each 5 in top 20?)    \n",
      "\n",
      "SECTION 4.3 \n",
      "\"we annotate its training instances with a fine-grained similarity signal\n",
      "according to their object names.\" - please, exemplify. \n",
      "\n",
      "LANGUAGE   \n",
      "Quite a few typos in the draft. Generally, language should be cleaned up (\"as\n",
      "well such as\"). \n",
      "Also, I believe the use of American English spelling standard is preferable\n",
      "(e.g., \"summarise\" -> \"summarize\"). Please, double check with your conference\n",
      "track chairs.\n",
      "- Strengths:\n",
      "\n",
      "- Weaknesses:\n",
      "Many grammar errors, such as the abstract\n",
      "\n",
      "- General Discussion:\n",
      "- Strengths: Introduces  a new document clustering approach and compares it to\n",
      "several established methods, showing that it improves results in most cases.\n",
      "The analysis is very detailed and thorough--quite dense in many places and\n",
      "requires careful reading.\n",
      "\n",
      "The presentation is organized and clear, and I am impressed by the range of\n",
      "comparisons and influential factors that were considered. Argument is\n",
      "convincing and the work should influence future approaches.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      " The paper does not provide any information on the availability of the software\n",
      "described.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "Needs some (minor) editing for English and typos--here are just a few:\n",
      "\n",
      "Line 124: regardless the size > regardless of the size\n",
      "Line 126: resources. Because > resources, because\n",
      "Line 205: consist- ing mk > consisting of mk\n",
      "Line 360: versionand > version and\n",
      "- Strengths:\n",
      "\n",
      "This paper presents a sophisticated application of Grid-type Recurrent Neural\n",
      "Nets to the task of determining predicate-argument structures (PAS) in\n",
      "Japanese.  The approach does not use any explicit syntactic structure, and\n",
      "outperforms the current SOA systems that do include syntactic structure.  The\n",
      "authors give a clear and detailed description of the implementation and of the\n",
      "results.  In particular, they pay close attention to the performance on dropped\n",
      "arguments, zero pronouns, which are prevalent in Japanese and especially\n",
      "challenging with respect to PAS. Their multi-sequence model, which takes all of\n",
      "the predicates in the sentence into account, achieves the best performance for\n",
      "these examples.  The paper is detailed and clearly written.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "I really only have minor comments. There are some typos listed below, the\n",
      "correction of which would improve English fluency. I think it would be worth\n",
      "illustrating the point about the PRED including context around the \"predicate\"\n",
      "with the example from Fig 6 where the accusative marker is included with the\n",
      "verb in the PRED string.  I didn't understand the use of boldface in Table 2,\n",
      "p. 7.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "Typos:\n",
      "\n",
      "p1 :  error propagation does not need a \"the\", nor does \"multi-predicate\n",
      "interactions\"\n",
      "p2: As an solution -> As a solution, single-sequence model -> a single-sequence\n",
      "model,                    multi-sequence model -> a multi-sequence model \n",
      "p. 3 Example in Fig 4.                    She ate a bread -> She ate bread.\n",
      "p. 4 assumes the independence -> assumed independence, the multi-predicate\n",
      "interactions -> multi-predicate interactions, the multi-sequence model -> a\n",
      "multi-sequence model\n",
      "p.7: the residual connections -> residual connections, the multi-predicate\n",
      "interactions -> multi-predicate interactions (twice)\n",
      "p8 NAIST Text Corpus -> the NAIST Text Corpus, the state-of-the-art result ->\n",
      "state-of-the-art results\n",
      "\n",
      "I have read the author response and am satisfied with it.\n",
      "This paper proposes new prediction models for Japanese SRL task by adopting the\n",
      "English state-of-the-art model of (Zhou and Xu, 2015).\n",
      "The authors also extend the model by applying the framework of Grid-RNNs in\n",
      "order to handle the interactions between the arguments of multiple predicates.\n",
      "\n",
      "The evaluation is performed on the well-known benchmark dataset in Japanese\n",
      "SRL, and obtained a significantly better performance than the current state of\n",
      "the art system.\n",
      "\n",
      "Strengths:\n",
      "The paper is well-structured and well-motivated.\n",
      "The proposed model obtains an improvement in accuracy compared with the current\n",
      "state of the art system.\n",
      "Also, the model using Grid-RNNs achieves a slightly better performance than\n",
      "that of proposed single-sequential model, mainly due to the improvement on the\n",
      "detection of zero arguments, that is the focus of this paper.\n",
      "\n",
      "Weakness:\n",
      "To the best of my understanding, the main contribution of this paper is an\n",
      "extension of the single-sequential model to the multi-sequential model. The\n",
      "impact of predicate interactions is a bit smaller than that of (Ouchi et al.,\n",
      "2015). There is a previous work (Shibata et al., 2016) that extends the (Ouchi\n",
      "et al., 2015)'s model\n",
      "with neural network modeling. I am curious about the comparison between them.\n",
      "This paper proposes a joint neural modelling approach to PAS analysis in\n",
      "Japanese, based on Grid-RNNs, which it compares variously with a conventional\n",
      "single-sequence RNN approach.\n",
      "\n",
      "This is a solidly-executed paper, targeting a well-established task from\n",
      "Japanese but achieving state-of-the-art results at the task, and presenting\n",
      "the task in a mostly accessible manner for those not versed in\n",
      "Japanese. Having said that, I felt you could have talked up the complexity of\n",
      "the task a bit, e.g. wrt your example in Figure 1, talking through the\n",
      "inherent ambiguity between the NOM and ACC arguments of the first predicate,\n",
      "as the NOM argument of the second predicate, and better describing how the\n",
      "task contrasts with SRL (largely through the ambiguity in zero pronouns). I\n",
      "would also have liked to have seen some stats re the proportion of zero\n",
      "pronouns which are actually intra-sententially resolvable, as this further\n",
      "complicates the task as defined (i.e. needing to implicitly distinguish\n",
      "between intra- and inter-sentential zero anaphors). One thing I wasn't sure of\n",
      "here: in the case of an inter-sentential zero pronoun for the argument of a\n",
      "given predicate, what representation do you use? Is there simply no marking of\n",
      "that argument at all, or is it marked as an empty argument? My reading of the\n",
      "paper is that it is the former, in which case there is no explicit\n",
      "representation of the fact that there is a zero pronoun, which seems like a\n",
      "slightly defective representation (which potentially impacts on the ability of\n",
      "the model to capture zero pronouns); some discussion of this would have been\n",
      "appreciated.\n",
      "\n",
      "There are some constraints that don't seem to be captured in the model (which\n",
      "some of the ILP-based methods for SRL explicitly model, e.g.): (1) a given\n",
      "predicate will generally have only one argument of a given type (esp. NOM and\n",
      "ACC); and (2) a given argument generally only fills one argument slot for a\n",
      "given predicate. I would have liked to have seen some analysis of the output\n",
      "of the model to see how well the model was able to learn these sorts of\n",
      "constraints. More generally, given the mix of numbers in Table 3 between\n",
      "Single-Seq and Multi-Seq (where it is really only NOM where there is any\n",
      "improvement for Multi-Seq), I would have liked to have seen some discussion of\n",
      "the relative differences in the outputs of the two models: are they largely\n",
      "identical, or very different but about the same in aggregate, e.g.? In what\n",
      "contexts do you observe differences between the two models? Some analysis like\n",
      "this to shed light on the internals of the models would have made the\n",
      "difference between a solid and a strong paper, and is the main area where I\n",
      "believe the paper could be improved (other than including results for SRL, but\n",
      "that would take quite a bit more work).\n",
      "\n",
      "The presentation of the paper was good, with the Figures aiding understanding\n",
      "of the model. There were some low-level language issues, but nothing major:\n",
      "\n",
      "l19: the error propagation -> error propagation\n",
      "l190: an solution -> a solution\n",
      "l264 (and Figure 2): a bread -> bread\n",
      "l351: the independence -> independence\n",
      "l512: the good -> good\n",
      "l531: from their model -> of their model\n",
      "l637: significent -> significance\n",
      "l638: both of -> both\n",
      "\n",
      "and watch casing in your references (e.g. \"japanese\", \"lstm\", \"conll\", \"ilp\")\n",
      "- Strengths:\n",
      "\n",
      "- this article puts two fields together: text readability for humans and\n",
      "machine comprehension of texts\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "- The goal of your paper is not entirely clear. I had to read the paper 4 times\n",
      "and I still do not understand what you are talking about!\n",
      "- The article is highly ambiguous what it talks about - machine comprehension\n",
      "or text readability for humans\n",
      "- you miss important work in the readability field\n",
      "- Section 2.2. has completely unrelated discussion of theoretical topics.\n",
      "- I have the feeling that this paper is trying to answer too many questions in\n",
      "the same time, by this making itself quite weak. Questions such as “does text\n",
      "readability have impact on RC datasets” should be analyzed separately from\n",
      "all these prerequisite skills.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "- The title is a bit ambiguous, it would be good to clarify that you are\n",
      "referring to machine comprehension of text, and not human reading\n",
      "comprehension, because “reading comprehension” and “readability”\n",
      "usually mean that.\n",
      "- You say that your “dataset analysis suggested that the readability of RC\n",
      "datasets does not directly affect the question difficulty”, but this depends\n",
      "on the method/features used for answer detection, e.g. if you use\n",
      "POS/dependency parse features.\n",
      "- You need to proofread the English of your paper, there are some important\n",
      "omissions, like “the question is easy to solve simply look..” on page 1.\n",
      "- How do you annotate datasets with “metrics”??\n",
      "- Here you are mixing machine reading comprehension of texts and human reading\n",
      "comprehension of texts, which, although somewhat similar, are also quite\n",
      "different, and also large areas.\n",
      "- “readability of text” is not “difficulty of reading contents”. Check\n",
      "this:\n",
      "DuBay, W.H. 2004. The Principles of Readability. Costa Mesa, CA: Impact\n",
      "information. \n",
      "- it would be good if you put more pointers distinguishing your work from\n",
      "readability of questions for humans, because this article is highly ambiguous.\n",
      "E.g. on page 1 “These two examples show that the readability of the text does\n",
      "not necessarily correlate with the difficulty of the questions” you should\n",
      "add “for machine comprehension”\n",
      "- Section 3.1. - Again: are you referring to such skills for humans or for\n",
      "machines? If for machines, why are you citing papers for humans, and how sure\n",
      "are you they are referring to machines too?\n",
      "- How many questions the annotators had to annotate? Were the annotators clear\n",
      "they annotate the questions keeping in mind machines and not people?\n",
      "- Strengths:\n",
      "The paper presents an interesting extension to attention-based neural MT\n",
      "approaches, which leverages source-sentence chunking as additional piece of\n",
      "information from the source sentence. The model is modified such that this\n",
      "chunking information is used differently by two recurrent layers: while one\n",
      "focuses in generating a chunk at a time, the other focuses on generating the\n",
      "words within the chunk. This is interesting. I believe readers will enjoy\n",
      "getting to know this approach and how it performs.\n",
      "The paper is very clearly written, and alternative approaches are clearly\n",
      "contrasted. The evaluation is well conducted, has a direct contrast with other\n",
      "papers (and evaluation tables), and even though it could be strengthened (see\n",
      "my comments below), it is convincing.\n",
      "\n",
      "- Weaknesses:\n",
      "As always, more could be done in the experiments section to strengthen the case\n",
      "for chunk-based models. For example, Table 3 indicates good results for Model 2\n",
      "and Model 3 compared to previous papers, but a careful reader will wonder\n",
      "whether these improvements come from switching from LSTMs to GRUs. In other\n",
      "words, it would be good to see the GRU tree-to-sequence result to verify that\n",
      "the chunk-based approach is still best.\n",
      "\n",
      "Another important aspect is the lack of ensembling results. The authors put a\n",
      "lot of emphasis is claiming that this is the best single NMT model ever\n",
      "published. While this is probably true, in the end the best WAT system for\n",
      "Eng-Jap is at 38.20 (if I'm reading the table correctly) - it's an ensemble of\n",
      "3. If the authors were able to report that their 3-way chunk-based ensemble\n",
      "comes top of the table, then this paper could have a much stronger impact.\n",
      "\n",
      "Finally, Table 3 would be more interesting if it included decoding times. The\n",
      "authors mention briefly that the character-based model is less time-consuming\n",
      "(presumably based on Eriguchi et al.'16), but no cite is provided, and no\n",
      "numbers from chunk-based decoding are reported either. Is the chunk-based model\n",
      "faster or slower than word-based? Similar? Who know... Adding a column to Table\n",
      "3 with decoding times would give more value to the paper.\n",
      "\n",
      "- General Discussion:\n",
      "Overall I think the paper is interesting and worth publishing. I have minor\n",
      "comments and suggestions to the authors about how to improve their presentation\n",
      "(in my opinion, of course). \n",
      "\n",
      "* I think they should clearly state early on that the chunks are supplied\n",
      "externally - in other words, that the model does not learn how to chunk. This\n",
      "only became apparent to me when reading about CaboCha on page 6 - I don't think\n",
      "it's mentioned earlier, and it is important.\n",
      "\n",
      "* I don't see why the authors contrast against the char-based baseline so often\n",
      "in the text (at least a couple of times they boast a +4.68 BLEU gain). I don't\n",
      "think readers are bothered... Readers are interested in gains over the best\n",
      "baseline.\n",
      "\n",
      "* It would be good to add a bit more detail about the way UNKs are being\n",
      "handled by the neural decoder, or at least add a citation to the\n",
      "dictionary-based replacement strategy being used here.\n",
      "\n",
      "* The sentence in line 212 (\"We train a GRU that encodes a source sentence into\n",
      "a single vector\") is not strictly correct. The correct way would be to say that\n",
      "you do a bidirectional encoder that encodes the source sentence into a set of\n",
      "vectors... at least, that's what I see in Figure 2.\n",
      "\n",
      "* The motivating example of lines 69-87 is a bit weird. Does \"you\" depend on\n",
      "\"bite\"? Or does it depend on the source side? Because if it doesn't depend on\n",
      "\"bite\", then the argument that this is a long-dependency problem doesn't really\n",
      "apply.\n",
      "- Summary\n",
      "\n",
      "This paper introduces chunk-level architecture for existing NMT models. Three\n",
      "models are proposed to model the correlation between word and chunk modelling\n",
      "on the target side in the existing NMT models. \n",
      "\n",
      "- Strengths:\n",
      "\n",
      "The paper is well-written and clear about the proposed models and its\n",
      "contributions. \n",
      "\n",
      "The proposed models to incorporating chunk information into NMT models are\n",
      "novel and well-motivated. I think such models can be generally applicable for\n",
      "many other language pairs. \n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "There are some minor points, listed as follows:\n",
      "\n",
      "1) Figure 1: I am a bit surprised that the function words dominate the content\n",
      "ones in a Japanese sentence. Sorry I may not understand Japanese. \n",
      "\n",
      "2) In all equations, sequences/vectors (like matrices) should be represented\n",
      "as bold texts to distinguish from scalars, e.g., hi, xi, c, s, ...\n",
      "\n",
      "3) Equation 12: s_j-1 instead of s_j.\n",
      "\n",
      "4) Line 244: all encoder states should be referred to bidirectional RNN states.\n",
      "\n",
      "5) Line 285: a bit confused about the phrase \"non-sequential information such\n",
      "as chunks\". Is chunk still sequential information???\n",
      "\n",
      "6) Equation 21: a bit confused, e.g, perhaps insert k into s1(w) like s1(w)(k)\n",
      "to indicate the word in a chunk.  \n",
      "\n",
      "7) Some questions for the experiments:\n",
      "\n",
      "Table 1: source language statistics? \n",
      "\n",
      "For the baselines, why not running a baseline (without using any chunk\n",
      "information) instead of using (Li et al., 2016) baseline (|V_src| is\n",
      "different)? It would be easy to see the effect of chunk-based models. Did (Li\n",
      "et al., 2016) and other baselines use the same pre-processing and\n",
      "post-processing steps? Other baselines are not very comparable. After authors's\n",
      "response, I still think that (Li et al., 2016) baseline can be a reference but\n",
      "the baseline from the existing model should be shown. \n",
      "\n",
      "Figure 5: baseline result will be useful for comparison? chunks in the\n",
      "translated examples are generated *automatically* by the model or manually by\n",
      "the authors? Is it possible to compare the no. of chunks generated by the model\n",
      "and by the bunsetsu-chunking toolkit? In that case, the chunk information for\n",
      "Dev and Test in Table 1 will be required. BTW, the authors's response did not\n",
      "address my point here. \n",
      "\n",
      "8) I am bit surprised about the beam size 20 used in the decoding process. I\n",
      "suppose large beam size is likely to make the model prefer shorter generated\n",
      "sentences. \n",
      "\n",
      "9) Past tenses should be used in the experiments, e.g.,\n",
      "\n",
      "Line 558: We *use* (used) ...\n",
      "\n",
      "Line 579-584: we *perform* (performed) ... *use* (used) ...\n",
      "\n",
      "...\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "Overall, this is a solid work - the first one tackling the chunk-based NMT;\n",
      "and it well deserves a slot at ACL.\n",
      "- Strengths: The authors have nice coverage of a different range of language\n",
      "settings to isolate the way that relatedness and amount of morphology interact\n",
      "(i.e., translating between closely related morphologically rich languages vs\n",
      "distant ones) in affecting what the system learns about morphology. They\n",
      "include an illuminating analysis of what parts of the architecture end up being\n",
      "responsible for learning morphology, particularly in examining how the\n",
      "attention mechanism leads to more impoverished target side representations.\n",
      "Their findings are of high interest and practical usefulness for other users of\n",
      "NMT. \n",
      "\n",
      "- Weaknesses: They gloss over the details of their character-based encoder.\n",
      "There are many different ways to learn character-based representations, and\n",
      "omitting a discussion of how they do this leaves open questions about the\n",
      "generality of their findings. Also, their analysis could've been made more\n",
      "interesting had they chosen languages with richer and more challenging\n",
      "morphology such as Turkish or Finnish, accompanied by finer-grained morphology\n",
      "prediction and analysis.\n",
      "\n",
      "- General Discussion: This paper brings insight into what NMT models learn\n",
      "about morphology by training NMT systems and using the encoder or decoder\n",
      "representations, respectively, as input feature representations to a POS- or\n",
      "morphology-tagging classification task. This paper is a straightforward\n",
      "extension of \"Does String-Based Neural MT Learn Source Syntax?,\" using the same\n",
      "methodology but this time applied to morphology. Their findings offer useful\n",
      "insights into what NMT systems learn.\n",
      "Strengths:\n",
      "\n",
      "- This paper describes experiments that aim to address a crucial\n",
      "problem for NMT: understanding what does the model learn about morphology and\n",
      "syntax, etc..\n",
      "- Very clear objectives and experiments effectively laid down.              Good\n",
      "state\n",
      "of the art review and comparison. In general, this paper is a pleasure to read.\n",
      "- Sound experimentation framework. Encoder/Decoder Recurrent layer\n",
      "outputs are used to train POS/morphological classifiers. They show the effect\n",
      "of certain changes in the framework on the classifier accuracy (e.g. use\n",
      "characters instead of words).\n",
      "- Experimentation is carried out on many language pairs.\n",
      "- Interesting conclusions derived from this work, and not all agree with\n",
      "intuition.\n",
      "\n",
      "Weaknesses:\n",
      "\n",
      " -  The contrast of character-based vs word-based representations  is slightly\n",
      "lacking: NMT with byte-pair encoding is showing v. strong performance in the\n",
      "literature. It would have been more relevant to have BPE in the mix, or replace\n",
      "word-based representations if three is too many.\n",
      " - Section 1: \"… while higher layers are more focused on word meaning\";\n",
      "similar sentence in Section 7. I am ready to agree with this intuition, but I\n",
      "think the experiments in this paper do not support this particular sentence.\n",
      "Therefore it should not be included, or it should be clearly stressed that this\n",
      "is a reasonable hypothesis based on indirect evidence (translation performance\n",
      "improves but morphology on higher layers does not).\n",
      "\n",
      "Discussion:\n",
      "\n",
      "This is a  fine paper that presents a thorough and systematic analysis of the\n",
      "NMT model, and derives several interesting conclusions based on many data\n",
      "points across several language pairs. I find particularly interesting that (a)\n",
      "the target language affects the quality of the encoding on the source side; in\n",
      "particular, when the target side is a morphologically-poor language (English)\n",
      "the pos tagger accuracy for the encoder improves. (b) increasing the depth of\n",
      "the encoder does not improve pos accuracy (more experiments needed to determine\n",
      "what does it improve); (c) the attention layer hurts the quality of the decoder\n",
      "representations.  I wonder if (a) and (c) are actually related? The attention\n",
      "hurts the decoder representation, which is more difficult to learn for a\n",
      "morphologically rich language; in turn, the encoders learn based on the global\n",
      "objective, and this backpropagates through the decoder. Would this not be a\n",
      "strong\n",
      "indication that we need separate objectives to govern the encoder/decoder\n",
      "modules of\n",
      "the NMT model?\n",
      "This paper develops an LSTM-based model for classifying connective uses for\n",
      "whether they indicate that a causal relation was intended. The guiding idea is\n",
      "that the expression of causal relations is extremely diverse and thus not\n",
      "amenable to syntactic treatment, and that the more abstract representations\n",
      "delivered by neural models are therefore more suitable as the basis for making\n",
      "these decisions.\n",
      "\n",
      "The experiments are on the AltLex corpus developed by Hidley and McKeown. The\n",
      "results offer modest but consistent support for the general idea, and they\n",
      "provide some initial insights into how best to translate this idea into a\n",
      "model. The paper distribution includes the TensorFlow-based models used for the\n",
      "experiments.\n",
      "\n",
      "Some critical comments and questions:\n",
      "\n",
      "* The introduction is unusual in that it is more like a literature review than\n",
      "a full overview of what the paper contains. This leads to some redundancy with\n",
      "the related work section that follows it. I guess I am open to a non-standard\n",
      "sort of intro, but this one really doesn't work: despite reviewing a lot of\n",
      "ideas, it doesn't take a stand on what causation is or how it is expressed, but\n",
      "rather only makes a negative point (it's not reducible to syntax). We aren't\n",
      "really told what the positive contribution will be except for the very general\n",
      "final paragraph of the section.\n",
      "\n",
      "* Extending the above, I found it disappointing that the paper isn't really\n",
      "clear about the theory of causation being assumed. The authors seem to default\n",
      "to a counterfactual view that is broadly like that of David Lewis, where\n",
      "causation is a modal sufficiency claim with some other counterfactual\n",
      "conditions added to it. See line 238 and following; that arrow needs to be a\n",
      "very special kind of implication for this to work at all, and there are\n",
      "well-known problems with Lewis's theory (see\n",
      "http://bcopley.com/wp-content/uploads/CopleyWolff2014.pdf). There are comments\n",
      "elsewhere in the paper that the authors don't endorse the counterfactual view,\n",
      "but then what is the theory being assumed? It can't just be the temporal\n",
      "constraint mentioned on page 3!\n",
      "\n",
      "* I don't understand the comments regarding the example on line 256. The\n",
      "authors seem to be saying that they regard the sentence as false. If it's true,\n",
      "then there should be some causal link between the argument and the breakage.\n",
      "There are remaining issues about how to divide events into sub-events, and\n",
      "these impact causal theories, but those are not being discussed here, leaving\n",
      "me confused.\n",
      "\n",
      "* The caption for Figure 1 is misleading, since the diagram is supposed to\n",
      "depict only the \"Pair_LSTM\" variant of the model. My bigger complaint is that\n",
      "this diagram is needlessly imprecise. I suppose it's okay to leave parts of the\n",
      "standard model definition out of the prose, but then these diagrams should have\n",
      "a clear and consistent semantics. What are all the empty circles between input\n",
      "and the \"LSTM\" boxes? The prose seems to say that the model has a look-up\n",
      "layer, a Glove layer, and then ... what? How many layers of representation are\n",
      "there? The diagram is precise about the pooling tanh layers pre-softmax, but\n",
      "not about this. I'm also not clear on what the \"LSTM\" boxes represent. It seems\n",
      "like it's just the leftmost/final representation that is directly connected to\n",
      "the layers above. I suggest depicting that connection clearly.\n",
      "\n",
      "* I don't understand the sentence beginning on line 480. The models under\n",
      "discussion do not intrinsically require any padding. I'm guessing this is a\n",
      "requirement of TensorFlow and/or efficient training. That's fine. If that's\n",
      "correct, please say that. I don't understand the final clause, though. How is\n",
      "this issue even related to the question of what is \"the most convenient way to\n",
      "encode the causal meaning\"? I don't see how convenience is an issue or how this\n",
      "relates directly to causal meaning.\n",
      "\n",
      "* The authors find that having two independent LSTMs (\"Stated_LSTM\") is\n",
      "somewhat better than one where the first feeds into the second. This issue is\n",
      "reminiscent of discussions in the literature on natural language entailment,\n",
      "where the question is whether to represent premise and hypothesis independently\n",
      "or have the first feed into the second. I regard this as an open question for\n",
      "entailment, and I bet it needs further investigation for causal relations too.\n",
      "So I can't really endorse the sentence beginning on line 587: \"This behaviour\n",
      "means that our assumption about the relation between the meanings of the two\n",
      "input events does not hold, so it is better to encode each argument\n",
      "independently and then to measure the relation between the arguments by using\n",
      "dense layers.\" This is very surprising since we are talking about subparts of a\n",
      "sentence that might share a lot of information.\n",
      "\n",
      "* It's hard to make sense of the hyperparameters that led to the best\n",
      "performance across tasks. Compare line 578 with line 636, for example. Should\n",
      "we interpret this or just attribute it to the unpredictability of how these\n",
      "models interact with data?\n",
      "\n",
      "* Section 4.3 concludes by saying, of the connective 'which then', that the\n",
      "system can \"correctly disambiguate its causal meaning\", whereas that of Hidey\n",
      "and McKeown does not. That might be correct, but one example doesn't suffice to\n",
      "show it. To substantiate this point, I suggest making up a wide range of\n",
      "examples that manifest the ambiguity and seeing how often the system delivers\n",
      "the right verdict. This will help address the question of whether it got lucky\n",
      "with the example from table 8.\n",
      "This paper proposes a method for detecting causal relations between clauses,\n",
      "using neural networks (\"deep learning\", although, as in many studies, the\n",
      "networks are not particularly deep).  Indeed, while certain discourse\n",
      "connectives are unambiguous regarding the relation they signal (e.g. 'because'\n",
      "is causal) the paper takes advantage of a recent dataset (called AltLex, by\n",
      "Hidey and McKeown, 2016) to solve the task of identifying causal vs. non-causal\n",
      "relations when the relation is not explicitly marked.  Arguing that\n",
      "convolutional networks are not as adept as representing the relevant features\n",
      "of clauses as LSTMs, the authors propose a classification architecture which\n",
      "uses a Glove-based representation of clauses, input in an LSTM layer, followed\n",
      "by three densely connected layers (tanh) and a final decision layer with a\n",
      "softmax.\n",
      "\n",
      "The best configuration of the system improves by 0.5-1.5% F1 over Hidey and\n",
      "MCkeown's 2016 one (SVM classifier).  Several examples of generalizations where\n",
      "the system performs well are shown (indicator words that are always causal in\n",
      "the training data, but are found correctly to be non causal in the test data).\n",
      "Therefore, I appreciate that the system is analyzed qualitatively and \n",
      "quantitatively.\n",
      "\n",
      "The paper is well written, and the description of the problem is particularly\n",
      "clear. However a clarification of the differences between this task and the \n",
      "task of implicit connective recognition would be welcome.  This could possibly \n",
      "include a discussion of why previous methods for implicit connective \n",
      "recognition cannot be used in this case.\n",
      "\n",
      "It is very appreciable that the authors uploaded their code to the submission\n",
      "site (I inspected it briefly but did not execute it).  Uploading the (older)\n",
      "data (with the code) is also useful as it provides many examples.  It was not\n",
      "clear to me what is the meaning of the 0-1-2 coding in the TSV files, given\n",
      "that the paper mentions binary classification. I wonder also, given that this\n",
      "is the data from Hidey and McKeown, if the authors have the right to repost it\n",
      "as they do.  -- One point to clarify in the paper would be the meaning of\n",
      "\"bootstrapping\", which apparently extends the corpus by about 15%: while the\n",
      "construction of the corpus is briefly but clearly explained in the paper, the\n",
      "additional bootstrapping is not. \n",
      "\n",
      "While it is certainly interesting to experiment with neural networks on this\n",
      "task, the merits of the proposed system are not entirely convincing.  It seems\n",
      "indeed that the best configuration (among 4-7 options) is found on the test\n",
      "data, and it is this best configuration that is announced as improving over\n",
      "Hidey by \"2.13% F1\".  However, a fair comparison would involve selecting the\n",
      "best configuration on the devset.\n",
      "\n",
      "Moreover, it is not entirely clear how significant the improvement is. On the\n",
      "one hand, it should be possible, given the size of the dataset, to compute some\n",
      "statistical significance indicators.  On the other hand, one should consider\n",
      "also the reliability of the gold-standard annotation itself (possibly from the\n",
      "creators of the dataset).  Upon inspection, the annotation obtained from the\n",
      "English/SimpleEnglish Wikipedia is not perfect, and therefore the scores might\n",
      "need to be considered with a grain of salt.\n",
      "\n",
      "Finally, neural methods have been previously shown to outperform human\n",
      "engineered features for binary classification tasks, so in a sense the results \n",
      "are rather a confirmation of a known property. It would be interesting to see\n",
      "experiments with simpler networks used as baselines, e.g. a 1-layer LSTM.  The\n",
      "analysis of results could try to explain why the neural method seems to favor \n",
      "precision over recall.\n"
     ]
    }
   ],
   "source": [
    "for text in accepted_paper['comments']:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                      0\n",
       "IMPACT                     6\n",
       "SUBSTANCE                  0\n",
       "APPROPRIATENESS            0\n",
       "MEANINGFUL_COMPARISON      6\n",
       "PRESENTATION_FORMAT        0\n",
       "comments                   0\n",
       "SOUNDNESS_CORRECTNESS      0\n",
       "ORIGINALITY                0\n",
       "RECOMMENDATION             0\n",
       "CLARITY                    0\n",
       "REVIEWER_CONFIDENCE        0\n",
       "citation_count             0\n",
       "source                     0\n",
       "PRESENTATION_FORMAT_NUM    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accepted_paper.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>IMPACT</th>\n",
       "      <th>SUBSTANCE</th>\n",
       "      <th>APPROPRIATENESS</th>\n",
       "      <th>MEANINGFUL_COMPARISON</th>\n",
       "      <th>PRESENTATION_FORMAT</th>\n",
       "      <th>comments</th>\n",
       "      <th>SOUNDNESS_CORRECTNESS</th>\n",
       "      <th>ORIGINALITY</th>\n",
       "      <th>RECOMMENDATION</th>\n",
       "      <th>CLARITY</th>\n",
       "      <th>REVIEWER_CONFIDENCE</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>source</th>\n",
       "      <th>PRESENTATION_FORMAT_NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Exploiting Argument Information to Improve Eve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Poster</td>\n",
       "      <td>- Strengths:\\nThis paper tries to use the info...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>164</td>\n",
       "      <td>acl_2017</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Time Expression Analysis and Recognition Using...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oral Presentation</td>\n",
       "      <td>This paper describes a rule based approach to ...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>acl_2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Time Expression Analysis and Recognition Using...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Poster</td>\n",
       "      <td>The paper proposes a method to recognize time ...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>acl_2017</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Attention-over-Attention Neural Networks for R...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oral Presentation</td>\n",
       "      <td>- Strengths:\\n\\n-- A well-motivated approach, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>acl_2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Generating and Exploiting Large-scale Pseudo T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oral Presentation</td>\n",
       "      <td>- Strengths:\\nThis paper introduced a novel me...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>acl_2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Generating and Exploiting Large-scale Pseudo T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Poster</td>\n",
       "      <td>- Strengths:\\n\\nThe approach is novel and the ...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>acl_2017</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title IMPACT SUBSTANCE  \\\n",
       "30   Exploiting Argument Information to Improve Eve...    NaN         4   \n",
       "109  Time Expression Analysis and Recognition Using...    NaN         4   \n",
       "110  Time Expression Analysis and Recognition Using...    NaN         4   \n",
       "141  Attention-over-Attention Neural Networks for R...    NaN         4   \n",
       "152  Generating and Exploiting Large-scale Pseudo T...    NaN         3   \n",
       "153  Generating and Exploiting Large-scale Pseudo T...    NaN         4   \n",
       "\n",
       "    APPROPRIATENESS MEANINGFUL_COMPARISON PRESENTATION_FORMAT  \\\n",
       "30                5                   NaN              Poster   \n",
       "109               5                   NaN   Oral Presentation   \n",
       "110               4                   NaN              Poster   \n",
       "141               5                   NaN   Oral Presentation   \n",
       "152               5                   NaN   Oral Presentation   \n",
       "153               5                   NaN              Poster   \n",
       "\n",
       "                                              comments SOUNDNESS_CORRECTNESS  \\\n",
       "30   - Strengths:\\nThis paper tries to use the info...                     5   \n",
       "109  This paper describes a rule based approach to ...                     4   \n",
       "110  The paper proposes a method to recognize time ...                     4   \n",
       "141  - Strengths:\\n\\n-- A well-motivated approach, ...                     5   \n",
       "152  - Strengths:\\nThis paper introduced a novel me...                     5   \n",
       "153  - Strengths:\\n\\nThe approach is novel and the ...                     5   \n",
       "\n",
       "    ORIGINALITY RECOMMENDATION CLARITY REVIEWER_CONFIDENCE  citation_count  \\\n",
       "30            3              4       4                   3             164   \n",
       "109           3              4       4                   3             140   \n",
       "110           3              3       4                   4             140   \n",
       "141           3              5       5                   4             200   \n",
       "152           3              4       4                   4             101   \n",
       "153           3              4       4                   3             101   \n",
       "\n",
       "       source  PRESENTATION_FORMAT_NUM  \n",
       "30   acl_2017                        2  \n",
       "109  acl_2017                        1  \n",
       "110  acl_2017                        2  \n",
       "141  acl_2017                        1  \n",
       "152  acl_2017                        1  \n",
       "153  acl_2017                        2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_rows = accepted_paper[accepted_paper.isna().any(axis=1)]\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The missing values should be removed because we have no way to impute the comments and the presentation format for the missing values\n",
    "accepted_new_paper = accepted_paper.dropna().drop(columns=['PRESENTATION_FORMAT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.boxplot(accepted_new_paper['citation_count'])\n",
    "plt.title('Boxplot of Citation Count')\n",
    "plt.ylabel('Citation Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the matadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title                      object\n",
      "IMPACT                     object\n",
      "SUBSTANCE                  object\n",
      "APPROPRIATENESS            object\n",
      "MEANINGFUL_COMPARISON      object\n",
      "comments                   object\n",
      "SOUNDNESS_CORRECTNESS      object\n",
      "ORIGINALITY                object\n",
      "RECOMMENDATION             object\n",
      "CLARITY                    object\n",
      "REVIEWER_CONFIDENCE        object\n",
      "citation_count              int64\n",
      "source                     object\n",
      "PRESENTATION_FORMAT_NUM     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(accepted_new_paper.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the 'object' to 'numerical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the string to numerical\n",
    "numerical_columns = ['IMPACT', 'SUBSTANCE', 'APPROPRIATENESS',\n",
    "       'MEANINGFUL_COMPARISON', 'PRESENTATION_FORMAT_NUM', \n",
    "       'SOUNDNESS_CORRECTNESS', 'ORIGINALITY', 'RECOMMENDATION', 'CLARITY',\n",
    "       'REVIEWER_CONFIDENCE', 'citation_count']\n",
    "for column in numerical_columns:\n",
    "    accepted_new_paper[column] = pd.to_numeric(accepted_new_paper[column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title                      object\n",
      "IMPACT                      int64\n",
      "SUBSTANCE                   int64\n",
      "APPROPRIATENESS             int64\n",
      "MEANINGFUL_COMPARISON       int64\n",
      "comments                   object\n",
      "SOUNDNESS_CORRECTNESS       int64\n",
      "ORIGINALITY                 int64\n",
      "RECOMMENDATION              int64\n",
      "CLARITY                     int64\n",
      "REVIEWER_CONFIDENCE         int64\n",
      "citation_count              int64\n",
      "source                     object\n",
      "PRESENTATION_FORMAT_NUM     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(accepted_new_paper.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMPACT</th>\n",
       "      <th>SUBSTANCE</th>\n",
       "      <th>APPROPRIATENESS</th>\n",
       "      <th>MEANINGFUL_COMPARISON</th>\n",
       "      <th>SOUNDNESS_CORRECTNESS</th>\n",
       "      <th>ORIGINALITY</th>\n",
       "      <th>RECOMMENDATION</th>\n",
       "      <th>CLARITY</th>\n",
       "      <th>REVIEWER_CONFIDENCE</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>PRESENTATION_FORMAT_NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>172.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>172.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.238372</td>\n",
       "      <td>3.668605</td>\n",
       "      <td>4.889535</td>\n",
       "      <td>3.563953</td>\n",
       "      <td>4.063953</td>\n",
       "      <td>3.848837</td>\n",
       "      <td>3.517442</td>\n",
       "      <td>4.034884</td>\n",
       "      <td>3.883721</td>\n",
       "      <td>117.744186</td>\n",
       "      <td>1.505814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.636266</td>\n",
       "      <td>0.787737</td>\n",
       "      <td>0.396632</td>\n",
       "      <td>0.879582</td>\n",
       "      <td>0.872908</td>\n",
       "      <td>0.892037</td>\n",
       "      <td>0.783409</td>\n",
       "      <td>0.871212</td>\n",
       "      <td>0.786116</td>\n",
       "      <td>64.085868</td>\n",
       "      <td>0.501426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>171.750000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           IMPACT   SUBSTANCE  APPROPRIATENESS  MEANINGFUL_COMPARISON  \\\n",
       "count  172.000000  172.000000       172.000000             172.000000   \n",
       "mean     3.238372    3.668605         4.889535               3.563953   \n",
       "std      0.636266    0.787737         0.396632               0.879582   \n",
       "min      2.000000    1.000000         2.000000               1.000000   \n",
       "25%      3.000000    3.000000         5.000000               3.000000   \n",
       "50%      3.000000    4.000000         5.000000               3.000000   \n",
       "75%      3.000000    4.000000         5.000000               4.000000   \n",
       "max      5.000000    5.000000         5.000000               5.000000   \n",
       "\n",
       "       SOUNDNESS_CORRECTNESS  ORIGINALITY  RECOMMENDATION     CLARITY  \\\n",
       "count             172.000000   172.000000      172.000000  172.000000   \n",
       "mean                4.063953     3.848837        3.517442    4.034884   \n",
       "std                 0.872908     0.892037        0.783409    0.871212   \n",
       "min                 3.000000     2.000000        1.000000    1.000000   \n",
       "25%                 3.000000     3.000000        3.000000    4.000000   \n",
       "50%                 4.000000     4.000000        4.000000    4.000000   \n",
       "75%                 5.000000     5.000000        4.000000    5.000000   \n",
       "max                 5.000000     5.000000        5.000000    5.000000   \n",
       "\n",
       "       REVIEWER_CONFIDENCE  citation_count  PRESENTATION_FORMAT_NUM  \n",
       "count           172.000000      172.000000               172.000000  \n",
       "mean              3.883721      117.744186                 1.505814  \n",
       "std               0.786116       64.085868                 0.501426  \n",
       "min               2.000000        1.000000                 1.000000  \n",
       "25%               3.000000       60.000000                 1.000000  \n",
       "50%               4.000000      115.000000                 2.000000  \n",
       "75%               4.000000      171.750000                 2.000000  \n",
       "max               5.000000      250.000000                 2.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accepted_new_paper.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After review, there is no outlier data, thus we have finished the data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                      object\n",
       "IMPACT                      int64\n",
       "SUBSTANCE                   int64\n",
       "APPROPRIATENESS             int64\n",
       "MEANINGFUL_COMPARISON       int64\n",
       "comments                   object\n",
       "SOUNDNESS_CORRECTNESS       int64\n",
       "ORIGINALITY                 int64\n",
       "RECOMMENDATION              int64\n",
       "CLARITY                     int64\n",
       "REVIEWER_CONFIDENCE         int64\n",
       "citation_count              int64\n",
       "source                     object\n",
       "PRESENTATION_FORMAT_NUM     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accepted_new_paper.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_clean = accepted_new_paper.drop(columns=['comments']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_clean.to_pickle('data/accepted_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_clean.to_pickle('data/accepted_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
